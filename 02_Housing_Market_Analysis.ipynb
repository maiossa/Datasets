{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b507988",
   "metadata": {},
   "source": [
    "# Berlin Housing Market Analysis - Hauptanalyse\n",
    "## Datenbereinigung, Zusammenführung und Explorative Analyse\n",
    "\n",
    "### Projektübersicht\n",
    "Dieses Notebook führt die Hauptanalyse des Berliner Wohnungsmarktes durch. Es baut auf dem PLZ-Mapping aus `01_Data_Preprocessing.ipynb` auf und erstellt eine umfassende Analyse der drei Datensätze.\n",
    "\n",
    "### Voraussetzungen\n",
    "- `01_Data_Preprocessing.ipynb` wurde erfolgreich ausgeführt\n",
    "- PLZ-Mapping-Tabelle liegt in `data/processed/berlin_plz_mapping.csv` vor\n",
    "- Originaldaten befinden sich in `data/raw/`\n",
    "\n",
    "### Analyseziele\n",
    "1. **Datenbereinigung und -normalisierung** aller drei Datensätze\n",
    "2. **Zusammenführung** in ein einheitliches Dataset\n",
    "3. **Zeitreihenanalyse** der Mietpreisentwicklung\n",
    "4. **Bezirksvergleiche** mit statistischen Tests\n",
    "5. **Explorative Datenanalyse** mit umfassenden Visualisierungen\n",
    "\n",
    "---\n",
    "\n",
    "**Autor**: Berlin Housing Market Analysis Team  \n",
    "**Datum**: 4. Juli 2025  \n",
    "**Version**: 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2d5df1",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries\n",
    "Importieren der erforderlichen Python-Bibliotheken für Datenverarbeitung, Analyse und Visualisierung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1cf07a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alle Bibliotheken erfolgreich importiert!\n",
      "Pandas Version: 2.3.0\n",
      "NumPy Version: 2.3.0\n",
      "Matplotlib Version: 3.10.3\n",
      "Seaborn Version: 0.13.2\n",
      "Verarbeitung gestartet am: 2025-07-04 03:46:55\n"
     ]
    }
   ],
   "source": [
    "# Datenmanipulation und -analyse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualisierung\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Statistik und Machine Learning\n",
    "from scipy import stats\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Textverarbeitung und Regex\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Datum und Zeit\n",
    "from datetime import datetime\n",
    "\n",
    "# Konfiguration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "print(\"Alle Bibliotheken erfolgreich importiert!\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"NumPy Version: {np.__version__}\")\n",
    "print(f\"Matplotlib Version: {matplotlib.__version__}\")\n",
    "print(f\"Seaborn Version: {sns.__version__}\")\n",
    "print(f\"Verarbeitung gestartet am: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca44272",
   "metadata": {},
   "source": [
    "## 2. Laden der Originaldaten und PLZ-Mapping\n",
    "### 2.1 PLZ-Mapping aus Preprocessing laden\n",
    "Laden der PLZ-zu-Bezirk-Mapping-Tabelle die im vorherigen Notebook erstellt wurde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1a7269b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LADEN DER PLZ-MAPPING-TABELLE\n",
      "============================================================\n",
      "PLZ-Mapping erfolgreich geladen: 208 PLZ-Zuordnungen\n",
      "Abgedeckte Bezirke: 12\n",
      "Bezirke: ['Charlottenburg-Wilmersdorf', 'Friedrichshain-Kreuzberg', 'Lichtenberg', 'Marzahn-Hellersdorf', 'Mitte', 'Neukölln', 'Pankow', 'Reinickendorf', 'Spandau', 'Steglitz-Zehlendorf', 'Tempelhof-Schöneberg', 'Treptow-Köpenick']\n",
      "\n",
      "============================================================\n",
      "LADEN DER ORIGINALDATEN\n",
      "============================================================\n",
      "\n",
      "Dataset 2025 (Immobilienscout24)\n",
      "Zeilen: 6,109\n",
      "Spalten: 5\n",
      "Spalten: ['title', 'price', 'size', 'address', 'link']\n",
      "\n",
      "Dataset 2018-2019 (Kaggle)\n",
      "Zeilen: 10,406\n",
      "Spalten: 9\n",
      "Spalten: ['regio3', 'street', 'livingSpace', 'baseRent', 'totalRent', 'noRooms', 'floor', 'typeOfFlat', 'yearConstructed']\n",
      "\n",
      "Dataset 2022 (Springer/Immowelt/Immonet)\n",
      "Zeilen: 2,950\n",
      "Spalten: 75\n",
      "Erste 10 Spalten: ['ID', 'SORTE', 'PLZ', 'KALTMIETE', 'WARMMIETE', 'NEBENKOSTEN', 'KAUTION', 'HEIZUNGSKOSTEN', 'ZIMMER', 'PARKPLAETZE']\n",
      "\n",
      "Gesamtanzahl Rohdaten: 19,465 Datenpunkte aus 3 Quellen\n"
     ]
    }
   ],
   "source": [
    "# Lade PLZ-Mapping-Tabelle aus dem Preprocessing\n",
    "print(\"=\" * 60)\n",
    "print(\"LADEN DER PLZ-MAPPING-TABELLE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    plz_mapping_df = pd.read_csv('data/processed/berlin_plz_mapping.csv')\n",
    "    print(f\"PLZ-Mapping erfolgreich geladen: {len(plz_mapping_df)} PLZ-Zuordnungen\")\n",
    "    print(f\"Abgedeckte Bezirke: {plz_mapping_df['Bezirk'].nunique()}\")\n",
    "    print(f\"Bezirke: {sorted(plz_mapping_df['Bezirk'].unique())}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"FEHLER: PLZ-Mapping-Tabelle nicht gefunden!\")\n",
    "    print(\"Bitte führen Sie zuerst 01_Data_Preprocessing.ipynb aus.\")\n",
    "    raise\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LADEN DER ORIGINALDATEN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Lade alle drei Originaldatensätze\n",
    "datasets = {}\n",
    "\n",
    "# Dataset 2025 (Immobilienscout24)\n",
    "print(\"\\nDataset 2025 (Immobilienscout24)\")\n",
    "datasets['2025'] = pd.read_csv('data/raw/Dataset_2025.csv')\n",
    "print(f\"Zeilen: {datasets['2025'].shape[0]:,}\")\n",
    "print(f\"Spalten: {datasets['2025'].shape[1]}\")\n",
    "print(f\"Spalten: {list(datasets['2025'].columns)}\")\n",
    "\n",
    "# Dataset 2018-2019 (Kaggle)\n",
    "print(\"\\nDataset 2018-2019 (Kaggle)\")\n",
    "datasets['2018_2019'] = pd.read_csv('data/raw/Dataset_2018_2019.csv')\n",
    "print(f\"Zeilen: {datasets['2018_2019'].shape[0]:,}\")\n",
    "print(f\"Spalten: {datasets['2018_2019'].shape[1]}\")\n",
    "print(f\"Spalten: {list(datasets['2018_2019'].columns)}\")\n",
    "\n",
    "# Dataset 2022 (Springer/Immowelt/Immonet)\n",
    "print(\"\\nDataset 2022 (Springer/Immowelt/Immonet)\")\n",
    "datasets['2022'] = pd.read_csv('data/raw/Dataset_2022.csv')\n",
    "print(f\"Zeilen: {datasets['2022'].shape[0]:,}\")\n",
    "print(f\"Spalten: {datasets['2022'].shape[1]}\")\n",
    "print(f\"Erste 10 Spalten: {list(datasets['2022'].columns[:10])}\")\n",
    "\n",
    "total_rows = sum(df.shape[0] for df in datasets.values())\n",
    "print(f\"\\nGesamtanzahl Rohdaten: {total_rows:,} Datenpunkte aus {len(datasets)} Quellen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0eb4242",
   "metadata": {},
   "source": [
    "### 2.2 Erste Datenqualitätsanalyse\n",
    "Analysiere die Struktur und Qualität der drei Datensätze vor der Bereinigung."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0cfcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detaillierte Analyse jedes Datensatzes\n",
    "print(\"=\" * 80)\n",
    "print(\"DATENQUALITÄTSANALYSE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for year, df in datasets.items():\n",
    "    print(f\"\\n{'='*20} DATASET {year} {'='*20}\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Fehlende Werte\n",
    "    missing_info = df.isnull().sum()\n",
    "    missing_pct = (missing_info / len(df) * 100).round(2)\n",
    "    \n",
    "    if missing_info.sum() > 0:\n",
    "        print(f\"\\nFehlende Werte:\")\n",
    "        for col in missing_info[missing_info > 0].index:\n",
    "            print(f\"  {col}: {missing_info[col]} ({missing_pct[col]}%)\")\n",
    "    else:\n",
    "        print(\"\\nKeine fehlenden Werte gefunden\")\n",
    "    \n",
    "    # Datentypen\n",
    "    print(f\"\\nDatentypen:\")\n",
    "    print(df.dtypes.value_counts())\n",
    "    \n",
    "    # Erste 3 Zeilen als Beispiel\n",
    "    print(f\"\\nBeispieldaten (erste 3 Zeilen):\")\n",
    "    print(df.head(3))\n",
    "    \n",
    "    print(f\"\\nNumerische Spalten Statistik:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        print(df[numeric_cols].describe())\n",
    "    else:\n",
    "        print(\"Keine numerischen Spalten gefunden\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ZUSAMMENFASSUNG DER DATENQUALITÄT\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Dataset 2025: {datasets['2025'].shape[0]:,} Zeilen, {datasets['2025'].shape[1]} Spalten\")\n",
    "print(f\"Dataset 2018-2019: {datasets['2018_2019'].shape[0]:,} Zeilen, {datasets['2018_2019'].shape[1]} Spalten\") \n",
    "print(f\"Dataset 2022: {datasets['2022'].shape[0]:,} Zeilen, {datasets['2022'].shape[1]} Spalten\")\n",
    "print(f\"Gesamtdatenpunkte: {sum(df.shape[0] for df in datasets.values()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c09965b",
   "metadata": {},
   "source": [
    "## 3. Datenbereinigung und Normalisierung\n",
    "### 3.1 Dataset 2022: PLZ-zu-Bezirk-Zuordnung anwenden\n",
    "\n",
    "Das kritische PLZ-Problem aus Phase 1 wird jetzt gelöst, indem wir die erstellte Mapping-Tabelle anwenden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06772b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLZ-zu-Bezirk-Zuordnung für Dataset 2022\n",
    "print(\"=\" * 60)\n",
    "print(\"PLZ-ZU-BEZIRK-ZUORDNUNG FÜR DATASET 2022\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Erstelle Kopie für Bearbeitung\n",
    "df_2022 = datasets['2022'].copy()\n",
    "\n",
    "# PLZ als String für Matching\n",
    "df_2022['PLZ_str'] = df_2022['PLZ'].astype(str)\n",
    "\n",
    "# Merge mit PLZ-Mapping\n",
    "df_2022_mapped = df_2022.merge(\n",
    "    plz_mapping_df, \n",
    "    left_on='PLZ_str', \n",
    "    right_on='PLZ', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Statistik der Zuordnung\n",
    "total_entries = len(df_2022_mapped)\n",
    "successful_mappings = df_2022_mapped['Bezirk'].notna().sum()\n",
    "mapping_rate = (successful_mappings / total_entries) * 100\n",
    "\n",
    "print(f\"Gesamteinträge Dataset 2022: {total_entries:,}\")\n",
    "print(f\"Erfolgreich zugeordnet: {successful_mappings:,}\")\n",
    "print(f\"Zuordnungsrate: {mapping_rate:.1f}%\")\n",
    "\n",
    "# Nicht zugeordnete PLZ analysieren\n",
    "unmapped = df_2022_mapped[df_2022_mapped['Bezirk'].isna()]\n",
    "if len(unmapped) > 0:\n",
    "    print(f\"\\nNicht zugeordnete PLZ ({len(unmapped)} Einträge):\")\n",
    "    unmapped_plz_counts = unmapped['PLZ_str'].value_counts()\n",
    "    for plz, count in unmapped_plz_counts.items():\n",
    "        print(f\"  PLZ {plz}: {count} Einträge\")\n",
    "\n",
    "# Bezirksverteilung\n",
    "print(f\"\\nBezirksverteilung (Top 10):\")\n",
    "bezirk_counts = df_2022_mapped['Bezirk'].value_counts()\n",
    "for bezirk, count in bezirk_counts.head(10).items():\n",
    "    print(f\"  {bezirk}: {count} Einträge\")\n",
    "\n",
    "# Nur erfolgreich zugeordnete Daten behalten\n",
    "df_2022_clean = df_2022_mapped[df_2022_mapped['Bezirk'].notna()].copy()\n",
    "print(f\"\\nBereinigte Dataset 2022: {len(df_2022_clean):,} Einträge (Verlust: {len(df_2022) - len(df_2022_clean)} Einträge)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
