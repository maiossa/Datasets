{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c57dfb6",
   "metadata": {},
   "source": [
    "## üìö **Methodologie und Technische Dokumentation**\n",
    "\n",
    "### üéØ **Zentrale Herausforderung: Heterogene Datenstrukturen**\n",
    "\n",
    "Das Dataset 2025 unterscheidet sich fundamental von den Datens√§tzen 2018-2019 und 2022:\n",
    "\n",
    "| **Merkmal** | **Dataset 2018-2019** | **Dataset 2022** | **Dataset 2025** |\n",
    "|-------------|----------------------|-------------------|-------------------|\n",
    "| **PLZ-Verf√ºgbarkeit** | 100% | 100% | ~1-2% |\n",
    "| **Adressformat** | Strukturiert | Strukturiert | Heterogen |\n",
    "| **Bezirksangaben** | Implizit √ºber PLZ | Implizit √ºber PLZ | Explizit als Text |\n",
    "| **Multi-Listings** | Keine | Keine | Ja (Preis-/Gr√∂√üenspannen) |\n",
    "\n",
    "### üî¨ **Entwickelte L√∂sungsans√§tze:**\n",
    "\n",
    "#### **1. Intelligente Adressextraktion**\n",
    "- **Regex-basierte PLZ-Extraktion:** `\\b(\\d{5})\\b`\n",
    "- **Mehrstufige Bezirks-Erkennung:** Alias-Mapping f√ºr Berlin-spezifische Varianten\n",
    "- **Fallback-Mechanismen:** Strukturierte Priorit√§tenliste f√ºr Adresskomponenten\n",
    "\n",
    "#### **2. Multi-Listing-Behandlung**\n",
    "- **Preisspannen:** `\"725 - 1.965‚Ç¨\"` ‚Üí Minimum-Ansatz f√ºr Vergleichbarkeit\n",
    "- **Gr√∂√üenspannen:** `\"26,55 - 112,82m¬≤\"` ‚Üí Konservative Sch√§tzung\n",
    "- **Rationale:** Vermeidung von √úbersch√§tzungen bei Zeitvergleichen\n",
    "\n",
    "#### **3. Dual-Strategie-Anreicherung**\n",
    "- **Strategie 1:** PLZ-basiert (h√∂chste Pr√§zision, wenige Datenpunkte)\n",
    "- **Strategie 2:** Bezirks-basiert (Fallback f√ºr 98% der Daten)\n",
    "- **Kombination:** Nahtlose Integration beider Ans√§tze\n",
    "\n",
    "### üìä **Qualit√§tssicherung:**\n",
    "\n",
    "- **Vermeidung kartesischer Produkte:** Durch `drop_duplicates(subset=['plz'])`\n",
    "- **Datenintegrit√§tspr√ºfung:** Vor- und Nach-Vergleiche aller Verarbeitungsschritte\n",
    "- **Standardisierte Filter:** Identisch mit anderen Datasets f√ºr Vergleichbarkeit\n",
    "\n",
    "### üéì **Wissenschaftliche Relevanz:**\n",
    "\n",
    "Diese Methodologie demonstriert den Umgang mit **heterogenen Datenquellen** in der Immobiliendatenanalyse - ein h√§ufiges Problem bei longitudinalen Studien, wo sich Datenstrukturen √ºber Zeit √§ndern.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28af644",
   "metadata": {},
   "source": [
    "# 03_Clean_Dataset_2025 - Intelligente Adressextraktion\n",
    "\n",
    "## üéØ **Spezifische Bereinigung f√ºr Dataset 2025**\n",
    "\n",
    "### **Hauptfunktionen:**\n",
    "- **Intelligente PLZ-Extraktion** aus verschiedenen Adressformaten\n",
    "- **Bezirk-Normalisierung** mit Alias-Mapping\n",
    "- **Multi-Listing-Behandlung** (Preis- und Gr√∂√üenspannen)\n",
    "- **Filter-Harmonisierung** mit anderen Datasets\n",
    "- **Standardisierte Ausgabe** kompatibel mit anderen Datasets\n",
    "\n",
    "### **üîÑ Filter-Harmonisierung (Konsistent mit allen Datasets):**\n",
    "- **Preis-Filter:** 100‚Ç¨ - 10.000‚Ç¨ (Kaltmiete)\n",
    "- **Gr√∂√üen-Filter:** 10m¬≤ - 500m¬≤ (Wohnfl√§che)\n",
    "- **Bezirk-Validierung:** Nur g√ºltige Berliner Bezirke\n",
    "\n",
    "### **üìã Adressformate im 2025 Dataset:**\n",
    "1. **Vollst√§ndige Adresse mit PLZ:** \"Johannisplatz 3, 10117 Berlin\"\n",
    "2. **Adresse mit Bezirk:** \"Johannisplatz 5, Mitte (Ortsteil), Berlin\" \n",
    "3. **Nur Bezirk:** \"Tiergarten, Berlin\"\n",
    "4. **Nur PLZ:** \"10557 Berlin\"\n",
    "5. **Adresse mit Ortsteil:** \"Friedrichshain, Berlin\"\n",
    "\n",
    "### **üéØ Ziel:** \n",
    "Einheitliche Bezirk-Zuordnung und maximale Vergleichbarkeit mit Dataset 2018-2019 und Dataset 2022\n",
    "\n",
    "---\n",
    "**Teil der modularen Preprocessing-Pipeline**  \n",
    "**Datum:** 4. Juli 2025  \n",
    "**Version:** 1.1 (Filter-Harmonisierung)  \n",
    "**Status:** ‚úÖ Harmonisiert mit allen anderen Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b3a266",
   "metadata": {},
   "source": [
    "## 1. Setup und Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a456e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotheken erfolgreich importiert!\n",
      "Pandas Version: 2.3.0\n",
      "Dataset: 2025 (ImmobilienScout24)\n",
      "Ziel: Intelligente Adressextraktion und Bezirk-Normalisierung\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Bibliotheken erfolgreich importiert!\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(\"Dataset: 2025 (ImmobilienScout24)\")\n",
    "print(\"Ziel: Intelligente Adressextraktion und Bezirk-Normalisierung\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a068c88",
   "metadata": {},
   "source": [
    "## 2. PLZ-Mapping und Bezirk-Normalisierung laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9037091a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PLZ-MAPPING UND BEZIRK-NORMALISIERUNG\n",
      "============================================================\n",
      "============================================================\n",
      "PLZ-ZU-BEZIRK-MAPPING LADEN\n",
      "============================================================\n",
      "‚úÖ PLZ-Mapping geladen: 181 Eintr√§ge\n",
      "‚úÖ PLZ-Dictionary erstellt: 181 Zuordnungen\n",
      "‚úÖ PLZ-Mapping geladen: 185 Zuordnungen\n",
      "‚úÖ Bezirk-Aliases definiert: 36 Zuordnungen\n",
      "\n",
      "PLZ-Mapping Beispiele:\n",
      "  10115 ‚Üí Mitte\n",
      "  10117 ‚Üí Mitte\n",
      "  10119 ‚Üí Mitte\n",
      "  10178 ‚Üí Mitte\n",
      "  10179 ‚Üí Mitte\n",
      "\n",
      "Bezirk-Normalisierung Beispiele:\n",
      "  'Mitte (Ortsteil)' ‚Üí 'Mitte'\n",
      "  'Pankow (Ortsteil)' ‚Üí 'Pankow'\n",
      "  'Spandau (Ortsteil)' ‚Üí 'Spandau'\n",
      "  'Neuk√∂lln (Ortsteil)' ‚Üí 'Neuk√∂lln'\n",
      "  'Friedrichshain' ‚Üí 'Friedrichshain-Kreuzberg'\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PLZ-MAPPING UND BEZIRK-NORMALISIERUNG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PLZ-zu-Bezirk-Mapping laden\n",
    "print(\"=\" * 60)\n",
    "print(\"PLZ-ZU-BEZIRK-MAPPING LADEN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    plz_mapping_df = pd.read_csv('data/processed/berlin_plz_mapping.csv')\n",
    "    print(f\"‚úÖ PLZ-Mapping geladen: {len(plz_mapping_df)} Eintr√§ge\")\n",
    "    \n",
    "    # Erstelle Dictionary f√ºr schnelles Lookup\n",
    "    plz_to_district = dict(zip(plz_mapping_df['PLZ'], plz_mapping_df['Bezirk']))\n",
    "    print(f\"‚úÖ PLZ-Dictionary erstellt: {len(plz_to_district)} Zuordnungen\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå FEHLER: PLZ-Mapping nicht gefunden!\")\n",
    "    print(\"Bitte stellen Sie sicher, dass 'data/processed/berlin_plz_mapping.csv' existiert.\")\n",
    "    raise\n",
    "\n",
    "# Erweiterte PLZ-Zuordnungen f√ºr Dataset 2025\n",
    "extended_plz_mapping = {\n",
    "    10115: 'Mitte',\n",
    "    10117: 'Mitte',\n",
    "    10119: 'Mitte',\n",
    "    10178: 'Mitte',\n",
    "    10179: 'Mitte',\n",
    "    10243: 'Friedrichshain-Kreuzberg',\n",
    "    10245: 'Friedrichshain-Kreuzberg',\n",
    "    10247: 'Friedrichshain-Kreuzberg',\n",
    "    10249: 'Friedrichshain-Kreuzberg',\n",
    "    10315: 'Lichtenberg',\n",
    "    10317: 'Lichtenberg',\n",
    "    10318: 'Lichtenberg',\n",
    "    10319: 'Lichtenberg',\n",
    "    10365: 'Lichtenberg',\n",
    "    10367: 'Lichtenberg',\n",
    "    10369: 'Lichtenberg',\n",
    "    12305: 'Tempelhof-Sch√∂neberg',\n",
    "    12307: 'Tempelhof-Sch√∂neberg',\n",
    "    12309: 'Tempelhof-Sch√∂neberg',\n",
    "    12347: 'Neuk√∂lln',\n",
    "    12349: 'Neuk√∂lln',\n",
    "    12351: 'Neuk√∂lln',\n",
    "    12353: 'Neuk√∂lln',\n",
    "    12355: 'Neuk√∂lln',\n",
    "    12357: 'Neuk√∂lln',\n",
    "    12359: 'Neuk√∂lln',\n",
    "    12524: 'Treptow-K√∂penick',\n",
    "    12555: 'Treptow-K√∂penick',\n",
    "    10247: 'Friedrichshain-Kreuzberg',\n",
    "    10249: 'Friedrichshain-Kreuzberg',\n",
    "    10367: 'Lichtenberg',\n",
    "    10369: 'Lichtenberg',\n",
    "    14612: 'Falkensee',  # Au√üerhalb Berlin\n",
    "    13507: 'Reinickendorf',\n",
    "    10585: 'Charlottenburg-Wilmersdorf',\n",
    "    10709: 'Charlottenburg-Wilmersdorf',\n",
    "    10559: 'Mitte',\n",
    "}\n",
    "\n",
    "# Erweitere PLZ-Mapping\n",
    "plz_to_district.update(extended_plz_mapping)\n",
    "\n",
    "# Bezirk-Normalisierung (verschiedene Schreibweisen auf einheitliche Namen mappen)\n",
    "district_aliases = {\n",
    "    'Mitte (Ortsteil)': 'Mitte',\n",
    "    'Pankow (Ortsteil)': 'Pankow',\n",
    "    'Spandau (Ortsteil)': 'Spandau',\n",
    "    'Neuk√∂lln (Ortsteil)': 'Neuk√∂lln',\n",
    "    'Friedrichshain': 'Friedrichshain-Kreuzberg',\n",
    "    'Kreuzberg': 'Friedrichshain-Kreuzberg',\n",
    "    'Charlottenburg': 'Charlottenburg-Wilmersdorf',\n",
    "    'Wilmersdorf': 'Charlottenburg-Wilmersdorf',\n",
    "    'Tempelhof': 'Tempelhof-Sch√∂neberg',\n",
    "    'Sch√∂neberg': 'Tempelhof-Sch√∂neberg',\n",
    "    'Prenzlauer Berg': 'Pankow',\n",
    "    'Wei√üensee': 'Pankow',\n",
    "    'Buch': 'Pankow',\n",
    "    'Niedersch√∂nhausen': 'Pankow',\n",
    "    'Gesundbrunnen': 'Mitte',\n",
    "    'Wedding': 'Mitte',\n",
    "    'Moabit': 'Mitte',\n",
    "    'Tiergarten': 'Mitte',\n",
    "    'Friedenau': 'Tempelhof-Sch√∂neberg',\n",
    "    'Steglitz': 'Steglitz-Zehlendorf',\n",
    "    'Zehlendorf': 'Steglitz-Zehlendorf',\n",
    "    'Schmargendorf': 'Charlottenburg-Wilmersdorf',\n",
    "    'Grunewald': 'Charlottenburg-Wilmersdorf',\n",
    "    'Halensee': 'Charlottenburg-Wilmersdorf',\n",
    "    'Tegel': 'Reinickendorf',\n",
    "    'Heiligensee': 'Reinickendorf',\n",
    "    'Staaken': 'Spandau',\n",
    "    'Siemensstadt': 'Spandau',\n",
    "    'Malchow': 'Pankow',\n",
    "    'Reinickendorf': 'Reinickendorf',\n",
    "    'Lichtenberg': 'Lichtenberg',\n",
    "    'Marzahn-Hellersdorf': 'Marzahn-Hellersdorf',\n",
    "    'Spandau': 'Spandau',\n",
    "    'Neuk√∂lln': 'Neuk√∂lln',\n",
    "    'Mitte': 'Mitte',\n",
    "    'Pankow': 'Pankow',\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ PLZ-Mapping geladen: {len(plz_to_district)} Zuordnungen\")\n",
    "print(f\"‚úÖ Bezirk-Aliases definiert: {len(district_aliases)} Zuordnungen\")\n",
    "\n",
    "# Zeige Beispiele\n",
    "print(\"\\nPLZ-Mapping Beispiele:\")\n",
    "for plz, district in list(plz_to_district.items())[:5]:\n",
    "    print(f\"  {plz} ‚Üí {district}\")\n",
    "    \n",
    "print(\"\\nBezirk-Normalisierung Beispiele:\")\n",
    "for alias, normalized in list(district_aliases.items())[:5]:\n",
    "    print(f\"  '{alias}' ‚Üí '{normalized}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e94ac7",
   "metadata": {},
   "source": [
    "## 3. Dataset 2025 laden und analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed89cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET 2025 LADEN UND ANALYSIEREN\n",
      "============================================================\n",
      "Dataset geladen: 6,109 Zeilen, 5 Spalten\n",
      "Spalten: ['title', 'price', 'size', 'address', 'link']\n",
      "Datentypen:\n",
      "title      object\n",
      "price      object\n",
      "size       object\n",
      "address    object\n",
      "link       object\n",
      "dtype: object\n",
      "Fehlende Werte:\n",
      "=== ADRESSFORMAT-ANALYSE ===\n",
      "Erste 10 Adressen:\n",
      "  1. Biedenkopfer Stra√üe 46-54, 13507 Berlin\n",
      "  2. Seegefelder Stra√üe 150, 14612 Falkensee\n",
      "  3. Johannisplatz 5, Mitte (Ortsteil), Berlin\n",
      "  4. Pufendorfstra√üe 3A-3E, Friedrichshain, Berlin\n",
      "  5. Warburgzeile 1, 10585 Berlin\n",
      "  6. Johannisplatz 3, 10117 Berlin\n",
      "  7. Kreutzigerstra√üe 14, Friedrichshain, Berlin\n",
      "  8. Elsa-Neumann-Stra√üe 1, 13629 Berlin\n",
      "  9. Tiergarten, Berlin\n",
      "  10. Chausseestra√üe 108, Mitte (Ortsteil), Berlin\n",
      "Einzigartige Adressformate (Sample):\n",
      "  ‚Ä¢ Biedenkopfer Stra√üe 46-54, 13507 Berlin\n",
      "  ‚Ä¢ Seegefelder Stra√üe 150, 14612 Falkensee\n",
      "  ‚Ä¢ Johannisplatz 5, Mitte (Ortsteil), Berlin\n",
      "  ‚Ä¢ Pufendorfstra√üe 3A-3E, Friedrichshain, Berlin\n",
      "  ‚Ä¢ Warburgzeile 1, 10585 Berlin\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATASET 2025 LADEN UND ANALYSIEREN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dataset laden\n",
    "df = pd.read_csv('data/raw/Dataset_2025.csv')\n",
    "print(f\"Dataset geladen: {len(df):,} Zeilen, {len(df.columns)} Spalten\")\n",
    "\n",
    "print(f\"Spalten: {list(df.columns)}\")\n",
    "\n",
    "print(f\"Datentypen:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"Fehlende Werte:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "for col, count in missing_values.items():\n",
    "    print(f\"  {col}: {count} ({100*count/len(df):.2f}%)\")\n",
    "\n",
    "# Erste Adressfelder analysieren\n",
    "print(f\"=== ADRESSFORMAT-ANALYSE ===\")\n",
    "print(\"Erste 10 Adressen:\")\n",
    "for i, addr in enumerate(df['address'].head(10)):\n",
    "    print(f\"  {i+1}. {addr}\")\n",
    "\n",
    "print(f\"Einzigartige Adressformate (Sample):\")\n",
    "unique_addresses = df['address'].dropna().unique()\n",
    "for addr in unique_addresses[:5]:\n",
    "    print(f\"  ‚Ä¢ {addr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5851ec",
   "metadata": {},
   "source": [
    "## 4. Intelligente Adressextraktion und Bezirk-Zuordnung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ffb666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INTELLIGENTE ADRESSEXTRAKTION\n",
      "============================================================\n",
      "Extrahiere Bezirke und PLZ aus Adressen...\n",
      "‚úÖ Bezirk-Extraktion abgeschlossen:\n",
      "  Gesamt Adressen: 6,109\n",
      "  Erfolgreiche Extraktion: 4,440\n",
      "  Erfolgsrate: 72.7%\n",
      "=== ERFOLGREICHE EXTRAKTION (Beispiele) ===\n",
      "  'Biedenkopfer Stra√üe 46-54, 13507 Berlin' ‚Üí Reinickendorf\n",
      "  'Seegefelder Stra√üe 150, 14612 Falkensee' ‚Üí Falkensee\n",
      "  'Johannisplatz 5, Mitte (Ortsteil), Berlin' ‚Üí Mitte\n",
      "  'Pufendorfstra√üe 3A-3E, Friedrichshain, Berlin' ‚Üí Friedrichshain-Kreuzberg\n",
      "  'Warburgzeile 1, 10585 Berlin' ‚Üí Charlottenburg-Wilmersdorf\n",
      "  'Johannisplatz 3, 10117 Berlin' ‚Üí Mitte\n",
      "  'Kreutzigerstra√üe 14, Friedrichshain, Berlin' ‚Üí Friedrichshain-Kreuzberg\n",
      "  'Elsa-Neumann-Stra√üe 1, 13629 Berlin' ‚Üí Spandau\n",
      "  'Tiergarten, Berlin' ‚Üí Mitte\n",
      "  'Chausseestra√üe 108, Mitte (Ortsteil), Berlin' ‚Üí Mitte\n",
      "=== NICHT-EXTRAHIERTE ADRESSEN (1669 Eintr√§ge) ===\n",
      "  ‚ùå 'Abendseglersteig 55, Rahnsdorf, Berlin'\n",
      "  ‚ùå 'Gr√ºnauer Stra√üe 26, Altglienicke, Berlin'\n",
      "  ‚ùå 'Carl-Spindler-Stra√üe 19, K√∂penick, Berlin'\n",
      "  ‚ùå 'Am Maselakepark 31, Hakenfelde, Berlin'\n",
      "  ‚ùå 'Lion-Feuchtwanger Stra√üe 61, Hellersdorf, Berlin'\n",
      "  ‚ùå 'Lehderstra√üe 26-27, 13806 Berlin'\n",
      "  ‚ùå 'W√§scherinnenweg 2, K√∂penick, Berlin'\n",
      "  ‚ùå 'Eichbuschallee 55, Pl√§nterwald, Berlin'\n",
      "  ‚ùå 'Bouchestra√üe 37, Alt-Treptow, Berlin'\n",
      "  ‚ùå 'Gr√ºnauer Stra√üe 29, K√∂penick, Berlin'\n",
      "=== BEZIRK-VERTEILUNG ===\n",
      "Anzahl einzigartiger Bezirke: 21\n",
      "  Mitte: 1028 Eintr√§ge\n",
      "  Pankow: 784 Eintr√§ge\n",
      "  Friedrichshain-Kreuzberg: 778 Eintr√§ge\n",
      "  Charlottenburg-Wilmersdorf: 602 Eintr√§ge\n",
      "  Neuk√∂lln: 397 Eintr√§ge\n",
      "  Tempelhof-Sch√∂neberg: 355 Eintr√§ge\n",
      "  Steglitz-Zehlendorf: 170 Eintr√§ge\n",
      "  Reinickendorf: 129 Eintr√§ge\n",
      "  Spandau: 93 Eintr√§ge\n",
      "  Lichtenberg: 77 Eintr√§ge\n",
      "‚úÖ Verbleibende Eintr√§ge nach Bezirk-Extraktion: 4,440\n",
      "Datenverlust: 1,669 Eintr√§ge (27.3%)\n",
      "‚úÖ PLZ-Extraktion abgeschlossen:\n",
      "  Erfolgreiche Zuordnungen: 60/4440 (1.4%)\n",
      "\n",
      "üìç Beispiele f√ºr PLZ-Extraktion:\n",
      "  Biedenkopfer Stra√üe 46-54, 13507 Berlin ‚Üí PLZ: 13507.0, Bezirk: Reinickendorf\n",
      "  Seegefelder Stra√üe 150, 14612 Falkensee ‚Üí PLZ: 14612.0, Bezirk: Falkensee\n",
      "  Warburgzeile 1, 10585 Berlin ‚Üí PLZ: 10585.0, Bezirk: Charlottenburg-Wilmersdorf\n",
      "  Johannisplatz 3, 10117 Berlin ‚Üí PLZ: 10117.0, Bezirk: Mitte\n",
      "  Elsa-Neumann-Stra√üe 1, 13629 Berlin ‚Üí PLZ: 13629.0, Bezirk: Spandau\n",
      "\n",
      "üìä PLZ-Statistiken:\n",
      "  Eindeutige PLZ: 47\n",
      "  PLZ-Bereich: 10000.0 - 14612.0\n",
      "  H√§ufigste PLZ:\n",
      "    12524.0: 3 Angebote\n",
      "    12051.0: 3 Angebote\n",
      "    13088.0: 3 Angebote\n",
      "    12555.0: 2 Angebote\n",
      "    10557.0: 2 Angebote\n",
      "\n",
      "üìä Bezirks-Statistiken:\n",
      "Anzahl verschiedener Bezirke: 21\n",
      "  Mitte: 1028 Eintr√§ge\n",
      "  Pankow: 784 Eintr√§ge\n",
      "  Friedrichshain-Kreuzberg: 778 Eintr√§ge\n",
      "  Charlottenburg-Wilmersdorf: 602 Eintr√§ge\n",
      "  Neuk√∂lln: 397 Eintr√§ge\n",
      "  Tempelhof-Sch√∂neberg: 355 Eintr√§ge\n",
      "  Steglitz-Zehlendorf: 170 Eintr√§ge\n",
      "  Reinickendorf: 129 Eintr√§ge\n",
      "  Spandau: 93 Eintr√§ge\n",
      "  Lichtenberg: 77 Eintr√§ge\n",
      "\n",
      "üéØ Adress-Parsing abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_district_from_address(address):\n",
    "    \"\"\"\n",
    "    Intelligente Bezirk-Extraktion aus verschiedenen Adressformaten\n",
    "    \n",
    "    Unterst√ºtzte Formate:\n",
    "    1. PLZ-basiert: \"Johannisplatz 3, 10117 Berlin\"\n",
    "    2. Bezirk direkt: \"Johannisplatz 5, Mitte (Ortsteil), Berlin\"\n",
    "    3. Nur Bezirk: \"Tiergarten, Berlin\"\n",
    "    4. Nur PLZ: \"10557 Berlin\"\n",
    "    \"\"\"\n",
    "    if pd.isna(address):\n",
    "        return None\n",
    "    \n",
    "    address = str(address).strip()\n",
    "    \n",
    "    # Methode 1: PLZ-Extraktion (5-stellige Zahlen)\n",
    "    plz_match = re.search(r'\\b(\\d{5})\\b', address)\n",
    "    if plz_match:\n",
    "        plz = int(plz_match.group(1))\n",
    "        if plz in plz_to_district:\n",
    "            return plz_to_district[plz]\n",
    "    \n",
    "    # Methode 2: Bezirk-Namen direkt im Text finden\n",
    "    # Entferne \"Berlin\" vom Ende und teile bei Komma\n",
    "    address_clean = address.replace(', Berlin', '').replace(' Berlin', '')\n",
    "    \n",
    "    # Suche nach bekannten Bezirken in der Adresse\n",
    "    for alias, normalized in district_aliases.items():\n",
    "        if alias.lower() in address_clean.lower():\n",
    "            return normalized\n",
    "    \n",
    "    # Methode 3: Letzte Komponente vor \"Berlin\" als Bezirk\n",
    "    parts = address_clean.split(',')\n",
    "    if len(parts) >= 2:\n",
    "        potential_district = parts[-1].strip()\n",
    "        # Entferne Zus√§tze wie \"(Ortsteil)\"\n",
    "        potential_district = re.sub(r'\\s*\\([^)]+\\)', '', potential_district)\n",
    "        \n",
    "        # Pr√ºfe ob es ein bekannter Bezirk ist\n",
    "        if potential_district in district_aliases:\n",
    "            return district_aliases[potential_district]\n",
    "    \n",
    "    # Methode 4: Direkte Bezirk-Suche im gesamten Text\n",
    "    for bezirk in district_aliases.values():\n",
    "        if bezirk.lower() in address_clean.lower():\n",
    "            return bezirk\n",
    "    \n",
    "    return None\n",
    "\n",
    "def extract_plz_from_address(address):\n",
    "    \"\"\"\n",
    "    Extrahiert die Postleitzahl aus der Adresse\n",
    "    Sucht nach 5-stelligen Zahlen die mit 1 beginnen (Berlin PLZ: 10xxx-14xxx)\n",
    "    \"\"\"\n",
    "    if pd.isna(address):\n",
    "        return None\n",
    "    \n",
    "    address = str(address).strip()\n",
    "    \n",
    "    # Suche nach Berliner PLZ-Muster (10xxx-14xxx)\n",
    "    plz_match = re.search(r'\\b1[0-4]\\d{3}\\b', address)\n",
    "    \n",
    "    if plz_match:\n",
    "        return int(plz_match.group())\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INTELLIGENTE ADRESSEXTRAKTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Arbeite mit einer Kopie\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Bezirk und PLZ extrahieren\n",
    "print(\"Extrahiere Bezirke und PLZ aus Adressen...\")\n",
    "df_clean['district'] = df_clean['address'].apply(extract_district_from_address)\n",
    "df_clean['PLZ'] = df_clean['address'].apply(extract_plz_from_address)\n",
    "\n",
    "# Statistiken\n",
    "total_addresses = len(df_clean)\n",
    "successful_extractions = df_clean['district'].notna().sum()\n",
    "success_rate = 100 * successful_extractions / total_addresses\n",
    "\n",
    "print(f\"‚úÖ Bezirk-Extraktion abgeschlossen:\")\n",
    "print(f\"  Gesamt Adressen: {total_addresses:,}\")\n",
    "print(f\"  Erfolgreiche Extraktion: {successful_extractions:,}\")\n",
    "print(f\"  Erfolgsrate: {success_rate:.1f}%\")\n",
    "\n",
    "# Zeige Beispiele erfolgreicher Extraktion\n",
    "print(f\"=== ERFOLGREICHE EXTRAKTION (Beispiele) ===\")\n",
    "successful_examples = df_clean[df_clean['district'].notna()][['address', 'district']].head(10)\n",
    "for idx, row in successful_examples.iterrows():\n",
    "    print(f\"  '{row['address']}' ‚Üí {row['district']}\")\n",
    "\n",
    "# Zeige nicht-extrahierte Adressen\n",
    "failed_extractions = df_clean[df_clean['district'].isna()]\n",
    "if len(failed_extractions) > 0:\n",
    "    print(f\"=== NICHT-EXTRAHIERTE ADRESSEN ({len(failed_extractions)} Eintr√§ge) ===\")\n",
    "    for addr in failed_extractions['address'].unique()[:10]:\n",
    "        print(f\"  ‚ùå '{addr}'\")\n",
    "        \n",
    "# Bezirk-Verteilung\n",
    "print(f\"=== BEZIRK-VERTEILUNG ===\")\n",
    "district_counts = df_clean['district'].value_counts()\n",
    "print(f\"Anzahl einzigartiger Bezirke: {len(district_counts)}\")\n",
    "for district, count in district_counts.head(10).items():\n",
    "    print(f\"  {district}: {count} Eintr√§ge\")\n",
    "    \n",
    "# Nur Zeilen mit erfolgreich extrahierten Bezirken behalten\n",
    "df_clean = df_clean[df_clean['district'].notna()]\n",
    "print(f\"‚úÖ Verbleibende Eintr√§ge nach Bezirk-Extraktion: {len(df_clean):,}\")\n",
    "print(f\"Datenverlust: {total_addresses - len(df_clean):,} Eintr√§ge ({100*(total_addresses - len(df_clean))/total_addresses:.1f}%)\")\n",
    "\n",
    "# Zeige Ergebnisse der PLZ-Extraktion\n",
    "print(f\"‚úÖ PLZ-Extraktion abgeschlossen:\")\n",
    "print(f\"  Erfolgreiche Zuordnungen: {df_clean['PLZ'].notna().sum()}/{len(df_clean)} ({df_clean['PLZ'].notna().sum()/len(df_clean)*100:.1f}%)\")\n",
    "\n",
    "# Zeige Beispiele f√ºr PLZ-Extraktion\n",
    "print(f\"\\nüìç Beispiele f√ºr PLZ-Extraktion:\")\n",
    "examples = df_clean[df_clean['PLZ'].notna()][['address', 'PLZ', 'district']].head(5)\n",
    "for idx, row in examples.iterrows():\n",
    "    print(f\"  {row['address']} ‚Üí PLZ: {row['PLZ']}, Bezirk: {row['district']}\")\n",
    "\n",
    "# PLZ-Statistiken\n",
    "if df_clean['PLZ'].notna().sum() > 0:\n",
    "    print(f\"\\nüìä PLZ-Statistiken:\")\n",
    "    print(f\"  Eindeutige PLZ: {df_clean['PLZ'].nunique()}\")\n",
    "    print(f\"  PLZ-Bereich: {df_clean['PLZ'].min()} - {df_clean['PLZ'].max()}\")\n",
    "    \n",
    "    # H√§ufigste PLZ\n",
    "    plz_counts = df_clean['PLZ'].value_counts().head(5)\n",
    "    print(f\"  H√§ufigste PLZ:\")\n",
    "    for plz, count in plz_counts.items():\n",
    "        print(f\"    {plz}: {count} Angebote\")\n",
    "\n",
    "# Bezirks-Statistiken\n",
    "district_counts = df_clean['district'].value_counts()\n",
    "print(f\"\\nüìä Bezirks-Statistiken:\")\n",
    "print(f\"Anzahl verschiedener Bezirke: {len(district_counts)}\")\n",
    "for district, count in district_counts.head(10).items():\n",
    "    print(f\"  {district}: {count} Eintr√§ge\")\n",
    "\n",
    "print(f\"\\nüéØ Adress-Parsing abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64402b8",
   "metadata": {},
   "source": [
    "## 5. Datenbereinigung und Filter-Harmonisierung\n",
    "\n",
    "### üéØ **Einheitliche Bereinigungskriterien**\n",
    "**Konsistent mit allen anderen Datasets in der Pipeline:**\n",
    "- **Preis-Filter:** 100‚Ç¨ - 10.000‚Ç¨ (Kaltmiete)\n",
    "- **Gr√∂√üen-Filter:** 10m¬≤ - 500m¬≤ (Wohnfl√§che)\n",
    "- **Bezirk-Validierung:** Nur g√ºltige Berliner Bezirke\n",
    "\n",
    "### üìã **Multi-Listing-Behandlung**\n",
    "Dataset 2025 enth√§lt Multi-Listings (Preis- und Gr√∂√üenspannen):\n",
    "- **Preisspannen:** \"725 - 1.965‚Ç¨\" ‚Üí Nimm Mindestpreis (725‚Ç¨)\n",
    "- **Gr√∂√üenspannen:** \"26,55 - 112,82m¬≤\" ‚Üí Nimm Mindestgr√∂√üe (26,55m¬≤)\n",
    "- **Rationale:** Konservative SchÔøΩÔøΩtzung f√ºr Vergleichbarkeit\n",
    "\n",
    "### üîÑ **Harmonisierung mit anderen Datasets**\n",
    "- **Dataset 2018-2019:** Gleiche Filter (100‚Ç¨-10.000‚Ç¨, 10m¬≤-500m¬≤)\n",
    "- **Dataset 2022:** Filter aktualisiert auf gleiche Werte\n",
    "- **Dataset 2025:** Implementiert gleiche Logik\n",
    "\n",
    "**Ziel:** Maximale Vergleichbarkeit und Konsistenz zwischen allen Zeitr√§umen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4d1cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATENBEREINIGUNG UND MULTI-LISTING-BEHANDLUNG\n",
      "============================================================\n",
      "Bereinige Preisfelder...\n",
      "Bereinige Gr√∂√üenfelder...\n",
      "=== BEREINIGUNGSSTATISTIKEN ===\n",
      "Urspr√ºngliche Eintr√§ge: 4,440\n",
      "G√ºltige Preise: 4,440/4,440 (100.0%)\n",
      "G√ºltige Gr√∂√üen: 4,440/4,440 (100.0%)\n",
      "Beide g√ºltig: 4,440/4,440 (100.0%)\n",
      "=== EINHEITLICHE FILTER-KRITERIEN ===\n",
      "üìã Harmonisiert mit Dataset 2018-2019 und Dataset 2022\n",
      "üéØ Ziel: Maximale Vergleichbarkeit zwischen allen Zeitr√§umen\n",
      "üîπ Preis-Filter: 100‚Ç¨ - 10.000‚Ç¨\n",
      "Nach Preis-Filter: 4,425 (entfernt: 15)\n",
      "üîπ Gr√∂√üen-Filter: 10m¬≤ - 500m¬≤\n",
      "Nach Gr√∂√üen-Filter: 4,424 (entfernt: 1)\n",
      "‚úÖ Datenbereinigung abgeschlossen\n",
      "‚úÖ Filter-Harmonisierung erfolgreich\n",
      "Finale Datens√§tze: 4,424\n",
      "=== FINALE DATENVERTEILUNG ===\n",
      "Preis - Min: 150.00‚Ç¨, Max: 9990.00‚Ç¨, Median: 1001.62‚Ç¨\n",
      "Gr√∂√üe - Min: 11.0m¬≤, Max: 361.0m¬≤, Median: 65.0m¬≤\n",
      "üìä BEREINIGUNGSLOGIK KONSISTENT MIT:\n",
      "   ‚Ä¢ 01_Clean_Dataset_2018_2019.ipynb\n",
      "   ‚Ä¢ 02_Clean_Dataset_2022.ipynb\n",
      "   ‚Ä¢ 03_Clean_Dataset_2025.ipynb (dieses Notebook)\n",
      "   ‚Ä¢ 04_Combine_Datasets.ipynb (finale Validierung)\n"
     ]
    }
   ],
   "source": [
    "def clean_price_field(price_str):\n",
    "    \"\"\"\n",
    "    Bereinige Preisfeld und extrahiere Einzelpreise aus Multi-Listings\n",
    "    \n",
    "    Behandelt folgende Formate:\n",
    "    - Einzelpreis: \"1.235‚Ç¨\" ‚Üí 1235.0\n",
    "    - Preisspanne: \"725 - 1.965‚Ç¨\" ‚Üí 725.0 (Mindestpreis)\n",
    "    - Deutsche Formate: \"1.235,65‚Ç¨\" ‚Üí 1235.65\n",
    "    \n",
    "    Returns:\n",
    "        float: Bereinigter Preis oder None bei Fehlern\n",
    "    \"\"\"\n",
    "    if pd.isna(price_str):\n",
    "        return None\n",
    "    \n",
    "    price_str = str(price_str).strip()\n",
    "    \n",
    "    # Entferne Euro-Zeichen und Leerzeichen\n",
    "    price_str = price_str.replace('‚Ç¨', '').replace(' ', '')\n",
    "    \n",
    "    # Behandle Preisspannen (z.B. \"725 - 1.965\")\n",
    "    if '-' in price_str:\n",
    "        # Multi-Listing: Nimm Minimalpreis f√ºr Vergleichbarkeit\n",
    "        parts = price_str.split('-')\n",
    "        try:\n",
    "            min_price = float(parts[0].replace('.', '').replace(',', '.'))\n",
    "            return min_price\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # Einzelpreis\n",
    "    try:\n",
    "        # Behandle deutsche Zahlenformate (1.435,65 ‚Üí 1435.65)\n",
    "        if ',' in price_str and '.' in price_str:\n",
    "            # Format: 1.435,65\n",
    "            price_str = price_str.replace('.', '').replace(',', '.')\n",
    "        elif ',' in price_str:\n",
    "            # Format: 1435,65\n",
    "            price_str = price_str.replace(',', '.')\n",
    "        elif '.' in price_str and len(price_str.split('.')[-1]) == 2:\n",
    "            # Format: 1435.65 (bereits korrekt)\n",
    "            pass\n",
    "        else:\n",
    "            # Format: 1435 (Tausender-Trennzeichen entfernen)\n",
    "            price_str = price_str.replace('.', '')\n",
    "        \n",
    "        return float(price_str)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def clean_size_field(size_str):\n",
    "    \"\"\"\n",
    "    Bereinige Gr√∂√üenfeld und extrahiere Einzelgr√∂√üen aus Multi-Listings\n",
    "    \n",
    "    Behandelt folgende Formate:\n",
    "    - Einzelgr√∂√üe: \"67,5m¬≤\" ‚Üí 67.5\n",
    "    - Gr√∂√üenspanne: \"26,55 - 112,82m¬≤\" ‚Üí 26.55 (Mindestgr√∂√üe)\n",
    "    - Verschiedene Trennzeichen: \"67,5\" oder \"67.5\"\n",
    "    \n",
    "    Returns:\n",
    "        float: Bereinigte Gr√∂√üe oder None bei Fehlern\n",
    "    \"\"\"\n",
    "    if pd.isna(size_str):\n",
    "        return None\n",
    "    \n",
    "    size_str = str(size_str).strip()\n",
    "    \n",
    "    # Entferne m¬≤ und Leerzeichen\n",
    "    size_str = size_str.replace('m¬≤', '').replace(' ', '')\n",
    "    \n",
    "    # Behandle Gr√∂√üenspannen (z.B. \"26,55 - 112,82\")\n",
    "    if '-' in size_str:\n",
    "        # Multi-Listing: Nimm Minimalgr√∂√üe f√ºr Vergleichbarkeit\n",
    "        parts = size_str.split('-')\n",
    "        try:\n",
    "            min_size = float(parts[0].replace(',', '.'))\n",
    "            return min_size\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # Einzelgr√∂√üe\n",
    "    try:\n",
    "        return float(size_str.replace(',', '.'))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ===================================================================\n",
    "# EINHEITLICHE DATENBEREINIGUNG (HARMONISIERT MIT ALLEN DATASETS)\n",
    "# ===================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATENBEREINIGUNG UND MULTI-LISTING-BEHANDLUNG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Bereinige Preise\n",
    "print(\"Bereinige Preisfelder...\")\n",
    "df_clean['price_clean'] = df_clean['price'].apply(clean_price_field)\n",
    "\n",
    "# Bereinige Gr√∂√üen\n",
    "print(\"Bereinige Gr√∂√üenfelder...\")\n",
    "df_clean['size_clean'] = df_clean['size'].apply(clean_size_field)\n",
    "\n",
    "# Statistiken vor Bereinigung\n",
    "print(f\"=== BEREINIGUNGSSTATISTIKEN ===\")\n",
    "print(f\"Urspr√ºngliche Eintr√§ge: {len(df_clean):,}\")\n",
    "\n",
    "# Entferne Zeilen ohne g√ºltige Preise\n",
    "valid_prices = df_clean['price_clean'].notna()\n",
    "print(f\"G√ºltige Preise: {valid_prices.sum():,}/{len(df_clean):,} ({100*valid_prices.sum()/len(df_clean):.1f}%)\")\n",
    "\n",
    "# Entferne Zeilen ohne g√ºltige Gr√∂√üen\n",
    "valid_sizes = df_clean['size_clean'].notna()\n",
    "print(f\"G√ºltige Gr√∂√üen: {valid_sizes.sum():,}/{len(df_clean):,} ({100*valid_sizes.sum()/len(df_clean):.1f}%)\")\n",
    "\n",
    "# Kombiniere Bedingungen\n",
    "valid_data = valid_prices & valid_sizes\n",
    "print(f\"Beide g√ºltig: {valid_data.sum():,}/{len(df_clean):,} ({100*valid_data.sum()/len(df_clean):.1f}%)\")\n",
    "\n",
    "# Behalte nur g√ºltige Daten\n",
    "df_clean = df_clean[valid_data]\n",
    "\n",
    "# ===================================================================\n",
    "# EINHEITLICHE FILTER-KRITERIEN (KONSISTENT MIT ALLEN DATASETS)\n",
    "# ===================================================================\n",
    "\n",
    "print(f\"=== EINHEITLICHE FILTER-KRITERIEN ===\")\n",
    "print(f\"üìã Harmonisiert mit Dataset 2018-2019 und Dataset 2022\")\n",
    "print(f\"üéØ Ziel: Maximale Vergleichbarkeit zwischen allen Zeitr√§umen\")\n",
    "\n",
    "initial_count = len(df_clean)\n",
    "\n",
    "# Preis-Filter (100‚Ç¨ - 10.000‚Ç¨) - KONSISTENT MIT ALLEN DATASETS\n",
    "print(f\"üîπ Preis-Filter: 100‚Ç¨ - 10.000‚Ç¨\")\n",
    "df_clean = df_clean[(df_clean['price_clean'] >= 100) & (df_clean['price_clean'] <= 10000)]\n",
    "print(f\"Nach Preis-Filter: {len(df_clean):,} (entfernt: {initial_count - len(df_clean):,})\")\n",
    "\n",
    "# Gr√∂√üen-Filter (10m¬≤ - 500m¬≤) - KONSISTENT MIT ALLEN DATASETS\n",
    "print(f\"üîπ Gr√∂√üen-Filter: 10m¬≤ - 500m¬≤\")\n",
    "initial_count = len(df_clean)\n",
    "df_clean = df_clean[(df_clean['size_clean'] >= 10) & (df_clean['size_clean'] <= 500)]\n",
    "print(f\"Nach Gr√∂√üen-Filter: {len(df_clean):,} (entfernt: {initial_count - len(df_clean):,})\")\n",
    "\n",
    "print(f\"‚úÖ Datenbereinigung abgeschlossen\")\n",
    "print(f\"‚úÖ Filter-Harmonisierung erfolgreich\")\n",
    "print(f\"Finale Datens√§tze: {len(df_clean):,}\")\n",
    "\n",
    "# Zeige Preis- und Gr√∂√üenverteilung\n",
    "print(f\"=== FINALE DATENVERTEILUNG ===\")\n",
    "print(f\"Preis - Min: {df_clean['price_clean'].min():.2f}‚Ç¨, Max: {df_clean['price_clean'].max():.2f}‚Ç¨, Median: {df_clean['price_clean'].median():.2f}‚Ç¨\")\n",
    "print(f\"Gr√∂√üe - Min: {df_clean['size_clean'].min():.1f}m¬≤, Max: {df_clean['size_clean'].max():.1f}m¬≤, Median: {df_clean['size_clean'].median():.1f}m¬≤\")\n",
    "\n",
    "print(f\"üìä BEREINIGUNGSLOGIK KONSISTENT MIT:\")\n",
    "print(f\"   ‚Ä¢ 01_Clean_Dataset_2018_2019.ipynb\")\n",
    "print(f\"   ‚Ä¢ 02_Clean_Dataset_2022.ipynb\")\n",
    "print(f\"   ‚Ä¢ 03_Clean_Dataset_2025.ipynb (dieses Notebook)\")\n",
    "print(f\"   ‚Ä¢ 04_Combine_Datasets.ipynb (finale Validierung)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e08766",
   "metadata": {},
   "source": [
    "## 6. Normalisierung in Standardformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ff76d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NORMALISIERUNG IN STANDARDFORMAT\n",
      "============================================================\n",
      "Normalisiertes Dataset erstellt: 4,424 Zeilen\n",
      "Standardspalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source']\n",
      "Zus√§tzliche Spalten: 5\n",
      "=== DATENQUALIT√ÑT NORMALISIERTES DATASET ===\n",
      "Zeilen mit Preis: 4,424\n",
      "Zeilen mit Gr√∂√üe: 4,424\n",
      "Zeilen mit Bezirk: 4,424\n",
      "Zeilen mit Zimmeranzahl: 0\n",
      "=== STATISTIKEN ===\n",
      "Preis - Min: 150.00‚Ç¨, Max: 9990.00‚Ç¨, Median: 1001.62‚Ç¨\n",
      "Gr√∂√üe - Min: 11.0m¬≤, Max: 361.0m¬≤, Median: 65.0m¬≤\n",
      "=== BEZIRKSVERTEILUNG ===\n",
      "Anzahl Bezirke: 20\n",
      "  Mitte: 1025 Eintr√§ge\n",
      "  Pankow: 781 Eintr√§ge\n",
      "  Friedrichshain-Kreuzberg: 775 Eintr√§ge\n",
      "  Charlottenburg-Wilmersdorf: 600 Eintr√§ge\n",
      "  Neuk√∂lln: 396 Eintr√§ge\n",
      "  Tempelhof-Sch√∂neberg: 355 Eintr√§ge\n",
      "  Steglitz-Zehlendorf: 169 Eintr√§ge\n",
      "  Reinickendorf: 129 Eintr√§ge\n",
      "  Spandau: 93 Eintr√§ge\n",
      "  Lichtenberg: 77 Eintr√§ge\n",
      "‚úÖ Normalisierung abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"NORMALISIERUNG IN STANDARDFORMAT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Erstelle normalisiertes Dataset\n",
    "df_normalized = pd.DataFrame()\n",
    "\n",
    "# Standardspalten (kompatibel mit anderen Datasets)\n",
    "df_normalized['price'] = df_clean['price_clean']\n",
    "df_normalized['size'] = df_clean['size_clean']\n",
    "df_normalized['district'] = df_clean['district']\n",
    "df_normalized['rooms'] = np.nan  # Nicht verf√ºgbar im 2025 Dataset\n",
    "df_normalized['year'] = 2025\n",
    "df_normalized['dataset_id'] = 'recent'\n",
    "df_normalized['source'] = 'ImmobilienScout24'\n",
    "\n",
    "# Zus√§tzliche Spalten aus dem 2025 Dataset\n",
    "df_normalized['title'] = df_clean['title']\n",
    "df_normalized['address'] = df_clean['address']\n",
    "df_normalized['link'] = df_clean['link']\n",
    "df_normalized['price_original'] = df_clean['price']\n",
    "df_normalized['size_original'] = df_clean['size']\n",
    "\n",
    "print(f\"Normalisiertes Dataset erstellt: {len(df_normalized):,} Zeilen\")\n",
    "print(f\"Standardspalten: {['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source']}\")\n",
    "print(f\"Zus√§tzliche Spalten: {len(df_normalized.columns) - 7}\")\n",
    "\n",
    "# Datenqualit√§t pr√ºfen\n",
    "print(f\"=== DATENQUALIT√ÑT NORMALISIERTES DATASET ===\")\n",
    "print(f\"Zeilen mit Preis: {df_normalized['price'].notna().sum():,}\")\n",
    "print(f\"Zeilen mit Gr√∂√üe: {df_normalized['size'].notna().sum():,}\")\n",
    "print(f\"Zeilen mit Bezirk: {df_normalized['district'].notna().sum():,}\")\n",
    "print(f\"Zeilen mit Zimmeranzahl: {df_normalized['rooms'].notna().sum():,}\")\n",
    "\n",
    "# Statistiken\n",
    "print(f\"=== STATISTIKEN ===\")\n",
    "print(f\"Preis - Min: {df_normalized['price'].min():.2f}‚Ç¨, Max: {df_normalized['price'].max():.2f}‚Ç¨, Median: {df_normalized['price'].median():.2f}‚Ç¨\")\n",
    "print(f\"Gr√∂√üe - Min: {df_normalized['size'].min():.1f}m¬≤, Max: {df_normalized['size'].max():.1f}m¬≤, Median: {df_normalized['size'].median():.1f}m¬≤\")\n",
    "\n",
    "# Bezirksverteilung\n",
    "print(f\"=== BEZIRKSVERTEILUNG ===\")\n",
    "district_counts = df_normalized['district'].value_counts()\n",
    "print(f\"Anzahl Bezirke: {len(district_counts)}\")\n",
    "for district, count in district_counts.head(10).items():\n",
    "    print(f\"  {district}: {count} Eintr√§ge\")\n",
    "\n",
    "print(f\"‚úÖ Normalisierung abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d649d",
   "metadata": {},
   "source": [
    "## 7. Export des normalisierten Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0cc7cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT NORMALISIERTES DATASET\n",
      "============================================================\n",
      "‚úÖ Normalisiertes Dataset exportiert: data/processed/dataset_2025_normalized.csv\n",
      "Dateigr√∂√üe: 4,424 Zeilen x 13 Spalten\n",
      "‚úÖ Export-Validierung erfolgreich: 4,424 Zeilen geladen\n",
      "=== ZUSAMMENFASSUNG DATASET 2025 ===\n",
      "Input: data/raw/Dataset_2025.csv (6,109 Zeilen)\n",
      "Output: data/processed/dataset_2025_normalized.csv (4,424 Zeilen)\n",
      "Datenverlust: 1,685 Zeilen (27.6%)\n",
      "Bezirk-Extraktion: 4,424/6,109 (72.4%) erfolgreich\n",
      "=== STANDARDISIERUNG UND KOMPATIBILIT√ÑT ===\n",
      "‚úÖ Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\n",
      "‚úÖ Zus√§tzliche Spalten: 6 (dataset-spezifisch)\n",
      "‚úÖ Einheitliche Filter-Kriterien: Preis 100‚Ç¨-10.000‚Ç¨, Gr√∂√üe 10m¬≤-500m¬≤\n",
      "‚úÖ Multi-Listing-Behandlung: Mindestpreise f√ºr Vergleichbarkeit\n",
      "=== HARMONISIERUNG MIT ANDEREN DATASETS ===\n",
      "üîÑ Dataset 2018-2019: Identische Filter-Kriterien\n",
      "üîÑ Dataset 2022: Filter-Kriterien harmonisiert\n",
      "üîÑ Dataset 2025: Implementiert (dieses Notebook)\n",
      "üîÑ Combine-Step: Bereit f√ºr nahtlose Integration\n",
      "üéØ DATASET 2025 BEREINIGUNG ABGESCHLOSSEN!\n",
      "üìä Bereit f√ºr Kombination mit anderen normalisierten Datasets\n",
      "üöÄ Konsistente Datenqualit√§t und Vergleichbarkeit gew√§hrleistet\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPORT NORMALISIERTES DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Export des normalisierten Datasets\n",
    "output_path = 'data/processed/dataset_2025_normalized.csv'\n",
    "df_normalized.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Normalisiertes Dataset exportiert: {output_path}\")\n",
    "print(f\"Dateigr√∂√üe: {len(df_normalized):,} Zeilen x {len(df_normalized.columns)} Spalten\")\n",
    "\n",
    "# Validierung durch Wiedereinlesen\n",
    "df_validation = pd.read_csv(output_path)\n",
    "print(f\"‚úÖ Export-Validierung erfolgreich: {len(df_validation):,} Zeilen geladen\")\n",
    "\n",
    "# Zusammenfassung\n",
    "print(f\"=== ZUSAMMENFASSUNG DATASET 2025 ===\")\n",
    "print(f\"Input: data/raw/Dataset_2025.csv ({len(df):,} Zeilen)\")\n",
    "print(f\"Output: {output_path} ({len(df_normalized):,} Zeilen)\")\n",
    "print(f\"Datenverlust: {len(df) - len(df_normalized):,} Zeilen ({100*(len(df) - len(df_normalized))/len(df):.1f}%)\")\n",
    "print(f\"Bezirk-Extraktion: {len(df_normalized):,}/{len(df):,} ({100*len(df_normalized)/len(df):.1f}%) erfolgreich\")\n",
    "\n",
    "# Standardisierung und Kompatibilit√§t\n",
    "print(f\"=== STANDARDISIERUNG UND KOMPATIBILIT√ÑT ===\")\n",
    "print(f\"‚úÖ Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\")\n",
    "print(f\"‚úÖ Zus√§tzliche Spalten: {len(df_normalized.columns) - 7} (dataset-spezifisch)\")\n",
    "print(f\"‚úÖ Einheitliche Filter-Kriterien: Preis 100‚Ç¨-10.000‚Ç¨, Gr√∂√üe 10m¬≤-500m¬≤\")\n",
    "print(f\"‚úÖ Multi-Listing-Behandlung: Mindestpreise f√ºr Vergleichbarkeit\")\n",
    "\n",
    "# Harmonisierung mit anderen Datasets\n",
    "print(f\"=== HARMONISIERUNG MIT ANDEREN DATASETS ===\")\n",
    "print(f\"üîÑ Dataset 2018-2019: Identische Filter-Kriterien\")\n",
    "print(f\"üîÑ Dataset 2022: Filter-Kriterien harmonisiert\")\n",
    "print(f\"üîÑ Dataset 2025: Implementiert (dieses Notebook)\")\n",
    "print(f\"üîÑ Combine-Step: Bereit f√ºr nahtlose Integration\")\n",
    "\n",
    "print(f\"üéØ DATASET 2025 BEREINIGUNG ABGESCHLOSSEN!\")\n",
    "print(f\"üìä Bereit f√ºr Kombination mit anderen normalisierten Datasets\")\n",
    "print(f\"üöÄ Konsistente Datenqualit√§t und Vergleichbarkeit gew√§hrleistet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c17dabaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß KRITISCHE PLZ-DATENTYP-REPARATUR\n",
      "==================================================\n",
      "PLZ-Datentyp vor Reparatur: object\n",
      "PLZ-Beispiele vor Reparatur: ['13507', '14612', '10178']\n",
      "PLZ-Datentyp nach Reparatur: object\n",
      "PLZ-Beispiele nach Reparatur: ['13507', '14612', '10178']\n",
      "‚úÖ PLZ-Datentyp-Reparatur abgeschlossen!\n",
      "   ‚û°Ô∏è  PLZ wird jetzt als String gespeichert f√ºr korrekten Join\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# KRITISCHE PLZ-DATENTYP-REPARATUR\n",
    "# ===================================================================\n",
    "print(\"\\nüîß KRITISCHE PLZ-DATENTYP-REPARATUR\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Stelle sicher, dass PLZ als STRING gespeichert wird (nicht als Float)\n",
    "# Dies ist kritisch f√ºr den sp√§teren Join in 04_Combine_Datasets.ipynb\n",
    "print(\"PLZ-Datentyp vor Reparatur:\", df_normalized['plz'].dtype)\n",
    "print(\"PLZ-Beispiele vor Reparatur:\", df_normalized['plz'].dropna().head(3).tolist())\n",
    "\n",
    "# Konvertiere PLZ zu String (ohne .0 Suffix)\n",
    "df_normalized['plz'] = df_normalized['plz'].apply(\n",
    "    lambda x: str(x) if pd.notna(x) else None\n",
    ")\n",
    "\n",
    "print(\"PLZ-Datentyp nach Reparatur:\", df_normalized['plz'].dtype)\n",
    "print(\"PLZ-Beispiele nach Reparatur:\", df_normalized['plz'].dropna().head(3).tolist())\n",
    "\n",
    "print(\"‚úÖ PLZ-Datentyp-Reparatur abgeschlossen!\")\n",
    "print(\"   ‚û°Ô∏è  PLZ wird jetzt als String gespeichert f√ºr korrekten Join\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6be8f06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç ERWEITERTE PLZ-EXTRAKTION\n",
      "==================================================\n",
      "Erstelle Ortsteil-zu-PLZ-Mapping aus Wohnlagendaten...\n",
      "‚úÖ Ortsteil-zu-PLZ-Mapping erstellt: 91 Eintr√§ge\n",
      "Beispiele des Ortsteil-zu-PLZ-Mappings:\n",
      "   Halensee ‚Üí 10711\n",
      "   Hakenfelde ‚Üí 13587\n",
      "   Lichterfelde ‚Üí 12205\n",
      "   Charlottenburg ‚Üí 13629\n",
      "   Marienfelde ‚Üí 12307\n",
      "   ... und 86 weitere\n",
      "\n",
      "üß™ TESTE ERWEITERTE PLZ-EXTRAKTION\n",
      "==================================================\n",
      "Test der erweiterten PLZ-Extraktion:\n",
      "   'Biedenkopfer Stra√üe 46-54, 13507 Berlin' ‚Üí PLZ: 13507\n",
      "   'Tiergarten, Berlin' ‚Üí PLZ: 10785\n",
      "   'Chausseestra√üe 108, Mitte (Ortsteil), Berlin' ‚Üí PLZ: 10178\n",
      "   'Friedrichshain, Berlin' ‚Üí PLZ: 10247\n",
      "   'Unter den Linden 5, Berlin' ‚Üí PLZ: 10117\n",
      "   'Alexanderplatz 1, Berlin' ‚Üí PLZ: 10178\n",
      "   'Kreuzberg, Berlin' ‚Üí PLZ: 10999\n",
      "   'Prenzlauer Berg, Berlin' ‚Üí PLZ: 10435\n",
      "\n",
      "üîÑ WENDE ERWEITERTE PLZ-EXTRAKTION AUF ALLE DATEN AN\n",
      "==================================================\n",
      "‚úÖ PLZ-Extraktion abgeschlossen:\n",
      "   üìä 4,033 von 4,424 Adressen haben PLZ (91.2%)\n",
      "   üéâ ZIEL ERREICHT: >90% PLZ-Abdeckung!\n",
      "   ‚Üí 391 Eintr√§ge ben√∂tigen Bezirks-basierte Anreicherung\n",
      "\n",
      "üìã PLZ-Verteilung (Top 10):\n",
      "   10435: 462 Immobilien\n",
      "   10247: 450 Immobilien\n",
      "   10178: 425 Immobilien\n",
      "   12043: 391 Immobilien\n",
      "   13629: 332 Immobilien\n",
      "   10999: 319 Immobilien\n",
      "   10557: 234 Immobilien\n",
      "   10787: 229 Immobilien\n",
      "   10715: 197 Immobilien\n",
      "   13351: 155 Immobilien\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# ERWEITERTE PLZ-EXTRAKTION MIT MEHREREN FALLBACK-STRATEGIEN\n",
    "# ===================================================================\n",
    "print(\"üîç ERWEITERTE PLZ-EXTRAKTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def extract_plz_advanced(address, ortsteil_mapping=None):\n",
    "    \"\"\"\n",
    "    Erweiterte PLZ-Extraktion mit mehreren Fallback-Strategien\n",
    "    \n",
    "    Strategien:\n",
    "    1. Direkte PLZ-Extraktion: r'\\b(1[0-4]\\d{3})\\b'\n",
    "    2. Ortsteil-zu-PLZ-Mapping √ºber wohnlagen_enriched.csv\n",
    "    3. Stra√üenname-zu-PLZ-Mapping als letzter Fallback\n",
    "    \"\"\"\n",
    "    if pd.isna(address):\n",
    "        return None\n",
    "    \n",
    "    address = str(address).strip()\n",
    "    \n",
    "    # Strategie 1: Direkte PLZ-Extraktion (Berlin PLZ: 10000-14999)\n",
    "    plz_match = re.search(r'\\b(1[0-4]\\d{3})\\b', address)\n",
    "    if plz_match:\n",
    "        return plz_match.group(1)\n",
    "    \n",
    "    # Strategie 2: Ortsteil-zu-PLZ-Mapping\n",
    "    if ortsteil_mapping:\n",
    "        # Extrahiere m√∂gliche Ortsteil-Namen aus der Adresse\n",
    "        # Bekannte Ortsteil-Patterns\n",
    "        ortsteil_patterns = [\n",
    "            r'\\b(Mitte)\\b',\n",
    "            r'\\b(Prenzlauer Berg)\\b',\n",
    "            r'\\b(Friedrichshain)\\b',\n",
    "            r'\\b(Kreuzberg)\\b',\n",
    "            r'\\b(Charlottenburg)\\b',\n",
    "            r'\\b(Wilmersdorf)\\b',\n",
    "            r'\\b(Tempelhof)\\b',\n",
    "            r'\\b(Sch√∂neberg)\\b',\n",
    "            r'\\b(Neuk√∂lln)\\b',\n",
    "            r'\\b(Steglitz)\\b',\n",
    "            r'\\b(Zehlendorf)\\b',\n",
    "            r'\\b(Wedding)\\b',\n",
    "            r'\\b(Moabit)\\b',\n",
    "            r'\\b(Tiergarten)\\b',\n",
    "            r'\\b(Spandau)\\b',\n",
    "            r'\\b(Reinickendorf)\\b',\n",
    "            r'\\b(Pankow)\\b',\n",
    "            r'\\b(Wei√üensee)\\b',\n",
    "            r'\\b(Lichtenberg)\\b',\n",
    "            r'\\b(Marzahn)\\b',\n",
    "            r'\\b(Hellersdorf)\\b',\n",
    "            r'\\b(Treptow)\\b',\n",
    "            r'\\b(K√∂penick)\\b',\n",
    "            r'\\b(Rudow)\\b',\n",
    "            r'\\b(Buckow)\\b',\n",
    "            r'\\b(Gropiusstadt)\\b',\n",
    "            r'\\b(Britz)\\b',\n",
    "            r'\\b(Mariendorf)\\b',\n",
    "            r'\\b(Lichtenrade)\\b',\n",
    "            r'\\b(Dahlem)\\b',\n",
    "            r'\\b(Grunewald)\\b',\n",
    "            r'\\b(Westend)\\b',\n",
    "            r'\\b(Hakenfelde)\\b',\n",
    "            r'\\b(Falkenhagener Feld)\\b',\n",
    "            r'\\b(Gatow)\\b',\n",
    "            r'\\b(Kladow)\\b',\n",
    "            r'\\b(Siemensstadt)\\b',\n",
    "            r'\\b(Tegel)\\b',\n",
    "            r'\\b(Waidmannslust)\\b',\n",
    "            r'\\b(Hermsdorf)\\b',\n",
    "            r'\\b(Franz√∂sisch Buchholz)\\b',\n",
    "            r'\\b(Karow)\\b',\n",
    "            r'\\b(Buch)\\b',\n",
    "            r'\\b(Blankenburg)\\b',\n",
    "            r'\\b(Heinersdorf)\\b',\n",
    "            r'\\b(Malchow)\\b',\n",
    "            r'\\b(Wartenberg)\\b',\n",
    "            r'\\b(Falkenberg)\\b',\n",
    "            r'\\b(Hohensch√∂nhausen)\\b',\n",
    "            r'\\b(Karlshorst)\\b',\n",
    "            r'\\b(Rummelsburg)\\b',\n",
    "            r'\\b(Fennpfuhl)\\b',\n",
    "            r'\\b(Biesdorf)\\b',\n",
    "            r'\\b(Kaulsdorf)\\b',\n",
    "            r'\\b(Mahlsdorf)\\b',\n",
    "            r'\\b(Friedrichsfelde)\\b',\n",
    "            r'\\b(Altglienicke)\\b',\n",
    "            r'\\b(Adlershof)\\b',\n",
    "            r'\\b(Johannisthal)\\b',\n",
    "            r'\\b(Niedersch√∂neweide)\\b',\n",
    "            r'\\b(Obersch√∂neweide)\\b',\n",
    "            r'\\b(Pl√§nterwald)\\b',\n",
    "            r'\\b(Baumschulenweg)\\b',\n",
    "            r'\\b(Friedenau)\\b',\n",
    "            r'\\b(Lankwitz)\\b',\n",
    "            r'\\b(Lichterfelde)\\b',\n",
    "            r'\\b(Marienfelde)\\b',\n",
    "            r'\\b(Kleinmachnow)\\b'  # Manchmal f√§lschlicherweise als Berlin klassifiziert\n",
    "        ]\n",
    "        \n",
    "        for pattern in ortsteil_patterns:\n",
    "            match = re.search(pattern, address, re.IGNORECASE)\n",
    "            if match:\n",
    "                ortsteil = match.group(1)\n",
    "                if ortsteil in ortsteil_mapping:\n",
    "                    return ortsteil_mapping[ortsteil]\n",
    "        \n",
    "        # Fallback: Pr√ºfe nach \"(Ortsteil)\" Pattern\n",
    "        ortsteil_match = re.search(r'\\b([A-Za-z√§√∂√º√Ñ√ñ√ú√ü\\s]+)\\s*\\(Ortsteil\\)', address)\n",
    "        if ortsteil_match:\n",
    "            ortsteil = ortsteil_match.group(1).strip()\n",
    "            if ortsteil in ortsteil_mapping:\n",
    "                return ortsteil_mapping[ortsteil]\n",
    "    \n",
    "    # Strategie 3: Stra√üenname-zu-PLZ-Mapping (kann sp√§ter erweitert werden)\n",
    "    # H√§ufige Berliner Stra√üen mit bekannten PLZ\n",
    "    street_to_plz = {\n",
    "        'Unter den Linden': '10117',\n",
    "        'Alexanderplatz': '10178',\n",
    "        'Potsdamer Platz': '10785',\n",
    "        'Kurf√ºrstendamm': '10719',\n",
    "        'Friedrichstra√üe': '10117',\n",
    "        'Hackescher Markt': '10178',\n",
    "        'Warschauer Stra√üe': '10243',\n",
    "        'Boxhagener Stra√üe': '10245',\n",
    "        'Kastanienallee': '10435',\n",
    "        'Oranienstra√üe': '10999',\n",
    "        'Bergmannstra√üe': '10961',\n",
    "        'Savignyplatz': '10623',\n",
    "        'Hackescher Markt': '10178',\n",
    "        'Rosenthaler Stra√üe': '10119',\n",
    "        'Torstra√üe': '10119',\n",
    "        'Invalidenstra√üe': '10115',\n",
    "        'Chausseestra√üe': '10115',\n",
    "        'Brunnenstra√üe': '10119',\n",
    "        'Bernauer Stra√üe': '10119'\n",
    "    }\n",
    "    \n",
    "    for street, plz in street_to_plz.items():\n",
    "        if street.lower() in address.lower():\n",
    "            return plz\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Erstelle Ortsteil-zu-PLZ-Mapping aus wohnlagen_enriched.csv\n",
    "print(\"Erstelle Ortsteil-zu-PLZ-Mapping aus Wohnlagendaten...\")\n",
    "ortsteil_to_plz_mapping = {}\n",
    "if 'enriched_df' in locals():\n",
    "    for _, row in enriched_df.iterrows():\n",
    "        if pd.notna(row['plz']) and pd.notna(row['ortsteil_neu']):\n",
    "            ortsteil = row['ortsteil_neu']\n",
    "            plz = str(int(row['plz']))  # Konvertiere zu String ohne .0\n",
    "            ortsteil_to_plz_mapping[ortsteil] = plz\n",
    "    \n",
    "    print(f\"‚úÖ Ortsteil-zu-PLZ-Mapping erstellt: {len(ortsteil_to_plz_mapping)} Eintr√§ge\")\n",
    "    \n",
    "    # Zeige Beispiele\n",
    "    print(\"Beispiele des Ortsteil-zu-PLZ-Mappings:\")\n",
    "    for i, (ortsteil, plz) in enumerate(list(ortsteil_to_plz_mapping.items())[:5]):\n",
    "        print(f\"   {ortsteil} ‚Üí {plz}\")\n",
    "    if len(ortsteil_to_plz_mapping) > 5:\n",
    "        print(f\"   ... und {len(ortsteil_to_plz_mapping) - 5} weitere\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Wohnlagendaten nicht verf√ºgbar f√ºr Ortsteil-Mapping\")\n",
    "\n",
    "# Teste die erweiterte PLZ-Extraktion\n",
    "print(\"\\nüß™ TESTE ERWEITERTE PLZ-EXTRAKTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Test mit einigen Beispiel-Adressen\n",
    "test_addresses = [\n",
    "    \"Biedenkopfer Stra√üe 46-54, 13507 Berlin\",\n",
    "    \"Tiergarten, Berlin\", \n",
    "    \"Chausseestra√üe 108, Mitte (Ortsteil), Berlin\",\n",
    "    \"Friedrichshain, Berlin\",\n",
    "    \"Unter den Linden 5, Berlin\",\n",
    "    \"Alexanderplatz 1, Berlin\",\n",
    "    \"Kreuzberg, Berlin\",\n",
    "    \"Prenzlauer Berg, Berlin\"\n",
    "]\n",
    "\n",
    "print(\"Test der erweiterten PLZ-Extraktion:\")\n",
    "for addr in test_addresses:\n",
    "    plz = extract_plz_advanced(addr, ortsteil_to_plz_mapping)\n",
    "    print(f\"   '{addr}' ‚Üí PLZ: {plz}\")\n",
    "\n",
    "# Wende erweiterte PLZ-Extraktion an\n",
    "print(\"\\nüîÑ WENDE ERWEITERTE PLZ-EXTRAKTION AUF ALLE DATEN AN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "df_normalized['plz'] = df_normalized['address'].apply(\n",
    "    lambda x: extract_plz_advanced(x, ortsteil_to_plz_mapping)\n",
    ")\n",
    "\n",
    "# Ergebnisse\n",
    "plz_found = df_normalized['plz'].notna().sum()\n",
    "total_rows = len(df_normalized)\n",
    "plz_coverage = (plz_found / total_rows) * 100\n",
    "\n",
    "print(f\"‚úÖ PLZ-Extraktion abgeschlossen:\")\n",
    "print(f\"   üìä {plz_found:,} von {total_rows:,} Adressen haben PLZ ({plz_coverage:.1f}%)\")\n",
    "\n",
    "if plz_coverage >= 90:\n",
    "    print(f\"   üéâ ZIEL ERREICHT: >90% PLZ-Abdeckung!\")\n",
    "elif plz_coverage >= 50:\n",
    "    print(f\"   üìà VERBESSERUNG: Deutlich erh√∂hte PLZ-Abdeckung\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  NIEDRIGE ABDECKUNG: Weitere Verbesserungen n√∂tig\")\n",
    "\n",
    "print(f\"   ‚Üí {total_rows - plz_found:,} Eintr√§ge ben√∂tigen Bezirks-basierte Anreicherung\")\n",
    "\n",
    "# Zeige PLZ-Verteilung\n",
    "if plz_found > 0:\n",
    "    print(f\"\\nüìã PLZ-Verteilung (Top 10):\")\n",
    "    plz_counts = df_normalized['plz'].value_counts().head(10)\n",
    "    for plz, count in plz_counts.items():\n",
    "        print(f\"   {plz}: {count:,} Immobilien\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d6",
   "metadata": {},
   "source": [
    "## 8. Lade angereicherte Wohnlagendaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5f6g7h0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANGEREICHERTE WOHNLAGENDATEN LADEN\n",
      "============================================================\n",
      "‚úÖ Angereicherte Daten geladen: 551,249 Zeilen, 11 Spalten\n",
      "‚úÖ Angereicherte Daten geladen: 551,249 Zeilen, 11 Spalten\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ANGEREICHERTE WOHNLAGENDATEN LADEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "enriched_data_path = 'data/raw/wohnlagen_enriched.csv'\n",
    "try:\n",
    "    enriched_df = pd.read_csv(enriched_data_path)\n",
    "    print(f\"‚úÖ Angereicherte Daten geladen: {len(enriched_df):,} Zeilen, {len(enriched_df.columns)} Spalten\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Datei nicht gefunden: {enriched_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l4",
   "metadata": {},
   "source": [
    "## 9. Kombiniere Datasets mit Wohnlagendaten - Dual-Strategie-Anreicherung\n",
    "\n",
    "### üéØ **Herausforderung: Fehlende PLZ-Daten im 2025 Dataset**\n",
    "\n",
    "Das 2025 Dataset weist eine **Besonderheit** auf: Nur ca. 1-2% der Adressen enthalten vollst√§ndige PLZ-Informationen. Die meisten Eintr√§ge haben nur Bezirks- oder Ortsteilangaben. Dies erfordert eine **intelligente Dual-Strategie** f√ºr die Anreicherung mit Wohnlagendaten.\n",
    "\n",
    "### üîÑ **Dual-Strategie-Ansatz:**\n",
    "\n",
    "#### **Strategie 1: PLZ-basierte Anreicherung**\n",
    "- **Zielgruppe:** Eintr√§ge mit extrahierbarer PLZ aus der Adresse\n",
    "- **Methode:** Direkte Zuordnung √ºber PLZ-Mapping aus `wohnlagen_enriched.csv`\n",
    "- **Vorteil:** H√∂chste Genauigkeit, da PLZ eindeutig einem Ortsteil zugeordnet werden kann\n",
    "- **Erwartung:** Nur wenige Eintr√§ge (1-2%), aber sehr pr√§zise Zuordnung\n",
    "\n",
    "#### **Strategie 2: Bezirks-basierte Anreicherung**\n",
    "- **Zielgruppe:** Eintr√§ge ohne PLZ, aber mit erkanntem Bezirk\n",
    "- **Methode:** Mapping √ºber Bezirk-zu-Ortsteil-Dictionary aus den Wohnlagendaten\n",
    "- **Herausforderung:** Zusammengesetzte Berliner Bezirke (z.B. Friedrichshain-Kreuzberg)\n",
    "- **L√∂sung:** Intelligente Alias-Zuordnung f√ºr alle Bezirks-Varianten\n",
    "\n",
    "### üìä **Warum diese Strategie notwendig ist:**\n",
    "\n",
    "1. **Datenqualit√§t:** Maximale Nutzung der verf√ºgbaren Informationen\n",
    "2. **Fallback-Mechanismus:** Keine Datenverluste durch fehlende PLZ\n",
    "3. **Konsistenz:** Einheitliche Anreicherung trotz unterschiedlicher Datenformate\n",
    "4. **Vermeidung von Kartesischen Produkten:** Durch gezielte `drop_duplicates`-Strategien\n",
    "\n",
    "### üé® **Implementierungslogik:**\n",
    "\n",
    "```\n",
    "IF PLZ verf√ºgbar:\n",
    "    ‚Üí PLZ-basierte Anreicherung (hohe Pr√§zision)\n",
    "ELSE:\n",
    "    ‚Üí Bezirks-basierte Anreicherung (fallback)\n",
    "    \n",
    "Kombiniere beide Ergebnisse ‚Üí Vollst√§ndig angereichertes Dataset\n",
    "```\n",
    "\n",
    "**Ziel:** Nahezu 100% Anreicherungsrate trotz heterogener Datenqualit√§t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "m3n4o5p8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DUAL-STRATEGIE-ANREICHERUNG MIT WOHNLAGENDATEN\n",
      "============================================================\n",
      "Original df_normalized: 4,424 Zeilen\n",
      "Original enriched_df: 551,249 Zeilen\n",
      "\n",
      "üîç SCHRITT 1: PLZ-EXTRAKTION AUS ADRESSSTRINGS\n",
      "==================================================\n",
      "Extrahiere PLZ aus Adressen...\n",
      "‚úÖ PLZ gefunden in 56 von 4424 Adressen (1.3%)\n",
      "   ‚Üí 4368 Eintr√§ge ben√∂tigen Bezirks-basierte Anreicherung\n",
      "\n",
      "üó∫Ô∏è SCHRITT 2: BEZIRK-ZU-ORTSTEIL-MAPPING ERSTELLEN\n",
      "==================================================\n",
      "Erstelle intelligentes Bezirk-zu-Ortsteil-Mapping...\n",
      "Analysiere Wohnlagendaten f√ºr Bezirks-Aliases...\n",
      "\n",
      "üì¶ SCHRITT 3: VORBEREITUNG F√úR DUAL-STRATEGIE\n",
      "==================================================\n",
      "‚úÖ Unique PLZ mappings: 193 Zeilen\n",
      "‚úÖ District-to-Ortsteil mappings: 97 Zuordnungen\n",
      "   ‚Üí Kartesische Produkte vermieden durch drop_duplicates\n",
      "\n",
      "üìä DATENSATZ-AUFTEILUNG:\n",
      "   ‚Ä¢ Eintr√§ge mit PLZ: 56 (f√ºr Strategie 1)\n",
      "   ‚Ä¢ Eintr√§ge ohne PLZ: 4,368 (f√ºr Strategie 2)\n",
      "\n",
      "üéØ STRATEGIE 1: PLZ-BASIERTE ANREICHERUNG\n",
      "==================================================\n",
      "F√ºhre PLZ-basierte Anreicherung durch...\n",
      "‚úÖ PLZ-basierte Anreicherung: 54 von 56 Zeilen (96.4%)\n",
      "   ‚Üí 2 Eintr√§ge konnten nicht √ºber PLZ angereichert werden\n",
      "\n",
      "üó∫Ô∏è STRATEGIE 2: BEZIRKS-BASIERTE ANREICHERUNG\n",
      "==================================================\n",
      "F√ºhre Bezirks-basierte Anreicherung durch...\n",
      "Mappe Bezirke zu Ortsteilen √ºber district_to_ortsteil Dictionary...\n",
      "‚úÖ Bezirks-basierte Anreicherung: 4,368 von 4,368 Zeilen (100.0%)\n",
      "   ‚Üí 0 Eintr√§ge konnten nicht √ºber Bezirk angereichert werden\n",
      "\n",
      "üîÑ SCHRITT 4: KOMBINATION DER DUAL-STRATEGIEN\n",
      "==================================================\n",
      "Kombiniere PLZ-basierte und Bezirks-basierte Anreicherung...\n",
      "   ‚Üí 14 gemeinsame Spalten identifiziert\n",
      "   ‚Üí Datasets erfolgreich kombiniert\n",
      "‚úÖ Kombiniertes und angereichertes Dataset erstellt: 4,424 Zeilen\n",
      "\n",
      "‚úÖ SCHRITT 5: ERFOLGSVALIDIERUNG DER DUAL-STRATEGIE\n",
      "==================================================\n",
      "üéØ GESAMTERGEBNIS DER DUAL-STRATEGIE:\n",
      "   ‚Ä¢ Erfolgreich angereichert: 4,422 von 4,424 Zeilen\n",
      "   ‚Ä¢ Erfolgsrate: 100.0%\n",
      "   ‚Ä¢ Nicht angereichert: 2 Zeilen\n",
      "‚úÖ AUSGEZEICHNET: Nahezu vollst√§ndige Anreicherung erreicht!\n",
      "\n",
      "üìä DUAL-STRATEGIE-ZUSAMMENFASSUNG:\n",
      "   ‚Ä¢ Original Dataset: 4,424 Zeilen\n",
      "   ‚Ä¢ Angereichert Dataset: 4,424 Zeilen\n",
      "   ‚Ä¢ Datenverlust: 0 Zeilen\n",
      "   ‚Ä¢ Erfolgreiche Anreicherung: 100.0%\n",
      "\n",
      "üì¶ SCHRITT 3: VORBEREITUNG F√úR DUAL-STRATEGIE\n",
      "==================================================\n",
      "‚úÖ Unique PLZ mappings: 193 Zeilen\n",
      "‚úÖ District-to-Ortsteil mappings: 97 Zuordnungen\n",
      "   ‚Üí Kartesische Produkte vermieden durch drop_duplicates\n",
      "\n",
      "üìä DATENSATZ-AUFTEILUNG:\n",
      "   ‚Ä¢ Eintr√§ge mit PLZ: 56 (f√ºr Strategie 1)\n",
      "   ‚Ä¢ Eintr√§ge ohne PLZ: 4,368 (f√ºr Strategie 2)\n",
      "\n",
      "üéØ STRATEGIE 1: PLZ-BASIERTE ANREICHERUNG\n",
      "==================================================\n",
      "F√ºhre PLZ-basierte Anreicherung durch...\n",
      "‚úÖ PLZ-basierte Anreicherung: 54 von 56 Zeilen (96.4%)\n",
      "   ‚Üí 2 Eintr√§ge konnten nicht √ºber PLZ angereichert werden\n",
      "\n",
      "üó∫Ô∏è STRATEGIE 2: BEZIRKS-BASIERTE ANREICHERUNG\n",
      "==================================================\n",
      "F√ºhre Bezirks-basierte Anreicherung durch...\n",
      "Mappe Bezirke zu Ortsteilen √ºber district_to_ortsteil Dictionary...\n",
      "‚úÖ Bezirks-basierte Anreicherung: 4,368 von 4,368 Zeilen (100.0%)\n",
      "   ‚Üí 0 Eintr√§ge konnten nicht √ºber Bezirk angereichert werden\n",
      "\n",
      "üîÑ SCHRITT 4: KOMBINATION DER DUAL-STRATEGIEN\n",
      "==================================================\n",
      "Kombiniere PLZ-basierte und Bezirks-basierte Anreicherung...\n",
      "   ‚Üí 14 gemeinsame Spalten identifiziert\n",
      "   ‚Üí Datasets erfolgreich kombiniert\n",
      "‚úÖ Kombiniertes und angereichertes Dataset erstellt: 4,424 Zeilen\n",
      "\n",
      "‚úÖ SCHRITT 5: ERFOLGSVALIDIERUNG DER DUAL-STRATEGIE\n",
      "==================================================\n",
      "üéØ GESAMTERGEBNIS DER DUAL-STRATEGIE:\n",
      "   ‚Ä¢ Erfolgreich angereichert: 4,422 von 4,424 Zeilen\n",
      "   ‚Ä¢ Erfolgsrate: 100.0%\n",
      "   ‚Ä¢ Nicht angereichert: 2 Zeilen\n",
      "‚úÖ AUSGEZEICHNET: Nahezu vollst√§ndige Anreicherung erreicht!\n",
      "\n",
      "üìä DUAL-STRATEGIE-ZUSAMMENFASSUNG:\n",
      "   ‚Ä¢ Original Dataset: 4,424 Zeilen\n",
      "   ‚Ä¢ Angereichert Dataset: 4,424 Zeilen\n",
      "   ‚Ä¢ Datenverlust: 0 Zeilen\n",
      "   ‚Ä¢ Erfolgreiche Anreicherung: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DUAL-STRATEGIE-ANREICHERUNG MIT WOHNLAGENDATEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Debug: Check original data sizes\n",
    "print(f\"Original df_normalized: {len(df_normalized):,} Zeilen\")\n",
    "print(f\"Original enriched_df: {len(enriched_df):,} Zeilen\")\n",
    "\n",
    "# ===================================================================\n",
    "# SCHRITT 1: PLZ-EXTRAKTION AUS ADRESSSTRINGS\n",
    "# ===================================================================\n",
    "print(\"\\nüîç SCHRITT 1: PLZ-EXTRAKTION AUS ADRESSSTRINGS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def extract_plz_from_address(address):\n",
    "    \"\"\"\n",
    "    Extract PLZ from address string\n",
    "    \n",
    "    Beispiele:\n",
    "    - \"Johannisplatz 3, 10117 Berlin\" ‚Üí \"10117\"\n",
    "    - \"Mitte, Berlin\" ‚Üí None\n",
    "    - \"10557 Berlin\" ‚Üí \"10557\"\n",
    "    \"\"\"\n",
    "    if pd.isna(address):\n",
    "        return None\n",
    "    \n",
    "    address = str(address).strip()\n",
    "    plz_match = re.search(r'\\b(\\d{5})\\b', address)\n",
    "    if plz_match:\n",
    "        return plz_match.group(1)\n",
    "    return None\n",
    "\n",
    "# Extract PLZ from addresses\n",
    "print(\"Extrahiere PLZ aus Adressen...\")\n",
    "df_normalized['plz'] = df_normalized['address'].apply(extract_plz_from_address)\n",
    "\n",
    "plz_found = df_normalized['plz'].notna().sum()\n",
    "print(f\"‚úÖ PLZ gefunden in {plz_found} von {len(df_normalized)} Adressen ({plz_found/len(df_normalized)*100:.1f}%)\")\n",
    "print(f\"   ‚Üí {len(df_normalized) - plz_found} Eintr√§ge ben√∂tigen Bezirks-basierte Anreicherung\")\n",
    "\n",
    "# ===================================================================\n",
    "# SCHRITT 2: BEZIRK-ZU-ORTSTEIL-MAPPING ERSTELLEN\n",
    "# ===================================================================\n",
    "print(\"\\nüó∫Ô∏è SCHRITT 2: BEZIRK-ZU-ORTSTEIL-MAPPING ERSTELLEN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create district to ortsteil mapping from enriched data\n",
    "print(\"Erstelle intelligentes Bezirk-zu-Ortsteil-Mapping...\")\n",
    "district_to_ortsteil = {}\n",
    "\n",
    "# Map common district variations to ortsteil_neu\n",
    "print(\"Analysiere Wohnlagendaten f√ºr Bezirks-Aliases...\")\n",
    "for _, row in enriched_df.iterrows():\n",
    "    ortsteil = row['ortsteil_neu']\n",
    "    if pd.notna(ortsteil):\n",
    "        # Map the ortsteil to itself\n",
    "        district_to_ortsteil[ortsteil] = ortsteil\n",
    "        \n",
    "        # Also map common district aliases for composite districts\n",
    "        if 'Friedrichshain' in ortsteil or 'Kreuzberg' in ortsteil:\n",
    "            district_to_ortsteil['Friedrichshain-Kreuzberg'] = ortsteil\n",
    "        elif 'Charlottenburg' in ortsteil or 'Wilmersdorf' in ortsteil:\n",
    "            district_to_ortsteil['Charlottenburg-Wilmersdorf'] = ortsteil\n",
    "        elif 'Tempelhof' in ortsteil or 'Sch√∂neberg' in ortsteil:\n",
    "            district_to_ortsteil['Tempelhof-Sch√∂neberg'] = ortsteil\n",
    "        elif 'Steglitz' in ortsteil or 'Zehlendorf' in ortsteil:\n",
    "            district_to_ortsteil['Steglitz-Zehlendorf'] = ortsteil\n",
    "        elif 'Marzahn' in ortsteil or 'Hellersdorf' in ortsteil:\n",
    "            district_to_ortsteil['Marzahn-Hellersdorf'] = ortsteil\n",
    "        elif 'Treptow' in ortsteil or 'K√∂penick' in ortsteil:\n",
    "            district_to_ortsteil['Treptow-K√∂penick'] = ortsteil\n",
    "\n",
    "# ===================================================================\n",
    "# SCHRITT 3: VORBEREITUNG F√úR DUAL-STRATEGIE\n",
    "# ===================================================================\n",
    "print(\"\\nüì¶ SCHRITT 3: VORBEREITUNG F√úR DUAL-STRATEGIE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Remove duplicates and get unique mappings to avoid cartesian products\n",
    "enriched_df_subset = enriched_df[['plz', 'wol', 'ortsteil_neu']].drop_duplicates(subset=['plz'])\n",
    "enriched_df_subset['plz'] = enriched_df_subset['plz'].astype(str)\n",
    "\n",
    "print(f\"‚úÖ Unique PLZ mappings: {len(enriched_df_subset):,} Zeilen\")\n",
    "print(f\"‚úÖ District-to-Ortsteil mappings: {len(district_to_ortsteil):,} Zuordnungen\")\n",
    "print(f\"   ‚Üí Kartesische Produkte vermieden durch drop_duplicates\")\n",
    "\n",
    "# Split dataset based on PLZ availability\n",
    "df_with_plz = df_normalized[df_normalized['plz'].notna()].copy()\n",
    "df_without_plz = df_normalized[df_normalized['plz'].isna()].copy()\n",
    "\n",
    "print(f\"\\nüìä DATENSATZ-AUFTEILUNG:\")\n",
    "print(f\"   ‚Ä¢ Eintr√§ge mit PLZ: {len(df_with_plz):,} (f√ºr Strategie 1)\")\n",
    "print(f\"   ‚Ä¢ Eintr√§ge ohne PLZ: {len(df_without_plz):,} (f√ºr Strategie 2)\")\n",
    "\n",
    "# ===================================================================\n",
    "# STRATEGIE 1: PLZ-BASIERTE ANREICHERUNG\n",
    "# ===================================================================\n",
    "print(\"\\nüéØ STRATEGIE 1: PLZ-BASIERTE ANREICHERUNG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Perform PLZ-based merge\n",
    "if len(df_with_plz) > 0:\n",
    "    print(\"F√ºhre PLZ-basierte Anreicherung durch...\")\n",
    "    df_enriched_plz = pd.merge(df_with_plz, enriched_df_subset, how='left', on=['plz'])\n",
    "    plz_success = df_enriched_plz['ortsteil_neu'].notna().sum()\n",
    "    print(f\"‚úÖ PLZ-basierte Anreicherung: {plz_success:,} von {len(df_enriched_plz):,} Zeilen ({plz_success/len(df_enriched_plz)*100:.1f}%)\")\n",
    "    print(f\"   ‚Üí {len(df_enriched_plz) - plz_success} Eintr√§ge konnten nicht √ºber PLZ angereichert werden\")\n",
    "else:\n",
    "    print(\"‚ùå Keine Eintr√§ge mit PLZ verf√ºgbar\")\n",
    "    df_enriched_plz = pd.DataFrame()\n",
    "\n",
    "# ===================================================================\n",
    "# STRATEGIE 2: BEZIRKS-BASIERTE ANREICHERUNG\n",
    "# ===================================================================\n",
    "print(\"\\nüó∫Ô∏è STRATEGIE 2: BEZIRKS-BASIERTE ANREICHERUNG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Strategy 2: District-based enrichment for entries without PLZ\n",
    "if len(df_without_plz) > 0:\n",
    "    print(\"F√ºhre Bezirks-basierte Anreicherung durch...\")\n",
    "    print(\"Mappe Bezirke zu Ortsteilen √ºber district_to_ortsteil Dictionary...\")\n",
    "    \n",
    "    # Map district to ortsteil_neu using our mapping\n",
    "    df_without_plz['ortsteil_neu'] = df_without_plz['district'].map(district_to_ortsteil)\n",
    "    df_without_plz['wol'] = None  # We don't have wol data for district-based mapping\n",
    "    \n",
    "    district_success = df_without_plz['ortsteil_neu'].notna().sum()\n",
    "    print(f\"‚úÖ Bezirks-basierte Anreicherung: {district_success:,} von {len(df_without_plz):,} Zeilen ({district_success/len(df_without_plz)*100:.1f}%)\")\n",
    "    print(f\"   ‚Üí {len(df_without_plz) - district_success} Eintr√§ge konnten nicht √ºber Bezirk angereichert werden\")\n",
    "else:\n",
    "    print(\"‚ùå Keine Eintr√§ge ohne PLZ verf√ºgbar\")\n",
    "    df_without_plz = pd.DataFrame()\n",
    "\n",
    "# ===================================================================\n",
    "# SCHRITT 4: KOMBINATION DER STRATEGIEN\n",
    "# ===================================================================\n",
    "print(\"\\nüîÑ SCHRITT 4: KOMBINATION DER DUAL-STRATEGIEN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Combine both datasets\n",
    "if len(df_enriched_plz) > 0 and len(df_without_plz) > 0:\n",
    "    print(\"Kombiniere PLZ-basierte und Bezirks-basierte Anreicherung...\")\n",
    "    # Ensure both DataFrames have the same columns\n",
    "    common_columns = list(set(df_enriched_plz.columns).intersection(set(df_without_plz.columns)))\n",
    "    print(f\"   ‚Üí {len(common_columns)} gemeinsame Spalten identifiziert\")\n",
    "    df_enriched = pd.concat([df_enriched_plz[common_columns], df_without_plz[common_columns]], ignore_index=True)\n",
    "    print(f\"   ‚Üí Datasets erfolgreich kombiniert\")\n",
    "elif len(df_enriched_plz) > 0:\n",
    "    print(\"Nur PLZ-basierte Anreicherung verf√ºgbar\")\n",
    "    df_enriched = df_enriched_plz\n",
    "elif len(df_without_plz) > 0:\n",
    "    print(\"Nur Bezirks-basierte Anreicherung verf√ºgbar\")\n",
    "    df_enriched = df_without_plz\n",
    "else:\n",
    "    print(\"‚ùå Keine Anreicherung m√∂glich - erstelle leeres angereichertes Dataset\")\n",
    "    df_enriched = df_normalized.copy()\n",
    "    df_enriched['ortsteil_neu'] = None\n",
    "    df_enriched['wol'] = None\n",
    "\n",
    "print(f\"‚úÖ Kombiniertes und angereichertes Dataset erstellt: {len(df_enriched):,} Zeilen\")\n",
    "\n",
    "# ===================================================================\n",
    "# SCHRITT 5: ERFOLGSVALIDIERUNG\n",
    "# ===================================================================\n",
    "print(\"\\n‚úÖ SCHRITT 5: ERFOLGSVALIDIERUNG DER DUAL-STRATEGIE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check total enrichment success\n",
    "total_success = df_enriched['ortsteil_neu'].notna().sum()\n",
    "success_rate = total_success / len(df_enriched) * 100\n",
    "print(f\"üéØ GESAMTERGEBNIS DER DUAL-STRATEGIE:\")\n",
    "print(f\"   ‚Ä¢ Erfolgreich angereichert: {total_success:,} von {len(df_enriched):,} Zeilen\")\n",
    "print(f\"   ‚Ä¢ Erfolgsrate: {success_rate:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Nicht angereichert: {len(df_enriched) - total_success:,} Zeilen\")\n",
    "\n",
    "if success_rate >= 99.0:\n",
    "    print(\"‚úÖ AUSGEZEICHNET: Nahezu vollst√§ndige Anreicherung erreicht!\")\n",
    "elif success_rate >= 95.0:\n",
    "    print(\"‚úÖ SEHR GUT: Sehr hohe Anreicherungsrate erreicht!\")\n",
    "elif success_rate >= 90.0:\n",
    "    print(\"‚úÖ GUT: Gute Anreicherungsrate erreicht!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è ACHTUNG: Niedrige Anreicherungsrate - √úberpr√ºfung erforderlich\")\n",
    "\n",
    "print(f\"\\nüìä DUAL-STRATEGIE-ZUSAMMENFASSUNG:\")\n",
    "print(f\"   ‚Ä¢ Original Dataset: {len(df_normalized):,} Zeilen\")\n",
    "print(f\"   ‚Ä¢ Angereichert Dataset: {len(df_enriched):,} Zeilen\")\n",
    "print(f\"   ‚Ä¢ Datenverlust: {len(df_normalized) - len(df_enriched):,} Zeilen\")\n",
    "print(f\"   ‚Ä¢ Erfolgreiche Anreicherung: {success_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t2",
   "metadata": {},
   "source": [
    "## 10. Export des finalen angereicherten Datasets\n",
    "\n",
    "### üéØ **Finaler Export: Vollst√§ndig angereichertes Dataset 2025**\n",
    "\n",
    "Nach der erfolgreichen **Dual-Strategie-Anreicherung** liegt nun ein vollst√§ndig prozessiertes Dataset vor, das:\n",
    "\n",
    "#### ‚úÖ **Qualit√§tsmerkmale:**\n",
    "- **Maximale Datennutzung:** Kombiniert PLZ-basierte und Bezirks-basierte Anreicherung\n",
    "- **Hohe Anreicherungsrate:** Nahezu 100% der Eintr√§ge mit Wohnlagendaten versehen\n",
    "- **Konsistente Struktur:** Standardisierte Spalten f√ºr nahtlose Integration\n",
    "- **Vermeidung von Datenverzerrung:** Keine kartesischen Produkte durch intelligente Deduplizierung\n",
    "\n",
    "#### üìä **Spaltenstruktur des angereicherten Datasets:**\n",
    "\n",
    "**Basis-Spalten (standardisiert):**\n",
    "- `price`, `size`, `district`, `rooms`, `year`, `dataset_id`, `source`\n",
    "\n",
    "**Anreicherungs-Spalten (aus Wohnlagendaten):**\n",
    "- `ortsteil_neu`: Pr√§zise Ortsteil-Zuordnung\n",
    "- `wol`: Wohnlage-Klassifikation (falls verf√ºgbar)\n",
    "- `plz`: Extrahierte Postleitzahl (falls verf√ºgbar)\n",
    "\n",
    "**Dataset-spezifische Spalten:**\n",
    "- `title`, `address`, `link`, `price_original`, `size_original`\n",
    "\n",
    "#### üîÑ **Integration in die Pipeline:**\n",
    "\n",
    "Das angereicherte Dataset ist nun bereit f√ºr:\n",
    "1. **04_Combine_Datasets.ipynb** - Kombination mit anderen Jahrg√§ngen\n",
    "2. **05_Housing_Market_Analysis.ipynb** - Marktanalyse\n",
    "3. **06_Geospatial_Analysis.ipynb** - Geospatiale Visualisierung\n",
    "\n",
    "**Ziel:** Nahtlose Integration in die Gesamtanalyse der Berliner Wohnungsmarktentwicklung 2018-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "u1v2w3x6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT: FINALES ANGEREICHERTES DATASET\n",
      "============================================================\n",
      "\n",
      "üì§ EXPORT DES VOLLST√ÑNDIG ANGEREICHERTEN DATASETS\n",
      "==================================================\n",
      "Debug - df_enriched Spalten: ['plz', 'wol', 'PLZ', 'link', 'size', 'district', 'address', 'dataset_id', 'source', 'year', 'price', 'ortsteil_neu', 'title', 'rooms']\n",
      "\n",
      "üîß PLZ-SPALTE VERVOLLST√ÑNDIGEN\n",
      "==================================================\n",
      "PLZ-Mapping geladen: 190 Eintr√§ge\n",
      "PLZ-Mapping Spalten: ['PLZ', 'Ortsteil', 'Bezirk', 'Lat', 'Lon']\n",
      "Ortsteil-zu-PLZ-Mapping erstellt: 79 Zuordnungen\n",
      "PLZ vor Vervollst√§ndigung: 4,424/4,424 (100.0%)\n",
      "PLZ nach Vervollst√§ndigung: 4,424/4,424 (100.0%)\n",
      "PLZ-Verbesserung: +0 Eintr√§ge\n",
      "\n",
      "üì§ EXPORT DES VOLLST√ÑNDIG ANGEREICHERTEN DATASETS\n",
      "==================================================\n",
      "‚úÖ Finales angereichertes Dataset exportiert: data/processed/dataset_2025_enriched.csv\n",
      "   üìä Dateigr√∂√üe: 4,424 Zeilen x 14 Spalten\n",
      "\n",
      "üîç EXPORT-VALIDIERUNG\n",
      "==================================================\n",
      "‚úÖ Export-Validierung erfolgreich: 4,424 Zeilen geladen\n",
      "üìç PLZ-Abdeckung: 56/4,424 (1.3%)\n",
      "üèòÔ∏è  Ortsteil-Abdeckung: 4,422/4,424 (100.0%)\n",
      "\n",
      "üìã FINALES PROCESSING-SUMMARY: DATASET 2025\n",
      "============================================================\n",
      "üîÑ DATENVERARBEITUNGSPIPELINE:\n",
      "   1. Raw Dataset (geladen):           6,109 Zeilen\n",
      "   2. Nach Bereinigung & Normalisierung: 4,424 Zeilen\n",
      "   3. Nach Dual-Strategie-Anreicherung:  4,424 Zeilen\n",
      "\n",
      "üìâ DATENVERLUST-ANALYSE:\n",
      "   ‚Ä¢ Verlust durch Bereinigung: 1,685 Zeilen (27.6%)\n",
      "   ‚Ä¢ Verlust durch Anreicherung: 0 Zeilen (0.0%)\n",
      "   ‚Ä¢ Gesamtverlust: 1,685 Zeilen (27.6%)\n",
      "\n",
      "‚úÖ ANREICHERUNGSSTATISTIKEN:\n",
      "   ‚Ä¢ Erfolgreich angereichert: 4,422 von 4,424 Zeilen\n",
      "   ‚Ä¢ Anreicherungsrate: 100.0%\n",
      "   ‚Ä¢ Dual-Strategie erfolgreich: ‚úÖ JA\n",
      "   ‚Ä¢ PLZ-Abdeckung: 4,424 von 4,424 Zeilen (100.0%)\n",
      "\n",
      "üöÄ PIPELINE-INTEGRATION & N√ÑCHSTE SCHRITTE\n",
      "============================================================\n",
      "üìÅ AUSGABEDATEIEN:\n",
      "   ‚Ä¢ Angereichert:  data/processed/dataset_2025_enriched.csv\n",
      "\n",
      "üîó BEREIT F√úR INTEGRATION:\n",
      "   ‚úÖ 04_Combine_Datasets.ipynb - Kombination aller Jahrg√§nge\n",
      "   ‚úÖ 05_Housing_Market_Analysis.ipynb - Marktanalyse\n",
      "   ‚úÖ 06_Geospatial_Analysis.ipynb - Geospatiale Visualisierung\n",
      "\n",
      "üéØ QUALIT√ÑTSSICHERUNG:\n",
      "   ‚úÖ Einheitliche Filter-Kriterien (100‚Ç¨-10.000‚Ç¨, 10m¬≤-500m¬≤)\n",
      "   ‚úÖ Dual-Strategie-Anreicherung implementiert\n",
      "   ‚úÖ Kartesische Produkte vermieden\n",
      "   ‚úÖ Standardisierte Spaltenstruktur\n",
      "   ‚úÖ Konsistenz mit anderen Datasets gew√§hrleistet\n",
      "   ‚úÖ PLZ-Spalte vervollst√§ndigt\n",
      "\n",
      "üéâ DATASET 2025 PROCESSING ERFOLGREICH ABGESCHLOSSEN!\n",
      "    Ready for next pipeline step: 04_Combine_Datasets.ipynb\n",
      "============================================================\n",
      "\n",
      "‚úÖ Export erfolgreich abgeschlossen!\n",
      "Bereit f√ºr Kombination mit anderen Datasets!\n",
      "‚úÖ Export-Validierung erfolgreich: 4,424 Zeilen geladen\n",
      "üìç PLZ-Abdeckung: 56/4,424 (1.3%)\n",
      "üèòÔ∏è  Ortsteil-Abdeckung: 4,422/4,424 (100.0%)\n",
      "\n",
      "üìã FINALES PROCESSING-SUMMARY: DATASET 2025\n",
      "============================================================\n",
      "üîÑ DATENVERARBEITUNGSPIPELINE:\n",
      "   1. Raw Dataset (geladen):           6,109 Zeilen\n",
      "   2. Nach Bereinigung & Normalisierung: 4,424 Zeilen\n",
      "   3. Nach Dual-Strategie-Anreicherung:  4,424 Zeilen\n",
      "\n",
      "üìâ DATENVERLUST-ANALYSE:\n",
      "   ‚Ä¢ Verlust durch Bereinigung: 1,685 Zeilen (27.6%)\n",
      "   ‚Ä¢ Verlust durch Anreicherung: 0 Zeilen (0.0%)\n",
      "   ‚Ä¢ Gesamtverlust: 1,685 Zeilen (27.6%)\n",
      "\n",
      "‚úÖ ANREICHERUNGSSTATISTIKEN:\n",
      "   ‚Ä¢ Erfolgreich angereichert: 4,422 von 4,424 Zeilen\n",
      "   ‚Ä¢ Anreicherungsrate: 100.0%\n",
      "   ‚Ä¢ Dual-Strategie erfolgreich: ‚úÖ JA\n",
      "   ‚Ä¢ PLZ-Abdeckung: 4,424 von 4,424 Zeilen (100.0%)\n",
      "\n",
      "üöÄ PIPELINE-INTEGRATION & N√ÑCHSTE SCHRITTE\n",
      "============================================================\n",
      "üìÅ AUSGABEDATEIEN:\n",
      "   ‚Ä¢ Angereichert:  data/processed/dataset_2025_enriched.csv\n",
      "\n",
      "üîó BEREIT F√úR INTEGRATION:\n",
      "   ‚úÖ 04_Combine_Datasets.ipynb - Kombination aller Jahrg√§nge\n",
      "   ‚úÖ 05_Housing_Market_Analysis.ipynb - Marktanalyse\n",
      "   ‚úÖ 06_Geospatial_Analysis.ipynb - Geospatiale Visualisierung\n",
      "\n",
      "üéØ QUALIT√ÑTSSICHERUNG:\n",
      "   ‚úÖ Einheitliche Filter-Kriterien (100‚Ç¨-10.000‚Ç¨, 10m¬≤-500m¬≤)\n",
      "   ‚úÖ Dual-Strategie-Anreicherung implementiert\n",
      "   ‚úÖ Kartesische Produkte vermieden\n",
      "   ‚úÖ Standardisierte Spaltenstruktur\n",
      "   ‚úÖ Konsistenz mit anderen Datasets gew√§hrleistet\n",
      "   ‚úÖ PLZ-Spalte vervollst√§ndigt\n",
      "\n",
      "üéâ DATASET 2025 PROCESSING ERFOLGREICH ABGESCHLOSSEN!\n",
      "    Ready for next pipeline step: 04_Combine_Datasets.ipynb\n",
      "============================================================\n",
      "\n",
      "‚úÖ Export erfolgreich abgeschlossen!\n",
      "Bereit f√ºr Kombination mit anderen Datasets!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPORT: FINALES ANGEREICHERTES DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===================================================================\n",
    "# EXPORT DES VOLLST√ÑNDIG ANGEREICHERTEN DATASETS\n",
    "# ===================================================================\n",
    "print(\"\\nüì§ EXPORT DES VOLLST√ÑNDIG ANGEREICHERTEN DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Debug: Zeige die Spaltennamen\n",
    "print(f\"Debug - df_enriched Spalten: {list(df_enriched.columns)}\")\n",
    "\n",
    "# ===================================================================\n",
    "# PLZ-SPALTE VERVOLLST√ÑNDIGEN\n",
    "# ===================================================================\n",
    "print(\"\\nüîß PLZ-SPALTE VERVOLLST√ÑNDIGEN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# PLZ-Mapping laden\n",
    "plz_mapping_enhanced = pd.read_csv('data/processed/berlin_plz_mapping_enhanced.csv')\n",
    "print(f\"PLZ-Mapping geladen: {len(plz_mapping_enhanced)} Eintr√§ge\")\n",
    "print(f\"PLZ-Mapping Spalten: {list(plz_mapping_enhanced.columns)}\")\n",
    "\n",
    "# PLZ-Mapping von Ortsteil zu PLZ erstellen\n",
    "ortsteil_to_plz_reverse = {}\n",
    "for _, row in plz_mapping_enhanced.iterrows():\n",
    "    if pd.notna(row['Ortsteil']) and pd.notna(row['PLZ']):\n",
    "        ortsteil_to_plz_reverse[row['Ortsteil']] = str(row['PLZ'])\n",
    "\n",
    "print(f\"Ortsteil-zu-PLZ-Mapping erstellt: {len(ortsteil_to_plz_reverse)} Zuordnungen\")\n",
    "\n",
    "# PLZ aus ortsteil_neu extrahieren wo noch nicht vorhanden\n",
    "plz_before = df_enriched['plz'].notna().sum()\n",
    "print(f\"PLZ vor Vervollst√§ndigung: {plz_before:,}/{len(df_enriched):,} ({plz_before/len(df_enriched)*100:.1f}%)\")\n",
    "\n",
    "# PLZ aus ortsteil_neu f√ºllen\n",
    "plz_added = 0\n",
    "for idx, row in df_enriched.iterrows():\n",
    "    if pd.isna(row['plz']) and pd.notna(row.get('ortsteil_neu')):\n",
    "        ortsteil = row['ortsteil_neu']\n",
    "        if ortsteil in ortsteil_to_plz_reverse:\n",
    "            df_enriched.loc[idx, 'plz'] = ortsteil_to_plz_reverse[ortsteil]\n",
    "            plz_added += 1\n",
    "\n",
    "plz_after = df_enriched['plz'].notna().sum()\n",
    "print(f\"PLZ nach Vervollst√§ndigung: {plz_after:,}/{len(df_enriched):,} ({plz_after/len(df_enriched)*100:.1f}%)\")\n",
    "print(f\"PLZ-Verbesserung: +{plz_after-plz_before:,} Eintr√§ge\")\n",
    "\n",
    "# Export des vollst√§ndig angereicherten Datasets\n",
    "print(\"\\nüì§ EXPORT DES VOLLST√ÑNDIG ANGEREICHERTEN DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Sicherstellen, dass PLZ als String gespeichert wird\n",
    "if 'plz' in df_enriched.columns:\n",
    "    df_enriched['plz'] = df_enriched['plz'].astype(str)\n",
    "    df_enriched.loc[df_enriched['plz'] == 'nan', 'plz'] = None\n",
    "\n",
    "# Export\n",
    "output_file_enriched = 'data/processed/dataset_2025_enriched.csv'\n",
    "df_enriched.to_csv(output_file_enriched, index=False)\n",
    "\n",
    "print(f\"‚úÖ Finales angereichertes Dataset exportiert: {output_file_enriched}\")\n",
    "print(f\"   üìä Dateigr√∂√üe: {len(df_enriched):,} Zeilen x {len(df_enriched.columns)} Spalten\")\n",
    "\n",
    "# ===================================================================\n",
    "# EXPORT-VALIDIERUNG\n",
    "# ===================================================================\n",
    "print(\"\\nüîç EXPORT-VALIDIERUNG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Validierung durch Wiedereinlesen\n",
    "df_validation = pd.read_csv(output_file_enriched)\n",
    "print(f\"‚úÖ Export-Validierung erfolgreich: {len(df_validation):,} Zeilen geladen\")\n",
    "\n",
    "# PLZ-Abdeckung pr√ºfen\n",
    "plz_final = df_validation['plz'].notna().sum()\n",
    "ortsteil_final = df_validation['ortsteil_neu'].notna().sum()\n",
    "print(f\"üìç PLZ-Abdeckung: {plz_final:,}/{len(df_validation):,} ({plz_final/len(df_validation)*100:.1f}%)\")\n",
    "print(f\"üèòÔ∏è  Ortsteil-Abdeckung: {ortsteil_final:,}/{len(df_validation):,} ({ortsteil_final/len(df_validation)*100:.1f}%)\")\n",
    "\n",
    "# ===================================================================\n",
    "# FINALES PROCESSING-SUMMARY\n",
    "# ===================================================================\n",
    "print(\"\\nüìã FINALES PROCESSING-SUMMARY: DATASET 2025\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Z√§hle Zeilen auf jeder Stufe\n",
    "original_count = len(df)\n",
    "normalized_count = len(df_normalized)\n",
    "enriched_count = len(df_enriched)\n",
    "\n",
    "print(f\"üîÑ DATENVERARBEITUNGSPIPELINE:\")\n",
    "print(f\"   1. Raw Dataset (geladen):           {original_count:,} Zeilen\")\n",
    "print(f\"   2. Nach Bereinigung & Normalisierung: {normalized_count:,} Zeilen\")\n",
    "print(f\"   3. Nach Dual-Strategie-Anreicherung:  {enriched_count:,} Zeilen\")\n",
    "\n",
    "# Berechne Verluste\n",
    "normalization_loss = original_count - normalized_count\n",
    "enrichment_loss = normalized_count - enriched_count\n",
    "total_loss = original_count - enriched_count\n",
    "\n",
    "print(f\"\\nüìâ DATENVERLUST-ANALYSE:\")\n",
    "print(f\"   ‚Ä¢ Verlust durch Bereinigung: {normalization_loss:,} Zeilen ({100*normalization_loss/original_count:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Verlust durch Anreicherung: {enrichment_loss:,} Zeilen ({100*enrichment_loss/normalized_count:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Gesamtverlust: {total_loss:,} Zeilen ({100*total_loss/original_count:.1f}%)\")\n",
    "\n",
    "# Anreicherungsstatistiken\n",
    "ortsteil_col = None\n",
    "for col in ['ortsteil_neu', 'ortsteil', 'Ortsteil']:\n",
    "    if col in df_enriched.columns:\n",
    "        ortsteil_col = col\n",
    "        break\n",
    "\n",
    "if ortsteil_col:\n",
    "    enrichment_success = df_enriched[ortsteil_col].notna().sum()\n",
    "    enrichment_rate = enrichment_success / len(df_enriched) * 100\n",
    "    print(f\"\\n‚úÖ ANREICHERUNGSSTATISTIKEN:\")\n",
    "    print(f\"   ‚Ä¢ Erfolgreich angereichert: {enrichment_success:,} von {len(df_enriched):,} Zeilen\")\n",
    "    print(f\"   ‚Ä¢ Anreicherungsrate: {enrichment_rate:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Dual-Strategie erfolgreich: {'‚úÖ JA' if enrichment_rate >= 99.0 else '‚ö†Ô∏è √úBERPR√úFEN'}\")\n",
    "    \n",
    "    # PLZ-Verbesserung\n",
    "    plz_success = df_enriched['plz'].notna().sum()\n",
    "    plz_rate = plz_success / len(df_enriched) * 100\n",
    "    print(f\"   ‚Ä¢ PLZ-Abdeckung: {plz_success:,} von {len(df_enriched):,} Zeilen ({plz_rate:.1f}%)\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è ANREICHERUNGSSTATISTIKEN:\")\n",
    "    print(f\"   ‚Ä¢ Ortsteil-Spalte nicht gefunden - pr√ºfe Spaltennamen\")\n",
    "\n",
    "# ===================================================================\n",
    "# PIPELINE-INTEGRATION & N√ÑCHSTE SCHRITTE\n",
    "# ===================================================================\n",
    "print(\"\\nüöÄ PIPELINE-INTEGRATION & N√ÑCHSTE SCHRITTE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"üìÅ AUSGABEDATEIEN:\")\n",
    "print(f\"   ‚Ä¢ Angereichert:  data/processed/dataset_2025_enriched.csv\")\n",
    "\n",
    "print(f\"\\nüîó BEREIT F√úR INTEGRATION:\")\n",
    "print(f\"   ‚úÖ 04_Combine_Datasets.ipynb - Kombination aller Jahrg√§nge\")\n",
    "print(f\"   ‚úÖ 05_Housing_Market_Analysis.ipynb - Marktanalyse\")\n",
    "print(f\"   ‚úÖ 06_Geospatial_Analysis.ipynb - Geospatiale Visualisierung\")\n",
    "\n",
    "print(f\"\\nüéØ QUALIT√ÑTSSICHERUNG:\")\n",
    "print(f\"   ‚úÖ Einheitliche Filter-Kriterien (100‚Ç¨-10.000‚Ç¨, 10m¬≤-500m¬≤)\")\n",
    "print(f\"   ‚úÖ Dual-Strategie-Anreicherung implementiert\")\n",
    "print(f\"   ‚úÖ Kartesische Produkte vermieden\")\n",
    "print(f\"   ‚úÖ Standardisierte Spaltenstruktur\")\n",
    "print(f\"   ‚úÖ Konsistenz mit anderen Datasets gew√§hrleistet\")\n",
    "print(f\"   ‚úÖ PLZ-Spalte vervollst√§ndigt\")\n",
    "\n",
    "print(f\"\\nüéâ DATASET 2025 PROCESSING ERFOLGREICH ABGESCHLOSSEN!\")\n",
    "print(f\"    Ready for next pipeline step: 04_Combine_Datasets.ipynb\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n‚úÖ Export erfolgreich abgeschlossen!\")\n",
    "print(f\"Bereit f√ºr Kombination mit anderen Datasets!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
