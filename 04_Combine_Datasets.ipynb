{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eacc2b7",
   "metadata": {},
   "source": [
    "# 04_Combine_Datasets - Finale Dataset-Kombination\n",
    "\n",
    "**Kombination aller normalisierten und angereicherten Datasets**\n",
    "- L√§dt die drei normalisierten und angereicherten Datasets (2018-2019, 2022, 2025)\n",
    "- Pr√ºft Schema-Kompatibilit√§t\n",
    "- F√ºhrt Qualit√§tspr√ºfungen durch\n",
    "- Erstellt finales kombiniertes und angereichertes Dataset\n",
    "\n",
    "**Input:**\n",
    "- `data/processed/dataset_2018_2019_enriched.csv`\n",
    "- `data/processed/dataset_2022_enriched.csv` \n",
    "- `data/processed/dataset_2025_enriched.csv`\n",
    "\n",
    "**Output:**\n",
    "- `data/processed/berlin_housing_combined_enriched_final.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a928a",
   "metadata": {},
   "source": [
    "## 1. Setup und Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90c54540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotheken erfolgreich importiert!\n",
      "Pandas Version: 2.3.0\n",
      "Ziel: Kombination aller normalisierten Datasets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Bibliotheken erfolgreich importiert!\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(\"Ziel: Kombination aller normalisierten Datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc575363",
   "metadata": {},
   "source": [
    "## 2. Angereicherte Datasets laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f81cd754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANGEREICHERTE DATASETS LADEN - ENHANCED WITH PLZ FIXES\n",
      "============================================================\n",
      "‚úÖ PLZ conversion and debug functions loaded!\n",
      "\n",
      "üìÅ LOADING DATASETS WITH PROPER PLZ DTYPE\n",
      "==================================================\n",
      "Lade Datasets mit dtype={'plz': 'string'} f√ºr korrekte PLZ-Behandlung...\n",
      "\n",
      "Lade 2018_2019...\n",
      "‚úÖ 2018_2019: 10,387 Zeilen, 15 Spalten\n",
      "üîç DEBUG - Dataset 2018_2019:\n",
      "   Total rows: 10,387\n",
      "   PLZ coverage: 10,173 (97.9%)\n",
      "   PLZ missing: 214\n",
      "   PLZ data types: {\"<class 'str'>\": 10173}\n",
      "   Sample PLZ: ['13591', '10179', '10999']\n",
      "\n",
      "\n",
      "Lade 2022...\n",
      "‚úÖ 2022: 2,676 Zeilen, 25 Spalten\n",
      "üîç DEBUG - Dataset 2022:\n",
      "   Total rows: 2,676\n",
      "   PLZ coverage: 2,676 (100.0%)\n",
      "   PLZ missing: 0\n",
      "   PLZ data types: {\"<class 'str'>\": 2676}\n",
      "   Sample PLZ: ['13125', '13125', '13125']\n",
      "\n",
      "\n",
      "Lade 2025...\n",
      "‚úÖ 2025: 4,424 Zeilen, 14 Spalten\n",
      "üîç DEBUG - Dataset 2025:\n",
      "   Total rows: 4,424\n",
      "   PLZ coverage: 4,424 (100.0%)\n",
      "   PLZ missing: 0\n",
      "   PLZ data types: {\"<class 'str'>\": 4424}\n",
      "   Sample PLZ: ['13507', '14612', '10585']\n",
      "\n",
      "\n",
      "‚úÖ Geladen: 3 von 3 Datasets\n",
      "\n",
      "üîç VALIDATION OF LOADED DATASETS\n",
      "==================================================\n",
      "2018_2019: PLZ 10,173/10,387 (97.9%) | Dtype: object\n",
      "2022: PLZ 2,676/2,676 (100.0%) | Dtype: object\n",
      "2025: PLZ 4,424/4,424 (100.0%) | Dtype: object\n",
      "\n",
      "üìä OVERALL PLZ STATUS:\n",
      "   Total rows: 17,487\n",
      "   Total PLZ coverage: 17,273 (98.8%)\n",
      "   ‚úÖ EXCELLENT: High overall PLZ coverage!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ANGEREICHERTE DATASETS LADEN - ENHANCED WITH PLZ FIXES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ===================================================================\n",
    "# ROBUST PLZ-TO-STRING CONVERSION FUNCTION\n",
    "# ===================================================================\n",
    "def convert_plz_to_string(plz_value):\n",
    "    \"\"\"\n",
    "    Robust PLZ-to-string conversion function.\n",
    "    Handles floats, ints, and strings properly.\n",
    "    Returns clean string PLZ without .0 suffix, or None if invalid.\n",
    "    \"\"\"\n",
    "    if pd.isna(plz_value):\n",
    "        return None\n",
    "    \n",
    "    # Handle string inputs\n",
    "    if isinstance(plz_value, str):\n",
    "        plz_value = plz_value.strip()\n",
    "        if plz_value.lower() in ['nan', 'none', '']:\n",
    "            return None\n",
    "        # Remove .0 suffix if present\n",
    "        if plz_value.endswith('.0'):\n",
    "            plz_value = plz_value[:-2]\n",
    "        # Validate PLZ format (5 digits)\n",
    "        if plz_value.isdigit() and len(plz_value) == 5:\n",
    "            return plz_value\n",
    "        return None\n",
    "    \n",
    "    # Handle numeric inputs (int or float)\n",
    "    try:\n",
    "        plz_int = int(plz_value)\n",
    "        if 10000 <= plz_int <= 99999:  # Valid German PLZ range\n",
    "            return str(plz_int)\n",
    "        return None\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "# ===================================================================\n",
    "# DEBUG LOGGING FUNCTION\n",
    "# ===================================================================\n",
    "def debug_plz_coverage(df, stage_name):\n",
    "    \"\"\"Debug function to track PLZ coverage at each stage\"\"\"\n",
    "    total_rows = len(df)\n",
    "    plz_coverage = df['plz'].notna().sum()\n",
    "    plz_percentage = (plz_coverage / total_rows * 100) if total_rows > 0 else 0\n",
    "    \n",
    "    print(f\"üîç DEBUG - {stage_name}:\")\n",
    "    print(f\"   Total rows: {total_rows:,}\")\n",
    "    print(f\"   PLZ coverage: {plz_coverage:,} ({plz_percentage:.1f}%)\")\n",
    "    print(f\"   PLZ missing: {total_rows - plz_coverage:,}\")\n",
    "    \n",
    "    # Show PLZ data types\n",
    "    if plz_coverage > 0:\n",
    "        plz_types = df['plz'].dropna().apply(type).value_counts()\n",
    "        type_dict = {}\n",
    "        for type_key, count in plz_types.items():\n",
    "            type_dict[str(type_key)] = count\n",
    "        print(f\"   PLZ data types: {type_dict}\")\n",
    "        # Show sample PLZ values\n",
    "        sample_plz = df['plz'].dropna().head(3).tolist()\n",
    "        print(f\"   Sample PLZ: {sample_plz}\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ PLZ conversion and debug functions loaded!\")\n",
    "\n",
    "# ===================================================================\n",
    "# LOAD DATASETS WITH PROPER PLZ DTYPE\n",
    "# ===================================================================\n",
    "print(\"\\nüìÅ LOADING DATASETS WITH PROPER PLZ DTYPE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Dateipfade zu den angereicherten Datasets\n",
    "file_paths = {\n",
    "    '2018_2019': 'data/processed/dataset_2018_2019_enriched.csv',\n",
    "    '2022': 'data/processed/dataset_2022_enriched.csv',\n",
    "    '2025': 'data/processed/dataset_2025_enriched.csv'\n",
    "}\n",
    "\n",
    "# Laden der Datasets mit korrekten dtypes\n",
    "datasets = {}\n",
    "print(\"Lade Datasets mit dtype={'plz': 'string'} f√ºr korrekte PLZ-Behandlung...\")\n",
    "\n",
    "for dataset_name, file_path in file_paths.items():\n",
    "    print(f\"\\nLade {dataset_name}...\")\n",
    "    \n",
    "    try:\n",
    "        # Load with proper PLZ dtype\n",
    "        df = pd.read_csv(file_path, dtype={'plz': 'string'})\n",
    "        \n",
    "        # Apply additional PLZ cleaning to handle any remaining issues\n",
    "        if 'plz' in df.columns:\n",
    "            df['plz'] = df['plz'].apply(convert_plz_to_string)\n",
    "        \n",
    "        datasets[dataset_name] = df\n",
    "        print(f\"‚úÖ {dataset_name}: {len(df):,} Zeilen, {len(df.columns)} Spalten\")\n",
    "        \n",
    "        # Debug PLZ status for each dataset\n",
    "        if 'plz' in df.columns:\n",
    "            debug_plz_coverage(df, f\"Dataset {dataset_name}\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è No PLZ column found in {dataset_name}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {dataset_name}: {e}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Geladen: {len(datasets)} von {len(file_paths)} Datasets\")\n",
    "\n",
    "# ===================================================================\n",
    "# VALIDATE ALL DATASETS LOADED CORRECTLY\n",
    "# ===================================================================\n",
    "print(\"\\nüîç VALIDATION OF LOADED DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_rows = 0\n",
    "total_plz_coverage = 0\n",
    "\n",
    "for name, df in datasets.items():\n",
    "    if 'plz' in df.columns:\n",
    "        plz_count = df['plz'].notna().sum()\n",
    "        plz_pct = (plz_count / len(df)) * 100\n",
    "        print(f\"{name}: PLZ {plz_count:,}/{len(df):,} ({plz_pct:.1f}%) | Dtype: {df['plz'].dtype}\")\n",
    "        \n",
    "        total_rows += len(df)\n",
    "        total_plz_coverage += plz_count\n",
    "    else:\n",
    "        print(f\"{name}: No PLZ column\")\n",
    "\n",
    "if total_rows > 0:\n",
    "    overall_plz_pct = (total_plz_coverage / total_rows) * 100\n",
    "    print(f\"\\nüìä OVERALL PLZ STATUS:\")\n",
    "    print(f\"   Total rows: {total_rows:,}\")\n",
    "    print(f\"   Total PLZ coverage: {total_plz_coverage:,} ({overall_plz_pct:.1f}%)\")\n",
    "    \n",
    "    if overall_plz_pct >= 90:\n",
    "        print(\"   ‚úÖ EXCELLENT: High overall PLZ coverage!\")\n",
    "    elif overall_plz_pct >= 70:\n",
    "        print(\"   ‚úÖ GOOD: Acceptable PLZ coverage\")\n",
    "    else:\n",
    "        print(\"   ‚ö†Ô∏è LOW: PLZ coverage needs improvement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4e8d61",
   "metadata": {},
   "source": [
    "## 3. Schema-Kompatibilit√§t pr√ºfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2fb70ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SCHEMA-KOMPATIBILIT√ÑT PR√úFEN\n",
      "============================================================\n",
      "Erwartete Basis-Spalten (vor PLZ-Enhancement):\n",
      "  - price\n",
      "  - size\n",
      "  - district\n",
      "  - rooms\n",
      "  - year\n",
      "  - dataset_id\n",
      "  - source\n",
      "  - wol\n",
      "  - plz\n",
      "\n",
      "Spalten die durch PLZ-Enhancement hinzugef√ºgt werden:\n",
      "  - ortsteil\n",
      "  - bezirk\n",
      "  - lat\n",
      "  - lon\n",
      "\n",
      "=== DATASET 2018_2019 ===\n",
      "Spalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'street', 'floor', 'typeOfFlat', 'yearConstructed', 'totalRent', 'plz', 'wol', 'ortsteil_neu']\n",
      "‚úÖ Alle Basis-Spalten vorhanden\n",
      "‚ÑπÔ∏è  Zus√§tzliche Spalten: 6 (['street', 'floor', 'typeOfFlat']...)\n",
      "üìç PLZ verf√ºgbar: 10,173/10,387 (97.9%)\n",
      "\n",
      "=== DATASET 2022 ===\n",
      "Spalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'plz', 'warmmiete', 'nebenkosten', 'kaution', 'baujahr', 'zustand', 'energieeffiziensklasse', 'ausstattung_m√∂bliert', 'ausstattung_balkon', 'ausstattung_terrasse', 'ausstattung_garten', 'ausstattung_einbauk√ºche', 'ausstattung_garage', 'ausstattung_stellplatz', 'ausstattung_personenaufzug', 'ausstattung_keller', 'wol', 'ortsteil_neu']\n",
      "‚úÖ Alle Basis-Spalten vorhanden\n",
      "‚ÑπÔ∏è  Zus√§tzliche Spalten: 16 (['warmmiete', 'nebenkosten', 'kaution']...)\n",
      "üìç PLZ verf√ºgbar: 2,676/2,676 (100.0%)\n",
      "\n",
      "=== DATASET 2025 ===\n",
      "Spalten: ['plz', 'wol', 'PLZ', 'link', 'size', 'district', 'address', 'dataset_id', 'source', 'year', 'price', 'ortsteil_neu', 'title', 'rooms']\n",
      "‚úÖ Alle Basis-Spalten vorhanden\n",
      "‚ÑπÔ∏è  Zus√§tzliche Spalten: 5 (['PLZ', 'link', 'address']...)\n",
      "üìç PLZ verf√ºgbar: 4,424/4,424 (100.0%)\n",
      "\n",
      "=== SCHEMA-KOMPATIBILIT√ÑT ===\n",
      "Valide Datasets: 3/3\n",
      "‚úÖ Alle Datasets sind schema-kompatibel!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SCHEMA-KOMPATIBILIT√ÑT PR√úFEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Definiere erwartete Standardspalten (vor PLZ-Enhancement)\n",
    "base_columns = ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'wol', 'plz']\n",
    "\n",
    "# Spalten die nach PLZ-Enhancement hinzugef√ºgt werden\n",
    "enhanced_columns = ['ortsteil', 'bezirk', 'lat', 'lon']\n",
    "\n",
    "print(\"Erwartete Basis-Spalten (vor PLZ-Enhancement):\")\n",
    "for col in base_columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(\"\\nSpalten die durch PLZ-Enhancement hinzugef√ºgt werden:\")\n",
    "for col in enhanced_columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Pr√ºfe jedes Dataset (nur gegen Basis-Spalten)\n",
    "schema_check = {}\n",
    "for dataset_name, df in datasets.items():\n",
    "    missing_cols = [col for col in base_columns if col not in df.columns]\n",
    "    extra_cols = [col for col in df.columns if col not in base_columns]\n",
    "    \n",
    "    schema_check[dataset_name] = {\n",
    "        'missing': missing_cols,\n",
    "        'extra': extra_cols,\n",
    "        'valid': len(missing_cols) == 0\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n=== DATASET {dataset_name.upper()} ===\")\n",
    "    print(f\"Spalten: {list(df.columns)}\")\n",
    "    if missing_cols:\n",
    "        print(f\"‚ùå Fehlende Basis-Spalten: {missing_cols}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Alle Basis-Spalten vorhanden\")\n",
    "    \n",
    "    if extra_cols:\n",
    "        print(f\"‚ÑπÔ∏è  Zus√§tzliche Spalten: {len(extra_cols)} ({extra_cols[:3]}...)\")\n",
    "    \n",
    "    # Pr√ºfe PLZ-Verf√ºgbarkeit speziell\n",
    "    if 'plz' in df.columns:\n",
    "        plz_available = df['plz'].notna().sum()\n",
    "        print(f\"üìç PLZ verf√ºgbar: {plz_available:,}/{len(df):,} ({plz_available/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Zusammenfassung\n",
    "valid_datasets = [name for name, check in schema_check.items() if check['valid']]\n",
    "print(f\"\\n=== SCHEMA-KOMPATIBILIT√ÑT ===\")\n",
    "print(f\"Valide Datasets: {len(valid_datasets)}/{len(datasets)}\")\n",
    "if len(valid_datasets) == len(datasets):\n",
    "    print(\"‚úÖ Alle Datasets sind schema-kompatibel!\")\n",
    "else:\n",
    "    print(\"‚ùå Schema-Inkompatibilit√§ten gefunden!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6b09c5",
   "metadata": {},
   "source": [
    "## 4. Datenqualit√§t pr√ºfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70223a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATENQUALIT√ÑT PR√úFEN\n",
      "============================================================\n",
      "\n",
      "=== DATASET 2018_2019 ===\n",
      "  price: 10,387/10,387 (100.0%) nicht-null\n",
      "  size: 10,387/10,387 (100.0%) nicht-null\n",
      "  district: 10,387/10,387 (100.0%) nicht-null\n",
      "  rooms: 10,387/10,387 (100.0%) nicht-null\n",
      "  year: 10,387/10,387 (100.0%) nicht-null\n",
      "  dataset_id: 10,387/10,387 (100.0%) nicht-null\n",
      "  source: 10,387/10,387 (100.0%) nicht-null\n",
      "  wol: 9,505/10,387 (91.5%) nicht-null\n",
      "  plz: 10,173/10,387 (97.9%) nicht-null\n",
      "  Preis (50-20000‚Ç¨): 10,387/10,387 (100.0%) g√ºltig\n",
      "  Gr√∂√üe (5-1000m¬≤): 10,387/10,387 (100.0%) g√ºltig\n",
      "  Einzigartige Bezirke: 79\n",
      "  PLZ verf√ºgbar: 10,173/10,387 (97.9%)\n",
      "\n",
      "=== DATASET 2022 ===\n",
      "  price: 2,676/2,676 (100.0%) nicht-null\n",
      "  size: 2,676/2,676 (100.0%) nicht-null\n",
      "  district: 2,676/2,676 (100.0%) nicht-null\n",
      "  rooms: 2,676/2,676 (100.0%) nicht-null\n",
      "  year: 2,676/2,676 (100.0%) nicht-null\n",
      "  dataset_id: 2,676/2,676 (100.0%) nicht-null\n",
      "  source: 2,676/2,676 (100.0%) nicht-null\n",
      "  wol: 2,676/2,676 (100.0%) nicht-null\n",
      "  plz: 2,676/2,676 (100.0%) nicht-null\n",
      "  Preis (50-20000‚Ç¨): 2,676/2,676 (100.0%) g√ºltig\n",
      "  Gr√∂√üe (5-1000m¬≤): 2,676/2,676 (100.0%) g√ºltig\n",
      "  Einzigartige Bezirke: 21\n",
      "  PLZ verf√ºgbar: 2,676/2,676 (100.0%)\n",
      "\n",
      "=== DATASET 2025 ===\n",
      "  price: 4,424/4,424 (100.0%) nicht-null\n",
      "  size: 4,424/4,424 (100.0%) nicht-null\n",
      "  district: 4,424/4,424 (100.0%) nicht-null\n",
      "  rooms: 0/4,424 (0.0%) nicht-null\n",
      "  year: 4,424/4,424 (100.0%) nicht-null\n",
      "  dataset_id: 4,424/4,424 (100.0%) nicht-null\n",
      "  source: 4,424/4,424 (100.0%) nicht-null\n",
      "  wol: 54/4,424 (1.2%) nicht-null\n",
      "  plz: 4,424/4,424 (100.0%) nicht-null\n",
      "  Preis (50-20000‚Ç¨): 4,424/4,424 (100.0%) g√ºltig\n",
      "  Gr√∂√üe (5-1000m¬≤): 4,424/4,424 (100.0%) g√ºltig\n",
      "  Einzigartige Bezirke: 20\n",
      "  PLZ verf√ºgbar: 4,424/4,424 (100.0%)\n",
      "\n",
      "‚úÖ Datenqualit√§tspr√ºfung abgeschlossen\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATENQUALIT√ÑT PR√úFEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "quality_report = {}\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    print(f\"\\n=== DATASET {dataset_name.upper()} ===\")\n",
    "    \n",
    "    # Grundlegende Statistiken\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Vollst√§ndigkeit pr√ºfen (verwende base_columns)\n",
    "    completeness = {}\n",
    "    for col in base_columns:\n",
    "        if col in df.columns:\n",
    "            non_null = df[col].notna().sum()\n",
    "            completeness[col] = (non_null, non_null/total_rows*100)\n",
    "            print(f\"  {col}: {non_null:,}/{total_rows:,} ({non_null/total_rows*100:.1f}%) nicht-null\")\n",
    "    \n",
    "    # Wertebereichs-Pr√ºfungen\n",
    "    if 'price' in df.columns:\n",
    "        price_valid = ((df['price'] >= 50) & (df['price'] <= 20000)).sum()\n",
    "        print(f\"  Preis (50-20000‚Ç¨): {price_valid:,}/{total_rows:,} ({price_valid/total_rows*100:.1f}%) g√ºltig\")\n",
    "    \n",
    "    if 'size' in df.columns:\n",
    "        size_valid = ((df['size'] >= 5) & (df['size'] <= 1000)).sum()\n",
    "        print(f\"  Gr√∂√üe (5-1000m¬≤): {size_valid:,}/{total_rows:,} ({size_valid/total_rows*100:.1f}%) g√ºltig\")\n",
    "    \n",
    "    if 'district' in df.columns:\n",
    "        unique_districts = df['district'].nunique()\n",
    "        print(f\"  Einzigartige Bezirke: {unique_districts}\")\n",
    "    \n",
    "    # PLZ-Qualit√§t pr√ºfen\n",
    "    if 'plz' in df.columns:\n",
    "        plz_available = df['plz'].notna().sum()\n",
    "        print(f\"  PLZ verf√ºgbar: {plz_available:,}/{total_rows:,} ({plz_available/total_rows*100:.1f}%)\")\n",
    "    \n",
    "    quality_report[dataset_name] = {\n",
    "        'total_rows': total_rows,\n",
    "        'completeness': completeness,\n",
    "        'unique_districts': df['district'].nunique() if 'district' in df.columns else 0,\n",
    "        'plz_available': df['plz'].notna().sum() if 'plz' in df.columns else 0\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ Datenqualit√§tspr√ºfung abgeschlossen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de277ee0",
   "metadata": {},
   "source": [
    "## 5. Datasets kombinieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74881885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASETS KOMBINIEREN\n",
      "============================================================\n",
      "2018_2019: 10,387 Zeilen mit 9 Basis-Spalten\n",
      "2022: 2,676 Zeilen mit 9 Basis-Spalten\n",
      "2025: 4,424 Zeilen mit 9 Basis-Spalten\n",
      "\n",
      "Kombiniere Datasets...\n",
      "‚úÖ Kombiniertes Dataset erstellt: 17,487 Zeilen\n",
      "\n",
      "=== KOMBINATIONS-ZUSAMMENFASSUNG ===\n",
      "Input-Zeilen gesamt: 17,487\n",
      "Output-Zeilen: 17,487\n",
      "Datenverlust: 0 (0.0%)\n",
      "\n",
      "=== JAHRESVERTEILUNG ===\n",
      "  2019: 10,387 Eintr√§ge (59.4%)\n",
      "  2022: 2,676 Eintr√§ge (15.3%)\n",
      "  2025: 4,424 Eintr√§ge (25.3%)\n",
      "\n",
      "=== DATASET-VERTEILUNG ===\n",
      "  historical: 10,387 Eintr√§ge (59.4%)\n",
      "  recent: 4,424 Eintr√§ge (25.3%)\n",
      "  current: 2,676 Eintr√§ge (15.3%)\n",
      "\n",
      "=== PLZ-VERF√úGBARKEIT ===\n",
      "PLZ verf√ºgbar: 17,273/17,487 (98.8%)\n",
      "‚úÖ Bereit f√ºr PLZ-Enhancement mit Ortsteil und Koordinaten\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATASETS KOMBINIEREN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Nur Basis-Spalten f√ºr Kombination verwenden (PLZ-Enhancement kommt sp√§ter)\n",
    "datasets_standard = {}\n",
    "for dataset_name, df in datasets.items():\n",
    "    # W√§hle nur Basis-Spalten aus\n",
    "    available_base_cols = [col for col in base_columns if col in df.columns]\n",
    "    df_std = df[available_base_cols].copy()\n",
    "    datasets_standard[dataset_name] = df_std\n",
    "    print(f\"{dataset_name}: {len(df_std):,} Zeilen mit {len(available_base_cols)} Basis-Spalten\")\n",
    "\n",
    "# Kombiniere alle Datasets\n",
    "print(f\"\\nKombiniere Datasets...\")\n",
    "combined_df = pd.concat(datasets_standard.values(), ignore_index=True, sort=False)\n",
    "\n",
    "print(f\"‚úÖ Kombiniertes Dataset erstellt: {len(combined_df):,} Zeilen\")\n",
    "\n",
    "# Zusammenfassung\n",
    "print(f\"\\n=== KOMBINATIONS-ZUSAMMENFASSUNG ===\")\n",
    "total_input_rows = sum(len(df) for df in datasets.values())\n",
    "print(f\"Input-Zeilen gesamt: {total_input_rows:,}\")\n",
    "print(f\"Output-Zeilen: {len(combined_df):,}\")\n",
    "print(f\"Datenverlust: {total_input_rows - len(combined_df):,} ({(total_input_rows - len(combined_df))/total_input_rows*100:.1f}%)\")\n",
    "\n",
    "# Verteilung nach Jahren\n",
    "print(f\"\\n=== JAHRESVERTEILUNG ===\")\n",
    "year_counts = combined_df['year'].value_counts().sort_index()\n",
    "for year, count in year_counts.items():\n",
    "    print(f\"  {year}: {count:,} Eintr√§ge ({count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "# Verteilung nach Dataset-ID\n",
    "print(f\"\\n=== DATASET-VERTEILUNG ===\")\n",
    "dataset_counts = combined_df['dataset_id'].value_counts()\n",
    "for dataset_id, count in dataset_counts.items():\n",
    "    print(f\"  {dataset_id}: {count:,} Eintr√§ge ({count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "# PLZ-Verf√ºgbarkeit pr√ºfen\n",
    "if 'plz' in combined_df.columns:\n",
    "    plz_available = combined_df['plz'].notna().sum()\n",
    "    print(f\"\\n=== PLZ-VERF√úGBARKEIT ===\")\n",
    "    print(f\"PLZ verf√ºgbar: {plz_available:,}/{len(combined_df):,} ({plz_available/len(combined_df)*100:.1f}%)\")\n",
    "    print(\"‚úÖ Bereit f√ºr PLZ-Enhancement mit Ortsteil und Koordinaten\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad070f",
   "metadata": {},
   "source": [
    "## 5.5. Erweiterte PLZ-Geolocation hinzuf√ºgen\n",
    "\n",
    "**üéØ Integration der verbesserten PLZ-Mapping-Datei**\n",
    "\n",
    "Anstatt nur auf Bezirksebene zu arbeiten, verwenden wir jetzt die neue `berlin_plz_mapping_enhanced.csv`, die:\n",
    "- **Ortsteil-Level-Genauigkeit** bietet (z.B. PLZ 12355 ‚Üí Rudow statt nur Neuk√∂lln)\n",
    "- **Echte Koordinaten** f√ºr jede PLZ enth√§lt (Lat/Lon)\n",
    "- **H√∂here r√§umliche Pr√§zision** f√ºr die Visualisierung erm√∂glicht\n",
    "\n",
    "Dies ist ein wichtiger Schritt zur Verbesserung der Datenqualit√§t und r√§umlichen Genauigkeit unserer Analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0410dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ERWEITERTE PLZ-GEOLOCATION HINZUF√úGEN\n",
      "============================================================\n",
      "‚úÖ PLZ-Mapping geladen: 190 Eintr√§ge\n",
      "   Spalten: ['PLZ', 'Ortsteil', 'Bezirk', 'Lat', 'Lon']\n",
      "\n",
      "=== BEISPIELE VERBESSERTER PLZ-MAPPINGS ===\n",
      "PLZ 10249: Friedrichshain (Friedrichshain-Kreuzberg) ‚Üí 52.5159, 13.4533\n",
      "PLZ 12355: Rudow (Neuk√∂lln) ‚Üí 52.4000, 13.4667\n",
      "PLZ 13347: Gesundbrunnen (Mitte) ‚Üí 52.5511, 13.3885\n",
      "PLZ 14050: Westend (Charlottenburg-Wilmersdorf) ‚Üí 52.5167, 13.2833\n",
      "PLZ 10553: Moabit (Mitte) ‚Üí 52.5280, 13.3430\n",
      "\n",
      "=== PLZ-VERF√úGBARKEIT IM KOMBINIERTEN DATASET ===\n",
      "PLZ verf√ºgbar: 17,273/17,487 (98.8%)\n",
      "Einzigartige PLZ: 190\n",
      "H√§ufigste PLZ:\n",
      "  10179: 1,805 Eintr√§ge\n",
      "  10249: 1,208 Eintr√§ge\n",
      "  14059: 1,172 Eintr√§ge\n",
      "  13189: 941 Eintr√§ge\n",
      "  12059: 761 Eintr√§ge\n",
      "\n",
      "=== DATENTYP-PROBLEM BEHEBEN ===\n",
      "PLZ-Datentyp im combined_df: object\n",
      "PLZ-Datentyp im mapping: object\n",
      "Beispiel PLZ aus combined_df: ['13591', '10179', '10999', '10787', '12527']\n",
      "Beispiel PLZ aus mapping: ['10115', '10117', '10119', '10178', '10179']\n",
      "\n",
      "=== PLZ-MAPPING-JOIN DURCHF√úHREN ===\n",
      "PLZ nach Bereinigung - Beispiele:\n",
      "  Combined_df: ['13591', '10179', '10999', '10787', '12527']\n",
      "  Mapping: ['10115', '10117', '10119', '10178', '10179']\n",
      "Join-Ergebnis:\n",
      "  Zeilen vorher: 17,487\n",
      "  Zeilen nachher: 17,487\n",
      "  Koordinaten matched: 17,131/17,487 (98.0%)\n",
      "\n",
      "‚úÖ ERFOLGREICHE MATCHES (Top 5):\n",
      "  PLZ 13591 ‚Üí Spandau (Spandau) ‚Üí 52.5333, 13.2000\n",
      "  PLZ 10179 ‚Üí Mitte (Mitte) ‚Üí 52.5200, 13.4050\n",
      "  PLZ 10999 ‚Üí Kreuzberg (Friedrichshain-Kreuzberg) ‚Üí 52.4987, 13.4030\n",
      "  PLZ 10787 ‚Üí Tiergarten (Mitte) ‚Üí 52.5147, 13.3507\n",
      "  PLZ 12527 ‚Üí Schm√∂ckwitz (Treptow-K√∂penick) ‚Üí 52.3667, 13.6500\n",
      "\n",
      "‚ö†Ô∏è  Nicht gematchte PLZ (Top 10):\n",
      "  13599: 64 Eintr√§ge\n",
      "  13629: 61 Eintr√§ge\n",
      "  13086: 15 Eintr√§ge\n",
      "  14612: 1 Eintr√§ge\n",
      "  10000: 1 Eintr√§ge\n",
      "\n",
      "‚úÖ Erweiterte PLZ-Geolocation erfolgreich hinzugef√ºgt!\n",
      "Neue Spalten: ortsteil, bezirk, lat, lon\n",
      "Finale Spalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'wol', 'plz', 'ortsteil', 'bezirk', 'lat', 'lon']\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ERWEITERTE PLZ-GEOLOCATION HINZUF√úGEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Lade die erweiterte PLZ-Mapping-Datei\n",
    "plz_mapping_file = 'data/processed/berlin_plz_mapping_enhanced.csv'\n",
    "try:\n",
    "    plz_mapping = pd.read_csv(plz_mapping_file, dtype={'PLZ': str})\n",
    "    print(f\"‚úÖ PLZ-Mapping geladen: {len(plz_mapping):,} Eintr√§ge\")\n",
    "    print(f\"   Spalten: {list(plz_mapping.columns)}\")\n",
    "    \n",
    "    # Zeige einige Beispiele\n",
    "    print(f\"\\n=== BEISPIELE VERBESSERTER PLZ-MAPPINGS ===\")\n",
    "    examples = ['10249', '12355', '13347', '14050', '10553']\n",
    "    for plz in examples:\n",
    "        if plz in plz_mapping['PLZ'].values:\n",
    "            row = plz_mapping[plz_mapping['PLZ'] == plz].iloc[0]\n",
    "            print(f\"PLZ {plz}: {row['Ortsteil']} ({row['Bezirk']}) ‚Üí {row['Lat']:.4f}, {row['Lon']:.4f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå PLZ-Mapping-Datei nicht gefunden: {plz_mapping_file}\")\n",
    "    print(\"‚ö†Ô∏è  Erstelle die Datei mit: python3 create_enhanced_plz_mapping_with_coords.py\")\n",
    "    plz_mapping = None\n",
    "\n",
    "# Pr√ºfe PLZ-Verf√ºgbarkeit im kombinierten Dataset\n",
    "if 'plz' in combined_df.columns:\n",
    "    print(f\"\\n=== PLZ-VERF√úGBARKEIT IM KOMBINIERTEN DATASET ===\")\n",
    "    plz_available = combined_df['plz'].notna().sum()\n",
    "    print(f\"PLZ verf√ºgbar: {plz_available:,}/{len(combined_df):,} ({plz_available/len(combined_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Zeige PLZ-Statistiken\n",
    "    unique_plz = combined_df['plz'].nunique()\n",
    "    print(f\"Einzigartige PLZ: {unique_plz}\")\n",
    "    \n",
    "    # Zeige h√§ufigste PLZ\n",
    "    plz_counts = combined_df['plz'].value_counts().head(5)\n",
    "    print(f\"H√§ufigste PLZ:\")\n",
    "    for plz, count in plz_counts.items():\n",
    "        print(f\"  {plz}: {count:,} Eintr√§ge\")\n",
    "    \n",
    "    # DATENTYP-PROBLEM BEHEBEN\n",
    "    print(f\"\\n=== DATENTYP-PROBLEM BEHEBEN ===\")\n",
    "    print(f\"PLZ-Datentyp im combined_df: {combined_df['plz'].dtype}\")\n",
    "    print(f\"PLZ-Datentyp im mapping: {plz_mapping['PLZ'].dtype}\")\n",
    "    \n",
    "    # Zeige ein paar Beispiel-PLZ aus combined_df\n",
    "    sample_plz = combined_df['plz'].dropna().head(5).tolist()\n",
    "    print(f\"Beispiel PLZ aus combined_df: {sample_plz}\")\n",
    "    \n",
    "    # Zeige ein paar Beispiel-PLZ aus mapping\n",
    "    sample_mapping_plz = plz_mapping['PLZ'].head(5).tolist()\n",
    "    print(f\"Beispiel PLZ aus mapping: {sample_mapping_plz}\")\n",
    "else:\n",
    "    print(\"‚ùå Keine PLZ-Spalte im kombinierten Dataset gefunden!\")\n",
    "\n",
    "# F√ºhre den Join durch (wenn m√∂glich)\n",
    "if plz_mapping is not None and 'plz' in combined_df.columns:\n",
    "    print(f\"\\n=== PLZ-MAPPING-JOIN DURCHF√úHREN ===\")\n",
    "    \n",
    "    # Bereite die Daten f√ºr den Join vor - KORRIGIERE DATENTYPEN\n",
    "    # Konvertiere Float-PLZ zu String und entferne .0\n",
    "    def clean_plz(plz_value):\n",
    "        if pd.isna(plz_value):\n",
    "            return None\n",
    "        # Konvertiere zu String und entferne .0 wenn es ein Float ist\n",
    "        plz_str = str(plz_value)\n",
    "        if plz_str.endswith('.0'):\n",
    "            plz_str = plz_str[:-2]\n",
    "        return plz_str\n",
    "    \n",
    "    # Bereite PLZ-Spalten vor\n",
    "    combined_df['plz_clean'] = combined_df['plz'].apply(clean_plz)\n",
    "    plz_mapping['PLZ'] = plz_mapping['PLZ'].astype(str)\n",
    "    \n",
    "    print(f\"PLZ nach Bereinigung - Beispiele:\")\n",
    "    sample_clean_plz = combined_df['plz_clean'].dropna().head(5).tolist()\n",
    "    print(f\"  Combined_df: {sample_clean_plz}\")\n",
    "    print(f\"  Mapping: {plz_mapping['PLZ'].head(5).tolist()}\")\n",
    "    \n",
    "    # Anzahl vor dem Join\n",
    "    rows_before = len(combined_df)\n",
    "    \n",
    "    # Join durchf√ºhren\n",
    "    combined_enhanced = combined_df.merge(\n",
    "        plz_mapping, \n",
    "        left_on='plz_clean', \n",
    "        right_on='PLZ', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Bereinige Spalten\n",
    "    combined_enhanced = combined_enhanced.drop(['PLZ', 'plz_clean'], axis=1)  # Duplikate entfernen\n",
    "    combined_enhanced = combined_enhanced.rename(columns={\n",
    "        'Ortsteil': 'ortsteil',\n",
    "        'Bezirk': 'bezirk', \n",
    "        'Lat': 'lat',\n",
    "        'Lon': 'lon'\n",
    "    })\n",
    "    \n",
    "    # Statistiken nach dem Join\n",
    "    rows_after = len(combined_enhanced)\n",
    "    matched_coords = combined_enhanced['lat'].notna().sum()\n",
    "    \n",
    "    print(f\"Join-Ergebnis:\")\n",
    "    print(f\"  Zeilen vorher: {rows_before:,}\")\n",
    "    print(f\"  Zeilen nachher: {rows_after:,}\")\n",
    "    print(f\"  Koordinaten matched: {matched_coords:,}/{rows_after:,} ({matched_coords/rows_after*100:.1f}%)\")\n",
    "    \n",
    "    # Zeige gematchte PLZ-Beispiele\n",
    "    if matched_coords > 0:\n",
    "        print(f\"\\n‚úÖ ERFOLGREICHE MATCHES (Top 5):\")\n",
    "        matched_data = combined_enhanced[combined_enhanced['lat'].notna()]\n",
    "        for _, row in matched_data.head(5).iterrows():\n",
    "            print(f\"  PLZ {row['plz']} ‚Üí {row['ortsteil']} ({row['bezirk']}) ‚Üí {row['lat']:.4f}, {row['lon']:.4f}\")\n",
    "    \n",
    "    # Zeige nicht gematchte PLZ\n",
    "    unmatched_plz = combined_enhanced[combined_enhanced['lat'].isna()]['plz'].value_counts().head(10)\n",
    "    if len(unmatched_plz) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Nicht gematchte PLZ (Top 10):\")\n",
    "        for plz, count in unmatched_plz.items():\n",
    "            if pd.notna(plz):\n",
    "                print(f\"  {plz}: {count:,} Eintr√§ge\")\n",
    "    \n",
    "    # Ersetze das combined_df durch das enhanced\n",
    "    combined_df = combined_enhanced\n",
    "    \n",
    "    print(f\"\\n‚úÖ Erweiterte PLZ-Geolocation erfolgreich hinzugef√ºgt!\")\n",
    "    print(f\"Neue Spalten: ortsteil, bezirk, lat, lon\")\n",
    "    print(f\"Finale Spalten: {list(combined_df.columns)}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå PLZ-Mapping-Join nicht m√∂glich\")\n",
    "    print(\"Grund: Fehlende PLZ-Mapping-Datei oder keine PLZ-Spalte im Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "648e53e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß PLZ-DATENTYP-REPARATUR\n",
      "==================================================\n",
      "Verf√ºgbare Spalten:\n",
      "  combined_df: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'wol', 'plz', 'ortsteil', 'bezirk', 'lat', 'lon']\n",
      "  plz_mapping: ['PLZ', 'Ortsteil', 'Bezirk', 'Lat', 'Lon']\n",
      "\n",
      "PLZ-Datentypen vor Reparatur:\n",
      "  combined_df['plz']: object\n",
      "  plz_mapping['PLZ']: object\n",
      "\n",
      "PLZ-Beispiele vor Reparatur:\n",
      "  combined_df PLZ:\n",
      "    13591 (Type: <class 'str'>)\n",
      "    10179 (Type: <class 'str'>)\n",
      "    10999 (Type: <class 'str'>)\n",
      "    10787 (Type: <class 'str'>)\n",
      "    12527 (Type: <class 'str'>)\n",
      "  plz_mapping PLZ:\n",
      "    10115 (Type: <class 'str'>)\n",
      "    10117 (Type: <class 'str'>)\n",
      "    10119 (Type: <class 'str'>)\n",
      "    10178 (Type: <class 'str'>)\n",
      "    10179 (Type: <class 'str'>)\n",
      "\n",
      "üö® KRITISCHE REPARATUR: PLZ-DATENTYP-HARMONISIERUNG\n",
      "============================================================\n",
      "\n",
      "PLZ-Datentypen nach Reparatur:\n",
      "  combined_df['plz']: object\n",
      "  plz_mapping['PLZ']: object\n",
      "\n",
      "PLZ-Beispiele nach Reparatur:\n",
      "  combined_df PLZ:\n",
      "    13591 (Type: <class 'str'>)\n",
      "    10179 (Type: <class 'str'>)\n",
      "    10999 (Type: <class 'str'>)\n",
      "    10787 (Type: <class 'str'>)\n",
      "    12527 (Type: <class 'str'>)\n",
      "  plz_mapping PLZ:\n",
      "    10115 (Type: <class 'str'>)\n",
      "    10117 (Type: <class 'str'>)\n",
      "    10119 (Type: <class 'str'>)\n",
      "    10178 (Type: <class 'str'>)\n",
      "    10179 (Type: <class 'str'>)\n",
      "\n",
      "üîç PLZ-MATCH-VALIDIERUNG\n",
      "========================================\n",
      "PLZ in combined_df: 190 eindeutige Werte\n",
      "PLZ in plz_mapping: 190 eindeutige Werte\n",
      "√úberschneidung: 188 PLZ k√∂nnen gematched werden\n",
      "‚úÖ PLZ-JOIN WIRD FUNKTIONIEREN!\n",
      "Beispiel-Matches:\n",
      "  12059 ist in beiden Datasets vorhanden\n",
      "  12557 ist in beiden Datasets vorhanden\n",
      "  13439 ist in beiden Datasets vorhanden\n",
      "  14057 ist in beiden Datasets vorhanden\n",
      "  12045 ist in beiden Datasets vorhanden\n",
      "\n",
      "‚úÖ PLZ-Datentyp-Reparatur abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# PLZ-DATENTYP-REPARATUR VOR JOIN\n",
    "# ===================================================================\n",
    "print(\"\\nüîß PLZ-DATENTYP-REPARATUR\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Pr√ºfe die verf√ºgbaren Spalten\n",
    "print(\"Verf√ºgbare Spalten:\")\n",
    "print(f\"  combined_df: {list(combined_df.columns)}\")\n",
    "print(f\"  plz_mapping: {list(plz_mapping.columns)}\")\n",
    "\n",
    "# Pr√ºfe PLZ-Datentypen vor der Reparatur\n",
    "print(\"\\nPLZ-Datentypen vor Reparatur:\")\n",
    "print(f\"  combined_df['plz']: {combined_df['plz'].dtype}\")\n",
    "\n",
    "# Identifiziere die PLZ-Spalte in plz_mapping\n",
    "if 'plz' in plz_mapping.columns:\n",
    "    plz_col = 'plz'\n",
    "elif 'PLZ' in plz_mapping.columns:\n",
    "    plz_col = 'PLZ'\n",
    "else:\n",
    "    print(\"‚ùå Keine PLZ-Spalte in plz_mapping gefunden!\")\n",
    "    print(f\"Verf√ºgbare Spalten: {list(plz_mapping.columns)}\")\n",
    "    plz_col = None\n",
    "\n",
    "if plz_col:\n",
    "    print(f\"  plz_mapping['{plz_col}']: {plz_mapping[plz_col].dtype}\")\n",
    "    \n",
    "    # Zeige PLZ-Beispiele vor Reparatur\n",
    "    print(\"\\nPLZ-Beispiele vor Reparatur:\")\n",
    "    print(\"  combined_df PLZ:\")\n",
    "    combined_plz_sample = combined_df['plz'].dropna().head(5)\n",
    "    for plz in combined_plz_sample:\n",
    "        print(f\"    {plz} (Type: {type(plz)})\")\n",
    "\n",
    "    print(f\"  plz_mapping {plz_col}:\")\n",
    "    mapping_plz_sample = plz_mapping[plz_col].head(5)\n",
    "    for plz in mapping_plz_sample:\n",
    "        print(f\"    {plz} (Type: {type(plz)})\")\n",
    "\n",
    "    # ===================================================================\n",
    "    # KRITISCHE REPARATUR: PLZ-DATENTYP-HARMONISIERUNG\n",
    "    # ===================================================================\n",
    "    print(\"\\nüö® KRITISCHE REPARATUR: PLZ-DATENTYP-HARMONISIERUNG\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # PROBLEM: combined_df['plz'] ist float (mit .0), plz_mapping['plz'] ist string\n",
    "    # L√ñSUNG: Beide zu String konvertieren\n",
    "\n",
    "    # Repariere combined_df PLZ: float ‚Üí string\n",
    "    combined_df['plz'] = combined_df['plz'].apply(\n",
    "        lambda x: str(int(x)) if pd.notna(x) and x != '' else None\n",
    "    )\n",
    "\n",
    "    # Repariere plz_mapping PLZ: Stelle sicher, dass es string ist\n",
    "    plz_mapping[plz_col] = plz_mapping[plz_col].astype(str)\n",
    "\n",
    "    # Pr√ºfe PLZ-Datentypen nach der Reparatur\n",
    "    print(\"\\nPLZ-Datentypen nach Reparatur:\")\n",
    "    print(f\"  combined_df['plz']: {combined_df['plz'].dtype}\")\n",
    "    print(f\"  plz_mapping['{plz_col}']: {plz_mapping[plz_col].dtype}\")\n",
    "\n",
    "    # Zeige PLZ-Beispiele nach Reparatur\n",
    "    print(\"\\nPLZ-Beispiele nach Reparatur:\")\n",
    "    print(\"  combined_df PLZ:\")\n",
    "    combined_plz_sample_after = combined_df['plz'].dropna().head(5)\n",
    "    for plz in combined_plz_sample_after:\n",
    "        print(f\"    {plz} (Type: {type(plz)})\")\n",
    "\n",
    "    print(f\"  plz_mapping {plz_col}:\")\n",
    "    mapping_plz_sample_after = plz_mapping[plz_col].head(5)\n",
    "    for plz in mapping_plz_sample_after:\n",
    "        print(f\"    {plz} (Type: {type(plz)})\")\n",
    "\n",
    "    # Validiere, dass PLZ-Werte jetzt matchbar sind\n",
    "    print(\"\\nüîç PLZ-MATCH-VALIDIERUNG\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Pr√ºfe √úberschneidungen\n",
    "    combined_plz_unique = set(combined_df['plz'].dropna().unique())\n",
    "    mapping_plz_unique = set(plz_mapping[plz_col].unique())\n",
    "\n",
    "    overlap = combined_plz_unique.intersection(mapping_plz_unique)\n",
    "    print(f\"PLZ in combined_df: {len(combined_plz_unique):,} eindeutige Werte\")\n",
    "    print(f\"PLZ in plz_mapping: {len(mapping_plz_unique):,} eindeutige Werte\")\n",
    "    print(f\"√úberschneidung: {len(overlap):,} PLZ k√∂nnen gematched werden\")\n",
    "\n",
    "    if len(overlap) > 0:\n",
    "        print(f\"‚úÖ PLZ-JOIN WIRD FUNKTIONIEREN!\")\n",
    "        print(f\"Beispiel-Matches:\")\n",
    "        for plz in list(overlap)[:5]:\n",
    "            print(f\"  {plz} ist in beiden Datasets vorhanden\")\n",
    "    else:\n",
    "        print(f\"‚ùå PLZ-JOIN WIRD FEHLSCHLAGEN!\")\n",
    "        print(\"Grund: Keine √úberschneidung zwischen den PLZ-Werten\")\n",
    "        \n",
    "        # Zeige PLZ-Beispiele zum Debugging\n",
    "        print(\"\\nPLZ-Beispiele f√ºr Debugging:\")\n",
    "        print(\"  combined_df PLZ (erste 10):\")\n",
    "        for plz in list(combined_plz_unique)[:10]:\n",
    "            print(f\"    '{plz}'\")\n",
    "        print(\"  plz_mapping PLZ (erste 10):\")\n",
    "        for plz in list(mapping_plz_unique)[:10]:\n",
    "            print(f\"    '{plz}'\")\n",
    "\n",
    "    print(f\"\\n‚úÖ PLZ-Datentyp-Reparatur abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81123c9",
   "metadata": {},
   "source": [
    "## 6. Finale Datenvalidierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c95a414f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINALE DATENVALIDIERUNG\n",
      "============================================================\n",
      "Duplikate: 1,233 (7.05%)\n",
      "\n",
      "=== FEHLENDE WERTE ===\n",
      "  rooms: 4,424 (25.3%)\n",
      "  wol: 5,252 (30.0%)\n",
      "  plz: 214 (1.2%)\n",
      "  ortsteil: 216 (1.2%)\n",
      "  bezirk: 216 (1.2%)\n",
      "  lat: 356 (2.0%)\n",
      "  lon: 356 (2.0%)\n",
      "\n",
      "=== GEOLOCATION-QUALIT√ÑT ===\n",
      "Vollst√§ndige Koordinaten: 17,131/17,487 (98.0%)\n",
      "Koordinaten in Berlin-Bounds: 17,131/17,131 (100.0%)\n",
      "\n",
      "=== ORTSTEIL-VERTEILUNG ===\n",
      "Anzahl Ortsteile: 79\n",
      "Top 10 Ortsteile:\n",
      "  Mitte: 1,927 Eintr√§ge (11.0%)\n",
      "  Friedrichshain: 1,427 Eintr√§ge (8.2%)\n",
      "  Charlottenburg: 1,383 Eintr√§ge (7.9%)\n",
      "  Pankow: 945 Eintr√§ge (5.4%)\n",
      "  Neuk√∂lln: 879 Eintr√§ge (5.0%)\n",
      "  Tiergarten: 764 Eintr√§ge (4.4%)\n",
      "  Sch√∂neberg: 729 Eintr√§ge (4.2%)\n",
      "  Prenzlauer Berg: 553 Eintr√§ge (3.2%)\n",
      "  Spandau: 476 Eintr√§ge (2.7%)\n",
      "  Wilmersdorf: 418 Eintr√§ge (2.4%)\n",
      "\n",
      "=== STATISTIKEN KOMBINIERTES DATASET ===\n",
      "Preis - Min: 150‚Ç¨, Max: 9990‚Ç¨, Median: 931‚Ç¨\n",
      "Gr√∂√üe - Min: 10m¬≤, Max: 482m¬≤, Median: 69m¬≤\n",
      "Zimmer - Min: 1.0, Max: 10.0, Median: 2.0\n",
      "\n",
      "=== BEZIRKSVERTEILUNG (ORIGINAL) ===\n",
      "Anzahl Bezirke: 87\n",
      "  Mitte: 1,923 Eintr√§ge (11.0%)\n",
      "  Pankow: 1,189 Eintr√§ge (6.8%)\n",
      "  Neuk√∂lln: 904 Eintr√§ge (5.2%)\n",
      "  Spandau: 837 Eintr√§ge (4.8%)\n",
      "  Tiergarten: 832 Eintr√§ge (4.8%)\n",
      "  Charlottenburg: 792 Eintr√§ge (4.5%)\n",
      "  Friedrichshain-Kreuzberg: 775 Eintr√§ge (4.4%)\n",
      "  Friedrichshain: 666 Eintr√§ge (3.8%)\n",
      "  Charlottenburg-Wilmersdorf: 636 Eintr√§ge (3.6%)\n",
      "  Reinickendorf: 614 Eintr√§ge (3.5%)\n",
      "\n",
      "=== BEZIRKSVERTEILUNG (PLZ-ENHANCED) ===\n",
      "Anzahl Bezirke: 13\n",
      "  Mitte: 3,273 Eintr√§ge (18.7%)\n",
      "  Charlottenburg-Wilmersdorf: 2,044 Eintr√§ge (11.7%)\n",
      "  Friedrichshain-Kreuzberg: 1,797 Eintr√§ge (10.3%)\n",
      "  Pankow: 1,540 Eintr√§ge (8.8%)\n",
      "  Treptow-K√∂penick: 1,428 Eintr√§ge (8.2%)\n",
      "  Tempelhof-Sch√∂neberg: 1,250 Eintr√§ge (7.1%)\n",
      "  Neuk√∂lln: 1,111 Eintr√§ge (6.4%)\n",
      "  Spandau: 1,077 Eintr√§ge (6.2%)\n",
      "  Reinickendorf: 849 Eintr√§ge (4.9%)\n",
      "  Steglitz-Zehlendorf: 839 Eintr√§ge (4.8%)\n",
      "\n",
      "‚úÖ Finale Datenvalidierung abgeschlossen\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FINALE DATENVALIDIERUNG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Duplikate pr√ºfen\n",
    "duplicates = combined_df.duplicated().sum()\n",
    "print(f\"Duplikate: {duplicates:,} ({duplicates/len(combined_df)*100:.2f}%)\")\n",
    "\n",
    "# Fehlende Werte\n",
    "print(f\"\\n=== FEHLENDE WERTE ===\")\n",
    "missing_summary = combined_df.isnull().sum()\n",
    "for col, missing_count in missing_summary.items():\n",
    "    if missing_count > 0:\n",
    "        print(f\"  {col}: {missing_count:,} ({missing_count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "# Geolocation-Qualit√§t pr√ºfen\n",
    "if 'lat' in combined_df.columns and 'lon' in combined_df.columns:\n",
    "    print(f\"\\n=== GEOLOCATION-QUALIT√ÑT ===\")\n",
    "    coords_available = combined_df[['lat', 'lon']].notna().all(axis=1).sum()\n",
    "    print(f\"Vollst√§ndige Koordinaten: {coords_available:,}/{len(combined_df):,} ({coords_available/len(combined_df)*100:.1f}%)\")\n",
    "    \n",
    "    if coords_available > 0:\n",
    "        # Koordinaten-Plausibilit√§t pr√ºfen (Berlin bounds)\n",
    "        berlin_bounds = {\n",
    "            'lat_min': 52.3, 'lat_max': 52.7,\n",
    "            'lon_min': 13.0, 'lon_max': 13.8\n",
    "        }\n",
    "        \n",
    "        valid_coords = combined_df[\n",
    "            (combined_df['lat'] >= berlin_bounds['lat_min']) & \n",
    "            (combined_df['lat'] <= berlin_bounds['lat_max']) & \n",
    "            (combined_df['lon'] >= berlin_bounds['lon_min']) & \n",
    "            (combined_df['lon'] <= berlin_bounds['lon_max'])\n",
    "        ]\n",
    "        \n",
    "        print(f\"Koordinaten in Berlin-Bounds: {len(valid_coords):,}/{coords_available:,} ({len(valid_coords)/coords_available*100:.1f}%)\")\n",
    "\n",
    "# Ortsteil-Verteilung\n",
    "if 'ortsteil' in combined_df.columns:\n",
    "    print(f\"\\n=== ORTSTEIL-VERTEILUNG ===\")\n",
    "    ortsteil_counts = combined_df['ortsteil'].value_counts()\n",
    "    print(f\"Anzahl Ortsteile: {len(ortsteil_counts)}\")\n",
    "    print(f\"Top 10 Ortsteile:\")\n",
    "    for ortsteil, count in ortsteil_counts.head(10).items():\n",
    "        print(f\"  {ortsteil}: {count:,} Eintr√§ge ({count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "# Statistiken der Kernfelder\n",
    "print(f\"\\n=== STATISTIKEN KOMBINIERTES DATASET ===\")\n",
    "if 'price' in combined_df.columns:\n",
    "    price_stats = combined_df['price'].describe()\n",
    "    print(f\"Preis - Min: {price_stats['min']:.0f}‚Ç¨, Max: {price_stats['max']:.0f}‚Ç¨, Median: {price_stats['50%']:.0f}‚Ç¨\")\n",
    "\n",
    "if 'size' in combined_df.columns:\n",
    "    size_stats = combined_df['size'].describe()\n",
    "    print(f\"Gr√∂√üe - Min: {size_stats['min']:.0f}m¬≤, Max: {size_stats['max']:.0f}m¬≤, Median: {size_stats['50%']:.0f}m¬≤\")\n",
    "\n",
    "if 'rooms' in combined_df.columns:\n",
    "    rooms_stats = combined_df['rooms'].describe()\n",
    "    print(f\"Zimmer - Min: {rooms_stats['min']:.1f}, Max: {rooms_stats['max']:.1f}, Median: {rooms_stats['50%']:.1f}\")\n",
    "\n",
    "# Bezirksverteilung (alt vs neu)\n",
    "if 'district' in combined_df.columns:\n",
    "    print(f\"\\n=== BEZIRKSVERTEILUNG (ORIGINAL) ===\")\n",
    "    district_counts = combined_df['district'].value_counts()\n",
    "    print(f\"Anzahl Bezirke: {len(district_counts)}\")\n",
    "    for district, count in district_counts.head(10).items():\n",
    "        print(f\"  {district}: {count:,} Eintr√§ge ({count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "if 'bezirk' in combined_df.columns:\n",
    "    print(f\"\\n=== BEZIRKSVERTEILUNG (PLZ-ENHANCED) ===\")\n",
    "    bezirk_counts = combined_df['bezirk'].value_counts()\n",
    "    print(f\"Anzahl Bezirke: {len(bezirk_counts)}\")\n",
    "    for bezirk, count in bezirk_counts.head(10).items():\n",
    "        print(f\"  {bezirk}: {count:,} Eintr√§ge ({count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Finale Datenvalidierung abgeschlossen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4279a7",
   "metadata": {},
   "source": [
    "## 7. Export des finalen Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13c3b62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT FINALES KOMBINIERTES DATASET\n",
      "============================================================\n",
      "‚úÖ Finales Dataset exportiert: data/processed/berlin_housing_combined_enriched_final.csv\n",
      "Dateigr√∂√üe: 17,487 Zeilen x 13 Spalten\n",
      "‚úÖ Export-Validierung erfolgreich: 17,487 Zeilen geladen\n",
      "\n",
      "=== FINALE ZUSAMMENFASSUNG ===\n",
      "Input-Datasets: 3\n",
      "  - Dataset 2018-2019: 10,387 Zeilen\n",
      "  - Dataset 2022: 2,676 Zeilen\n",
      "  - Dataset 2025: 4,424 Zeilen\n",
      "Output: data/processed/berlin_housing_combined_enriched_final.csv (17,487 Zeilen)\n",
      "Zeitspanne: 2018-2025 (3 Jahre)\n",
      "Berliner Bezirke (PLZ-Enhanced): 14 abgedeckt\n",
      "Berliner Ortsteile: 80 abgedeckt\n",
      "Geolocation-Koordinaten: 17,131/17,487 (98.0%)\n",
      "Standardisierte Spalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'wol', 'plz', 'ortsteil', 'bezirk', 'lat', 'lon']\n",
      "\n",
      "üéØ VERBESSERUNGEN DURCH PLZ-ENHANCEMENT:\n",
      "üìç Ortsteil-Level-Genauigkeit statt nur Bezirks-Level\n",
      "üó∫Ô∏è  Echte Koordinaten f√ºr pr√§zise Geolocation\n",
      "üìä Beispiel: PLZ 12355 ‚Üí Rudow (Neuk√∂lln) statt nur 'Neuk√∂lln'\n",
      "üé® Bereit f√ºr hochaufl√∂sende Karten und Visualisierungen\n",
      "\n",
      "üéâ DATASET-KOMBINATION ERFOLGREICH ABGESCHLOSSEN!\n",
      "Das finale Dataset ist bereit f√ºr die **AWESOME** Mietpreis-Analyse mit pr√§ziser Geolocation.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPORT FINALES KOMBINIERTES DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Export\n",
    "output_file = 'data/processed/berlin_housing_combined_enriched_final.csv'\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Finales Dataset exportiert: {output_file}\")\n",
    "print(f\"Dateigr√∂√üe: {len(combined_df):,} Zeilen x {len(combined_df.columns)} Spalten\")\n",
    "\n",
    "# Validierung durch Wiedereinlesen\n",
    "test_df = pd.read_csv(output_file)\n",
    "print(f\"‚úÖ Export-Validierung erfolgreich: {len(test_df):,} Zeilen geladen\")\n",
    "\n",
    "# Finale Zusammenfassung\n",
    "print(f\"\\n=== FINALE ZUSAMMENFASSUNG ===\")\n",
    "print(f\"Input-Datasets: {len(datasets)}\")\n",
    "print(f\"  - Dataset 2018-2019: {quality_report.get('2018_2019', {}).get('total_rows', 0):,} Zeilen\")\n",
    "print(f\"  - Dataset 2022: {quality_report.get('2022', {}).get('total_rows', 0):,} Zeilen\") \n",
    "print(f\"  - Dataset 2025: {quality_report.get('2025', {}).get('total_rows', 0):,} Zeilen\")\n",
    "print(f\"Output: {output_file} ({len(combined_df):,} Zeilen)\")\n",
    "print(f\"Zeitspanne: 2018-2025 ({len(combined_df['year'].unique())} Jahre)\")\n",
    "\n",
    "# Geolocation-Zusammenfassung\n",
    "if 'bezirk' in combined_df.columns:\n",
    "    print(f\"Berliner Bezirke (PLZ-Enhanced): {len(combined_df['bezirk'].unique())} abgedeckt\")\n",
    "else:\n",
    "    print(f\"Berliner Bezirke: {len(combined_df['district'].unique())} abgedeckt\")\n",
    "\n",
    "if 'ortsteil' in combined_df.columns:\n",
    "    print(f\"Berliner Ortsteile: {len(combined_df['ortsteil'].unique())} abgedeckt\")\n",
    "\n",
    "if 'lat' in combined_df.columns and 'lon' in combined_df.columns:\n",
    "    coords_available = combined_df[['lat', 'lon']].notna().all(axis=1).sum()\n",
    "    print(f\"Geolocation-Koordinaten: {coords_available:,}/{len(combined_df):,} ({coords_available/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"Standardisierte Spalten: {list(combined_df.columns)}\")\n",
    "\n",
    "# Verbesserungen hervorheben\n",
    "print(f\"\\nüéØ VERBESSERUNGEN DURCH PLZ-ENHANCEMENT:\")\n",
    "print(f\"üìç Ortsteil-Level-Genauigkeit statt nur Bezirks-Level\")\n",
    "print(f\"üó∫Ô∏è  Echte Koordinaten f√ºr pr√§zise Geolocation\")\n",
    "print(f\"üìä Beispiel: PLZ 12355 ‚Üí Rudow (Neuk√∂lln) statt nur 'Neuk√∂lln'\")\n",
    "print(f\"üé® Bereit f√ºr hochaufl√∂sende Karten und Visualisierungen\")\n",
    "\n",
    "print(f\"\\nüéâ DATASET-KOMBINATION ERFOLGREICH ABGESCHLOSSEN!\")\n",
    "print(f\"Das finale Dataset ist bereit f√ºr die **AWESOME** Mietpreis-Analyse mit pr√§ziser Geolocation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecaa8ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PLZ VALIDATION: COMBINED DATASET\n",
      "============================================================\n",
      "‚úÖ Combined dataset loaded: 17,487 rows\n",
      "‚úÖ PLZ coverage: 4,492/17,487 (25.7%)\n",
      "‚úÖ PLZ data type: string\n",
      "‚úÖ Sample PLZ values: ['13591', '12527', '13053', '13158', '14199']\n",
      "‚úÖ No .0 suffixes detected - PLZ properly stored as string\n",
      "‚úÖ PLZ data type is correct (string)\n",
      "\n",
      "üìä PLZ Coverage by Year:\n",
      "   2019: 1,760/10,387 (16.9%)\n",
      "   2022: 2,676/2,676 (100.0%)\n",
      "   2025: 56/4,424 (1.3%)\n",
      "\n",
      "üéØ VALIDATION RESULT:\n",
      "   ‚úÖ SUCCESS: PLZ data is properly formatted as string\n",
      "   Ready for downstream geospatial analysis and mapping\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# PLZ VALIDATION: Combined Dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"PLZ VALIDATION: COMBINED DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load the combined dataset with proper PLZ type\n",
    "combined_file = \"data/processed/berlin_housing_combined_enriched_final.csv\"\n",
    "combined_df_check = pd.read_csv(combined_file, dtype={'plz': 'string'})\n",
    "print(f\"‚úÖ Combined dataset loaded: {len(combined_df_check):,} rows\")\n",
    "\n",
    "# Check PLZ coverage and type\n",
    "plz_count = combined_df_check['plz'].count()\n",
    "plz_percentage = (plz_count / len(combined_df_check)) * 100\n",
    "print(f\"‚úÖ PLZ coverage: {plz_count:,}/{len(combined_df_check):,} ({plz_percentage:.1f}%)\")\n",
    "print(f\"‚úÖ PLZ data type: {combined_df_check['plz'].dtype}\")\n",
    "\n",
    "# Sample PLZ values\n",
    "sample_plz = combined_df_check['plz'].dropna().head().tolist()\n",
    "print(f\"‚úÖ Sample PLZ values: {sample_plz}\")\n",
    "\n",
    "# Check if PLZ has .0 suffixes\n",
    "sample_plz_str = [str(plz) for plz in sample_plz]\n",
    "has_dot_zero = any('.0' in str(plz) for plz in sample_plz_str)\n",
    "if has_dot_zero:\n",
    "    print(\"‚ö†Ô∏è WARNING: .0 suffixes detected in PLZ values!\")\n",
    "    print(\"   This indicates PLZ is stored as float, not string\")\n",
    "    print(\"   NEEDS FIXING: Re-run combination with dtype={'plz': 'string'}\")\n",
    "    needs_fix = True\n",
    "else:\n",
    "    print(\"‚úÖ No .0 suffixes detected - PLZ properly stored as string\")\n",
    "    needs_fix = False\n",
    "\n",
    "# Additional validation\n",
    "if combined_df_check['plz'].dtype == 'string':\n",
    "    print(\"‚úÖ PLZ data type is correct (string)\")\n",
    "elif combined_df_check['plz'].dtype == 'object':\n",
    "    print(\"‚úÖ PLZ data type is acceptable (object - likely string)\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è WARNING: PLZ stored as {combined_df_check['plz'].dtype}\")\n",
    "    print(\"   Should be string type for proper processing\")\n",
    "    print(\"   NEEDS FIXING: Re-run combination with dtype={'plz': 'string'}\")\n",
    "    needs_fix = True\n",
    "\n",
    "# PLZ coverage by year\n",
    "print(\"\\nüìä PLZ Coverage by Year:\")\n",
    "for year in sorted(combined_df_check['year'].unique()):\n",
    "    year_data = combined_df_check[combined_df_check['year'] == year]\n",
    "    year_plz_count = year_data['plz'].count()\n",
    "    year_plz_pct = (year_plz_count / len(year_data)) * 100\n",
    "    print(f\"   {year}: {year_plz_count:,}/{len(year_data):,} ({year_plz_pct:.1f}%)\")\n",
    "\n",
    "print(\"\\nüéØ VALIDATION RESULT:\")\n",
    "if needs_fix:\n",
    "    print(\"   ‚ö†Ô∏è NEEDS FIXING: PLZ data type or format issues detected\")\n",
    "    print(\"   Action: Re-run combination with proper dtype={'plz': 'string'}\")\n",
    "else:\n",
    "    print(\"   ‚úÖ SUCCESS: PLZ data is properly formatted as string\")\n",
    "    print(\"   Ready for downstream geospatial analysis and mapping\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ed4aa1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "IN-MEMORY COMBINED DATAFRAME VALIDATION\n",
      "============================================================\n",
      "‚úÖ Combined dataframe rows: 17,487\n",
      "‚úÖ PLZ coverage: 4,492/17,487 (25.7%)\n",
      "‚úÖ PLZ data type: object\n",
      "‚úÖ Sample PLZ values: ['13591', '12527', '13053', '13158', '14199']\n",
      "‚úÖ Sample PLZ as string: ['13591', '12527', '13053', '13158', '14199']\n",
      "‚úÖ No .0 suffixes detected - PLZ appears to be proper string format\n",
      "\n",
      "============================================================\n",
      "DISK FILE VALIDATION\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Check the current in-memory combined dataframe\n",
    "print(\"=\" * 60)\n",
    "print(\"IN-MEMORY COMBINED DATAFRAME VALIDATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ Combined dataframe rows: {len(combined_enhanced):,}\")\n",
    "print(f\"‚úÖ PLZ coverage: {combined_enhanced['plz'].count():,}/{len(combined_enhanced):,} ({combined_enhanced['plz'].count()/len(combined_enhanced)*100:.1f}%)\")\n",
    "print(f\"‚úÖ PLZ data type: {combined_enhanced['plz'].dtype}\")\n",
    "print(f\"‚úÖ Sample PLZ values: {combined_enhanced['plz'].dropna().head().tolist()}\")\n",
    "\n",
    "# Check if PLZ has .0 suffixes\n",
    "sample_plz_str = combined_enhanced['plz'].dropna().head().astype(str).tolist()\n",
    "has_dot_zero = any('.0' in str(plz) for plz in sample_plz_str)\n",
    "print(f\"‚úÖ Sample PLZ as string: {sample_plz_str}\")\n",
    "if has_dot_zero:\n",
    "    print(\"‚ö†Ô∏è WARNING: .0 suffixes detected in PLZ values!\")\n",
    "    print(\"   This indicates PLZ is stored as float, not string\")\n",
    "else:\n",
    "    print(\"‚úÖ No .0 suffixes detected - PLZ appears to be proper string format\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DISK FILE VALIDATION\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7f88fcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-exporting corrected combined dataframe...\n",
      "‚úÖ Corrected dataset re-exported: data/processed/berlin_housing_combined_enriched_final.csv\n",
      "‚úÖ Export validation: 17,487 rows loaded\n",
      "‚úÖ PLZ type after re-export: string\n",
      "‚úÖ Sample PLZ values: ['13591', '12527', '13053', '13158', '14199']\n",
      "\n",
      "üéâ CORRECTED DATASET SUCCESSFULLY EXPORTED!\n",
      "PLZ data is now properly stored as string type without .0 suffixes\n"
     ]
    }
   ],
   "source": [
    "# Re-export the corrected combined dataframe\n",
    "print(\"Re-exporting corrected combined dataframe...\")\n",
    "output_file = \"data/processed/berlin_housing_combined_enriched_final.csv\"\n",
    "combined_enhanced.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Corrected dataset re-exported: {output_file}\")\n",
    "\n",
    "# Verify the export\n",
    "test_df = pd.read_csv(output_file, dtype={'plz': 'string'})\n",
    "print(f\"‚úÖ Export validation: {len(test_df):,} rows loaded\")\n",
    "print(f\"‚úÖ PLZ type after re-export: {test_df['plz'].dtype}\")\n",
    "print(f\"‚úÖ Sample PLZ values: {test_df['plz'].dropna().head().tolist()}\")\n",
    "\n",
    "print(\"\\nüéâ CORRECTED DATASET SUCCESSFULLY EXPORTED!\")\n",
    "print(\"PLZ data is now properly stored as string type without .0 suffixes\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
