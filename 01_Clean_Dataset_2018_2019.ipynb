{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70f42e6",
   "metadata": {},
   "source": [
    "# Dataset 2018-2019 Bereinigung und Normalisierung\n",
    "## Spezialisiertes Modul f√ºr Kaggle/Immobilienscout24 Dataset\n",
    "\n",
    "### Ziel\n",
    "Bereinigung und Normalisierung des historischen Datasets (2018-2019) in ein standardisiertes Format f√ºr die gemeinsame Analyse.\n",
    "\n",
    "### Input\n",
    "- `data/raw/Dataset_2018_2019.csv`\n",
    "\n",
    "### Output\n",
    "- `data/processed/dataset_2018_2019_normalized.csv`\n",
    "\n",
    "### Standardisierte Ausgabespalten\n",
    "- `price`: Normalisierter Preis (Kaltmiete in ‚Ç¨)\n",
    "- `size`: Normalisierte Gr√∂√üe (m¬≤)\n",
    "- `district`: Berliner Bezirk (standardisiert)\n",
    "- `rooms`: Anzahl Zimmer\n",
    "- `year`: Jahr des Datasets (2019)\n",
    "- `dataset_id`: Eindeutige Dataset-Kennzeichnung (historical)\n",
    "- `source`: Datenquelle\n",
    "\n",
    "---\n",
    "**Teil der modularen Preprocessing-Pipeline**  \n",
    "**Datum:** 4. Juli 2025  \n",
    "**Version:** 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ef7a1",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3a55584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotheken erfolgreich importiert!\n",
      "Pandas Version: 2.3.0\n",
      "Dataset: 2018-2019 (Kaggle/Immobilienscout24)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "print(\"Bibliotheken erfolgreich importiert!\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"Dataset: 2018-2019 (Kaggle/Immobilienscout24)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e7e8f",
   "metadata": {},
   "source": [
    "## 2. Daten laden und erste Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee850b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET 2018-2019 LADEN UND ANALYSIEREN\n",
      "============================================================\n",
      "Dataset geladen: 10,406 Zeilen, 9 Spalten\n",
      "\n",
      "Spalten: ['regio3', 'street', 'livingSpace', 'baseRent', 'totalRent', 'noRooms', 'floor', 'typeOfFlat', 'yearConstructed']\n",
      "\n",
      "Datentypen:\n",
      "regio3              object\n",
      "street              object\n",
      "livingSpace        float64\n",
      "baseRent           float64\n",
      "totalRent          float64\n",
      "noRooms            float64\n",
      "floor              float64\n",
      "typeOfFlat          object\n",
      "yearConstructed    float64\n",
      "dtype: object\n",
      "\n",
      "Fehlende Werte:\n",
      "  totalRent: 662 (6.36%)\n",
      "  floor: 1100 (10.57%)\n",
      "  typeOfFlat: 804 (7.73%)\n",
      "  yearConstructed: 1425 (13.69%)\n",
      "\n",
      "Erste 5 Zeilen:\n",
      "            regio3                      street  livingSpace  baseRent  totalRent  noRooms  floor    typeOfFlat  yearConstructed\n",
      "0  Staaken_Spandau           Metropolitan Park        77.00    820.00    1140.00      3.0    0.0  ground_floor              NaN\n",
      "1        Wei√üensee      B&ouml;rnestra&szlig;e        62.63    808.00     955.00      2.0    0.0  ground_floor           1918.0\n",
      "2            Mitte  Stallschreiberstra&szlig;e        46.40   1150.00    1300.00      2.0    3.0     apartment           2019.0\n",
      "3        Kreuzberg      Hallesche Stra&szlig;e        67.00   1200.00    1428.78      2.5    6.0     apartment           2017.0\n",
      "4       Tiergarten           Heidestra&szlig;e        73.54   1338.43    1559.05      2.0    0.0  ground_floor           2019.0\n"
     ]
    }
   ],
   "source": [
    "# Lade Dataset 2018-2019\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET 2018-2019 LADEN UND ANALYSIEREN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Lade Rohdaten\n",
    "df_raw = pd.read_csv('data/raw/Dataset_2018_2019.csv')\n",
    "print(f\"Dataset geladen: {df_raw.shape[0]:,} Zeilen, {df_raw.shape[1]} Spalten\")\n",
    "\n",
    "# Grundlegende Informationen\n",
    "print(f\"\\nSpalten: {list(df_raw.columns)}\")\n",
    "print(f\"\\nDatentypen:\")\n",
    "print(df_raw.dtypes)\n",
    "\n",
    "# Fehlende Werte\n",
    "print(f\"\\nFehlende Werte:\")\n",
    "missing_values = df_raw.isnull().sum()\n",
    "missing_pct = (missing_values / len(df_raw) * 100).round(2)\n",
    "for col in missing_values[missing_values > 0].index:\n",
    "    print(f\"  {col}: {missing_values[col]} ({missing_pct[col]}%)\")\n",
    "\n",
    "# Erste 5 Zeilen\n",
    "print(f\"\\nErste 5 Zeilen:\")\n",
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fadae8",
   "metadata": {},
   "source": [
    "## 3. Spezifische Bereinigung Dataset 2018-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f6ec445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPEZIFISCHE BEREINIGUNG DATASET 2018-2019\n",
      "============================================================\n",
      "Arbeitskopie erstellt: 10406 Zeilen\n",
      "\n",
      "=== PREIS-BEREINIGUNG ===\n",
      "baseRent - Statistik vor Bereinigung:\n",
      "  Typ: float64\n",
      "  Nicht-null Werte: 10406\n",
      "  Min: 0.0, Max: 20000.0\n",
      "Entfernte unrealistische Preise: 11\n",
      "\n",
      "=== GR√ñSSEN-BEREINIGUNG ===\n",
      "livingSpace - Statistik vor Bereinigung:\n",
      "  Typ: float64\n",
      "  Nicht-null Werte: 10395\n",
      "  Min: 3.0, Max: 542.53\n",
      "Entfernte unrealistische Gr√∂√üen: 7\n",
      "\n",
      "=== BEZIRKS-NORMALISIERUNG ===\n",
      "regio3 - Einzigartige Werte: 79\n",
      "Bezirke: ['Adlershof_Treptow', 'Alt_Hohensch√∂nhausen_Hohensch√∂nhausen', 'Altglienicke_Treptow', 'Baumschulenweg_Treptow', 'Biesdorf_Marzahn', 'Blankenburg_Wei√üensee', 'Bohnsdorf_Treptow', 'Britz_Neuk√∂lln', 'Buch_Pankow', 'Buckow_Neuk√∂lln', 'Charlottenburg', 'Dahlem_Zehlendorf', 'Falkenberg_Hohensch√∂nhausen', 'Franz√∂sisch_Buchholz_Pankow', 'Friedenau_Sch√∂neberg', 'Friedrichsfelde_Lichtenberg', 'Friedrichshagen_K√∂penick', 'Friedrichshain', 'Frohnau_Reinickendorf', 'Gatow_Spandau', 'Grunewald_Wilmersdorf', 'Gr√ºnau_K√∂penick', 'Haselhorst_Spandau', 'Heiligensee_Reinickendorf', 'Heinersdorf_Wei√üensee', 'Hellersdorf', 'Hermsdorf_Reinickendorf', 'Johannisthal_Treptow', 'Karlshorst_Lichtenberg', 'Karow_Wei√üensee', 'Kaulsdorf_Hellersdorf', 'Kladow_Spandau', 'Konradsh√∂he_Reinickendorf', 'Kreuzberg', 'K√∂penick', 'Lankwitz_Steglitz', 'Lichtenberg', 'Lichtenrade_Tempelhof', 'Lichterfelde_Steglitz', 'L√ºbars_Reinickendorf', 'Mahlsdorf_Hellersdorf', 'Malchow_Hohensch√∂nhausen', 'Mariendorf_Tempelhof', 'Marienfelde_Tempelhof', 'Marzahn', 'Mitte', 'M√ºggelheim_K√∂penick', 'Neu_Hohensch√∂nhausen_Hohensch√∂nhausen', 'Neuk√∂lln', 'Niedersch√∂neweide_Treptow', 'Niedersch√∂nhausen_Pankow', 'Nikolassee_Zehlendorf', 'Obersch√∂neweide_K√∂penick', 'Pankow', 'Pl√§nterwald_Treptow', 'Prenzlauer_Berg_Prenzlauer_Berg', 'Rahnsdorf_K√∂penick', 'Reinickendorf', 'Rosenthal_Pankow', 'Rudow_Neuk√∂lln', 'Rummelsburg_Lichtenberg', 'Schmargendorf_Wilmersdorf', 'Schm√∂ckwitz_K√∂penick', 'Sch√∂neberg', 'Siemensstadt_Spandau', 'Spandau', 'Staaken_Spandau', 'Steglitz', 'Tegel_Reinickendorf', 'Tempelhof', 'Tiergarten', 'Treptow', 'Waidmannslust_Reinickendorf', 'Wannsee_Zehlendorf', 'Wedding', 'Wei√üensee', 'Wilmersdorf', 'Wittenau_Reinickendorf', 'Zehlendorf']\n",
      "Normalisierte Bezirke: ['Adlershof', 'Alt', 'Altglienicke', 'Baumschulenweg', 'Biesdorf', 'Blankenburg', 'Bohnsdorf', 'Britz', 'Buch', 'Buckow', 'Charlottenburg', 'Dahlem', 'Falkenberg', 'Franz√∂sisch', 'Friedenau', 'Friedrichsfelde', 'Friedrichshagen', 'Friedrichshain', 'Frohnau', 'Gatow', 'Grunewald', 'Gr√ºnau', 'Haselhorst', 'Heiligensee', 'Heinersdorf', 'Hellersdorf', 'Hermsdorf', 'Johannisthal', 'Karlshorst', 'Karow', 'Kaulsdorf', 'Kladow', 'Konradsh√∂he', 'Kreuzberg', 'K√∂penick', 'Lankwitz', 'Lichtenberg', 'Lichtenrade', 'Lichterfelde', 'L√ºbars', 'Mahlsdorf', 'Malchow', 'Mariendorf', 'Marienfelde', 'Marzahn', 'Mitte', 'M√ºggelheim', 'Neu', 'Neuk√∂lln', 'Niedersch√∂neweide', 'Niedersch√∂nhausen', 'Nikolassee', 'Obersch√∂neweide', 'Pankow', 'Pl√§nterwald', 'Prenzlauer', 'Rahnsdorf', 'Reinickendorf', 'Rosenthal', 'Rudow', 'Rummelsburg', 'Schmargendorf', 'Schm√∂ckwitz', 'Sch√∂neberg', 'Siemensstadt', 'Spandau', 'Staaken', 'Steglitz', 'Tegel', 'Tempelhof', 'Tiergarten', 'Treptow', 'Waidmannslust', 'Wannsee', 'Wedding', 'Wei√üensee', 'Wilmersdorf', 'Wittenau', 'Zehlendorf']\n",
      "Anzahl normalisierte Bezirke: 79\n",
      "\n",
      "=== ZIMMER-BEREINIGUNG ===\n",
      "noRooms - Statistik:\n",
      "  Typ: float64\n",
      "  Nicht-null Werte: 10388\n",
      "  Einzigartige Werte: [np.float64(1.0), np.float64(1.1), np.float64(1.5), np.float64(2.0), np.float64(2.1), np.float64(2.2), np.float64(2.5), np.float64(3.0), np.float64(3.5), np.float64(4.0), np.float64(4.5), np.float64(5.0), np.float64(5.5), np.float64(6.0), np.float64(6.5), np.float64(7.0), np.float64(7.5), np.float64(8.0), np.float64(8.5), np.float64(9.0), np.float64(10.0), np.float64(11.0)]\n",
      "Entfernte unrealistische Zimmeranzahlen: 1\n",
      "Spezifische Bereinigung abgeschlossen\n",
      "Verbleibende Datens√§tze: 10387 (Verlust: 19)\n"
     ]
    }
   ],
   "source": [
    "# Spezifische Bereinigung f√ºr Dataset 2018-2019\n",
    "print(\"=\" * 60)\n",
    "print(\"SPEZIFISCHE BEREINIGUNG DATASET 2018-2019\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Erstelle Arbeitskopie\n",
    "df = df_raw.copy()\n",
    "print(f\"Arbeitskopie erstellt: {len(df)} Zeilen\")\n",
    "\n",
    "# 1. Preis-Bereinigung (baseRent)\n",
    "print(f\"\\n=== PREIS-BEREINIGUNG ===\")\n",
    "print(f\"baseRent - Statistik vor Bereinigung:\")\n",
    "print(f\"  Typ: {df['baseRent'].dtype}\")\n",
    "print(f\"  Nicht-null Werte: {df['baseRent'].notna().sum()}\")\n",
    "print(f\"  Min: {df['baseRent'].min()}, Max: {df['baseRent'].max()}\")\n",
    "\n",
    "# Preis ist bereits numerisch, nur Plausibilit√§tspr√ºfung\n",
    "# Entferne unrealistische Preise (< 100‚Ç¨ oder > 10.000‚Ç¨)\n",
    "original_count = len(df)\n",
    "df = df[(df['baseRent'] >= 100) & (df['baseRent'] <= 10000)]\n",
    "removed_price = original_count - len(df)\n",
    "print(f\"Entfernte unrealistische Preise: {removed_price}\")\n",
    "\n",
    "# 2. Gr√∂√üen-Bereinigung (livingSpace)\n",
    "print(f\"\\n=== GR√ñSSEN-BEREINIGUNG ===\")\n",
    "print(f\"livingSpace - Statistik vor Bereinigung:\")\n",
    "print(f\"  Typ: {df['livingSpace'].dtype}\")\n",
    "print(f\"  Nicht-null Werte: {df['livingSpace'].notna().sum()}\")\n",
    "print(f\"  Min: {df['livingSpace'].min()}, Max: {df['livingSpace'].max()}\")\n",
    "\n",
    "# Gr√∂√üe ist bereits numerisch, nur Plausibilit√§tspr√ºfung\n",
    "# Entferne unrealistische Gr√∂√üen (< 10m¬≤ oder > 500m¬≤)\n",
    "original_count = len(df)\n",
    "df = df[(df['livingSpace'] >= 10) & (df['livingSpace'] <= 500)]\n",
    "removed_size = original_count - len(df)\n",
    "print(f\"Entfernte unrealistische Gr√∂√üen: {removed_size}\")\n",
    "\n",
    "# 3. Bezirks-Normalisierung (regio3)\n",
    "print(f\"\\n=== BEZIRKS-NORMALISIERUNG ===\")\n",
    "print(f\"regio3 - Einzigartige Werte: {df['regio3'].nunique()}\")\n",
    "print(f\"Bezirke: {sorted(df['regio3'].unique())}\")\n",
    "\n",
    "# Bezirk-Normalisierung (entferne _Suffix)\n",
    "def normalize_district_2018_2019(district):\n",
    "    \"\"\"Normalisiert Bezirksnamen f√ºr Dataset 2018-2019\"\"\"\n",
    "    if pd.isna(district):\n",
    "        return None\n",
    "    \n",
    "    # Entferne Suffix nach Unterstrich\n",
    "    if '_' in str(district):\n",
    "        return str(district).split('_')[0]\n",
    "    \n",
    "    return str(district)\n",
    "\n",
    "df['district_normalized'] = df['regio3'].apply(normalize_district_2018_2019)\n",
    "\n",
    "print(f\"Normalisierte Bezirke: {sorted(df['district_normalized'].unique())}\")\n",
    "print(f\"Anzahl normalisierte Bezirke: {df['district_normalized'].nunique()}\")\n",
    "\n",
    "# 4. Zimmer-Bereinigung (noRooms)\n",
    "print(f\"\\n=== ZIMMER-BEREINIGUNG ===\")\n",
    "print(f\"noRooms - Statistik:\")\n",
    "print(f\"  Typ: {df['noRooms'].dtype}\")\n",
    "print(f\"  Nicht-null Werte: {df['noRooms'].notna().sum()}\")\n",
    "print(f\"  Einzigartige Werte: {sorted(df['noRooms'].dropna().unique())}\")\n",
    "\n",
    "# Zimmeranzahl ist bereits numerisch\n",
    "# Plausibilit√§tspr√ºfung (0.5 bis 10 Zimmer)\n",
    "original_count = len(df)\n",
    "df = df[(df['noRooms'] >= 0.5) & (df['noRooms'] <= 10)]\n",
    "removed_rooms = original_count - len(df)\n",
    "print(f\"Entfernte unrealistische Zimmeranzahlen: {removed_rooms}\")\n",
    "\n",
    "print(f\"Spezifische Bereinigung abgeschlossen\")\n",
    "print(f\"Verbleibende Datens√§tze: {len(df)} (Verlust: {len(df_raw) - len(df)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47813a15",
   "metadata": {},
   "source": [
    "## 4. Normalisierung in Standardformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "86dc1e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NORMALISIERUNG IN STANDARDFORMAT\n",
      "============================================================\n",
      "Normalisiertes Dataset erstellt: 10387 Zeilen\n",
      "Standardspalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source']\n",
      "Zus√§tzliche Spalten: ['street', 'floor', 'typeOfFlat', 'yearConstructed', 'totalRent']\n",
      "=== DATENQUALIT√ÑT NORMALISIERTES DATASET ===\n",
      "Zeilen mit Preis: 10387\n",
      "Zeilen mit Gr√∂√üe: 10387\n",
      "Zeilen mit Bezirk: 10387\n",
      "Zeilen mit Zimmeranzahl: 10387\n",
      "=== STATISTIKEN ===\n",
      "Preis - Min: 178.16‚Ç¨, Max: 9500.00‚Ç¨, Median: 945.00‚Ç¨\n",
      "Gr√∂√üe - Min: 10.0m¬≤, Max: 482.0m¬≤, Median: 72.0m¬≤\n",
      "Zimmer - Min: 1.0, Max: 10.0, Median: 2.0\n",
      "=== BEZIRKSVERTEILUNG ===\n",
      "Anzahl Bezirke: 79\n",
      "  Mitte: 799 Eintr√§ge\n",
      "  Tiergarten: 768 Eintr√§ge\n",
      "  Charlottenburg: 701 Eintr√§ge\n",
      "  Friedrichshain: 553 Eintr√§ge\n",
      "  Prenzlauer: 473 Eintr√§ge\n",
      "  Spandau: 415 Eintr√§ge\n",
      "  Wedding: 397 Eintr√§ge\n",
      "  Wilmersdorf: 370 Eintr√§ge\n",
      "  Neuk√∂lln: 361 Eintr√§ge\n",
      "  K√∂penick: 351 Eintr√§ge\n",
      "Normalisierung abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "# Normalisierung in Standardformat\n",
    "print(\"=\" * 60)\n",
    "print(\"NORMALISIERUNG IN STANDARDFORMAT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Erstelle normalisiertes Dataset mit Standardspalten\n",
    "df_normalized = pd.DataFrame()\n",
    "\n",
    "# Standardspalten zuweisen\n",
    "df_normalized['price'] = df['baseRent'].astype('float64')\n",
    "df_normalized['size'] = df['livingSpace'].astype('float64')\n",
    "df_normalized['district'] = df['district_normalized'].astype('string')\n",
    "df_normalized['rooms'] = df['noRooms'].astype('float64')\n",
    "df_normalized['year'] = 2019\n",
    "df_normalized['dataset_id'] = 'historical'\n",
    "df_normalized['source'] = 'Kaggle/Immobilienscout24'\n",
    "\n",
    "# Zus√§tzliche Spalten aus Original-Dataset beibehalten\n",
    "df_normalized['street'] = df['street']\n",
    "df_normalized['floor'] = df['floor']\n",
    "df_normalized['typeOfFlat'] = df['typeOfFlat']\n",
    "df_normalized['yearConstructed'] = df['yearConstructed']\n",
    "df_normalized['totalRent'] = df['totalRent']\n",
    "\n",
    "print(f\"Normalisiertes Dataset erstellt: {len(df_normalized)} Zeilen\")\n",
    "print(f\"Standardspalten: {['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source']}\")\n",
    "print(f\"Zus√§tzliche Spalten: {list(df_normalized.columns[7:])}\")\n",
    "\n",
    "# Datenqualit√§t pr√ºfen\n",
    "print(f\"=== DATENQUALIT√ÑT NORMALISIERTES DATASET ===\")\n",
    "print(f\"Zeilen mit Preis: {df_normalized['price'].notna().sum()}\")\n",
    "print(f\"Zeilen mit Gr√∂√üe: {df_normalized['size'].notna().sum()}\")\n",
    "print(f\"Zeilen mit Bezirk: {df_normalized['district'].notna().sum()}\")\n",
    "print(f\"Zeilen mit Zimmeranzahl: {df_normalized['rooms'].notna().sum()}\")\n",
    "\n",
    "# Statistiken\n",
    "print(f\"=== STATISTIKEN ===\")\n",
    "print(f\"Preis - Min: {df_normalized['price'].min():.2f}‚Ç¨, Max: {df_normalized['price'].max():.2f}‚Ç¨, Median: {df_normalized['price'].median():.2f}‚Ç¨\")\n",
    "print(f\"Gr√∂√üe - Min: {df_normalized['size'].min():.1f}m¬≤, Max: {df_normalized['size'].max():.1f}m¬≤, Median: {df_normalized['size'].median():.1f}m¬≤\")\n",
    "print(f\"Zimmer - Min: {df_normalized['rooms'].min():.1f}, Max: {df_normalized['rooms'].max():.1f}, Median: {df_normalized['rooms'].median():.1f}\")\n",
    "\n",
    "# Bezirksverteilung\n",
    "print(f\"=== BEZIRKSVERTEILUNG ===\")\n",
    "district_counts = df_normalized['district'].value_counts()\n",
    "print(f\"Anzahl Bezirke: {len(district_counts)}\")\n",
    "for district, count in district_counts.head(10).items():\n",
    "    print(f\"  {district}: {count} Eintr√§ge\")\n",
    "\n",
    "print(f\"Normalisierung abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db94d3",
   "metadata": {},
   "source": [
    "## 5. Export des normalisierten Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c728c108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PLZ-EXTRAKTION VOR EXPORT\n",
      "============================================================\n",
      "PLZ-Mapping geladen: 190 Eintr√§ge\n",
      "Ortsteil->PLZ Mappings: 79\n",
      "Bezirk->PLZ Mappings: 13\n",
      "PLZ-Spalte vorhanden vor Extraktion: True\n",
      "PLZ extrahiert f√ºr 0 Zeilen\n",
      "PLZ-Abdeckung: 10173/10387 (97.9%)\n",
      "Konvertiere PLZ zu sauberen Strings...\n",
      "PLZ-Abdeckung nach String-Konvertierung: 10173/10387 (97.9%)\n",
      "PLZ-Beispiele:\n",
      "['13591', '10409', '10967', '10785', '12527']\n",
      "============================================================\n",
      "EXPORT NORMALISIERTES DATASET\n",
      "============================================================\n",
      "‚úÖ Normalisiertes Dataset exportiert: data/processed/dataset_2018_2019_normalized.csv\n",
      "Dateigr√∂√üe: 10387 Zeilen x 13 Spalten\n",
      "‚úÖ Export-Validierung erfolgreich: 10387 Zeilen geladen\n",
      "PLZ-Spalte im Export: True\n",
      "PLZ-Datentyp im Export: float64\n",
      "PLZ-Abdeckung im Export: 10173/10387 (97.9%)\n",
      "PLZ-Beispiele im Export: [13591.0, 10409.0, 10967.0]\n",
      "============================================================\n",
      "ZUSAMMENFASSUNG DATASET 2018-2019\n",
      "============================================================\n",
      "Input: data/raw/Dataset_2018_2019.csv (10406 Zeilen)\n",
      "Output: data/processed/dataset_2018_2019_normalized.csv (10387 Zeilen)\n",
      "Datenverlust: 19 Zeilen (0.2%)\n",
      "Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\n",
      "Zus√§tzliche Spalten: 6\n",
      "üéØ DATASET 2018-2019 BEREINIGUNG ABGESCHLOSSEN!\n",
      "Bereit f√ºr Kombination mit anderen normalisierten Datasets.\n",
      "PLZ extrahiert f√ºr 0 Zeilen\n",
      "PLZ-Abdeckung: 10173/10387 (97.9%)\n",
      "Konvertiere PLZ zu sauberen Strings...\n",
      "PLZ-Abdeckung nach String-Konvertierung: 10173/10387 (97.9%)\n",
      "PLZ-Beispiele:\n",
      "['13591', '10409', '10967', '10785', '12527']\n",
      "============================================================\n",
      "EXPORT NORMALISIERTES DATASET\n",
      "============================================================\n",
      "‚úÖ Normalisiertes Dataset exportiert: data/processed/dataset_2018_2019_normalized.csv\n",
      "Dateigr√∂√üe: 10387 Zeilen x 13 Spalten\n",
      "‚úÖ Export-Validierung erfolgreich: 10387 Zeilen geladen\n",
      "PLZ-Spalte im Export: True\n",
      "PLZ-Datentyp im Export: float64\n",
      "PLZ-Abdeckung im Export: 10173/10387 (97.9%)\n",
      "PLZ-Beispiele im Export: [13591.0, 10409.0, 10967.0]\n",
      "============================================================\n",
      "ZUSAMMENFASSUNG DATASET 2018-2019\n",
      "============================================================\n",
      "Input: data/raw/Dataset_2018_2019.csv (10406 Zeilen)\n",
      "Output: data/processed/dataset_2018_2019_normalized.csv (10387 Zeilen)\n",
      "Datenverlust: 19 Zeilen (0.2%)\n",
      "Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\n",
      "Zus√§tzliche Spalten: 6\n",
      "üéØ DATASET 2018-2019 BEREINIGUNG ABGESCHLOSSEN!\n",
      "Bereit f√ºr Kombination mit anderen normalisierten Datasets.\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# PLZ-EXTRAKTION VOR EXPORT\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"PLZ-EXTRAKTION VOR EXPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Lade PLZ-Mapping\n",
    "plz_mapping = pd.read_csv('data/processed/berlin_plz_mapping_enhanced.csv')\n",
    "print(f\"PLZ-Mapping geladen: {len(plz_mapping)} Eintr√§ge\")\n",
    "\n",
    "# Erstelle Ortsteil->PLZ und Bezirk->PLZ Mappings\n",
    "ortsteil_plz_mapping = dict(zip(plz_mapping['Ortsteil'], plz_mapping['PLZ']))\n",
    "bezirk_plz_mapping = plz_mapping.groupby('Bezirk')['PLZ'].first().to_dict()\n",
    "\n",
    "print(f\"Ortsteil->PLZ Mappings: {len(ortsteil_plz_mapping)}\")\n",
    "print(f\"Bezirk->PLZ Mappings: {len(bezirk_plz_mapping)}\")\n",
    "\n",
    "# Funktion f√ºr PLZ-Extraktion\n",
    "def extract_plz_from_district(district):\n",
    "    \"\"\"Extrahiere PLZ aus Bezirk/Ortsteil\"\"\"\n",
    "    if pd.isna(district):\n",
    "        return None\n",
    "    \n",
    "    district_clean = str(district).strip()\n",
    "    \n",
    "    # Strategie 1: Direkte Ortsteil-Suche\n",
    "    if district_clean in ortsteil_plz_mapping:\n",
    "        return ortsteil_plz_mapping[district_clean]\n",
    "    \n",
    "    # Strategie 2: Bezirk-Suche\n",
    "    if district_clean in bezirk_plz_mapping:\n",
    "        return bezirk_plz_mapping[district_clean]\n",
    "    \n",
    "    # Strategie 3: Fuzzy Match f√ºr h√§ufige Varianten\n",
    "    for ortsteil, plz in ortsteil_plz_mapping.items():\n",
    "        if district_clean.lower() in ortsteil.lower() or ortsteil.lower() in district_clean.lower():\n",
    "            return plz\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Funktion f√ºr saubere PLZ-String-Konvertierung\n",
    "def convert_plz_to_clean_string(plz_value):\n",
    "    \"\"\"Konvertiere PLZ zu sauberem String ohne .0\"\"\"\n",
    "    if pd.isna(plz_value):\n",
    "        return None\n",
    "    try:\n",
    "        # Konvertiere zu int um .0 zu entfernen, dann zu string\n",
    "        return str(int(float(plz_value)))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Pr√ºfe PLZ-Verf√ºgbarkeit vor Extraktion\n",
    "print(f\"PLZ-Spalte vorhanden vor Extraktion: {'plz' in df_normalized.columns}\")\n",
    "\n",
    "# F√ºge PLZ-Spalte hinzu, falls nicht vorhanden\n",
    "if 'plz' not in df_normalized.columns:\n",
    "    df_normalized['plz'] = np.nan\n",
    "\n",
    "# Extrahiere PLZ f√ºr alle Zeilen\n",
    "plz_extracted = 0\n",
    "for idx, row in df_normalized.iterrows():\n",
    "    if pd.isna(row['plz']):  # Nur wenn PLZ noch nicht vorhanden\n",
    "        plz = extract_plz_from_district(row['district'])\n",
    "        if plz:\n",
    "            df_normalized.at[idx, 'plz'] = plz\n",
    "            plz_extracted += 1\n",
    "\n",
    "print(f\"PLZ extrahiert f√ºr {plz_extracted} Zeilen\")\n",
    "\n",
    "# Pr√ºfe PLZ-Verf√ºgbarkeit nach Extraktion\n",
    "plz_final = df_normalized['plz'].notna().sum()\n",
    "print(f\"PLZ-Abdeckung: {plz_final}/{len(df_normalized)} ({plz_final/len(df_normalized)*100:.1f}%)\")\n",
    "\n",
    "# WICHTIG: Konvertiere PLZ zu sauberen Strings\n",
    "print(\"Konvertiere PLZ zu sauberen Strings...\")\n",
    "df_normalized['plz'] = df_normalized['plz'].apply(convert_plz_to_clean_string)\n",
    "plz_final_clean = df_normalized['plz'].notna().sum()\n",
    "print(f\"PLZ-Abdeckung nach String-Konvertierung: {plz_final_clean}/{len(df_normalized)} ({plz_final_clean/len(df_normalized)*100:.1f}%)\")\n",
    "\n",
    "# Zeige Beispiele\n",
    "print(\"PLZ-Beispiele:\")\n",
    "print(df_normalized['plz'].dropna().head(5).tolist())\n",
    "\n",
    "# ============================================================\n",
    "# EXPORT NORMALISIERTES DATASET\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPORT NORMALISIERTES DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ausgabedatei\n",
    "output_file = 'data/processed/dataset_2018_2019_normalized.csv'\n",
    "\n",
    "# Exportiere das normalisierte Dataset\n",
    "df_normalized.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Normalisiertes Dataset exportiert: {output_file}\")\n",
    "print(f\"Dateigr√∂√üe: {len(df_normalized)} Zeilen x {len(df_normalized.columns)} Spalten\")\n",
    "\n",
    "# Validierung des Exports\n",
    "test_load = pd.read_csv(output_file)\n",
    "print(f\"‚úÖ Export-Validierung erfolgreich: {len(test_load)} Zeilen geladen\")\n",
    "print(f\"PLZ-Spalte im Export: {'plz' in test_load.columns}\")\n",
    "if 'plz' in test_load.columns:\n",
    "    print(f\"PLZ-Datentyp im Export: {test_load['plz'].dtype}\")\n",
    "    plz_export = test_load['plz'].notna().sum()\n",
    "    print(f\"PLZ-Abdeckung im Export: {plz_export}/{len(test_load)} ({plz_export/len(test_load)*100:.1f}%)\")\n",
    "    print(f\"PLZ-Beispiele im Export: {test_load['plz'].dropna().head(3).tolist()}\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ZUSAMMENFASSUNG DATASET 2018-2019\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Input: data/raw/Dataset_2018_2019.csv ({len(df_raw)} Zeilen)\")\n",
    "print(f\"Output: {output_file} ({len(df_normalized)} Zeilen)\")\n",
    "print(f\"Datenverlust: {len(df_raw) - len(df_normalized)} Zeilen ({(len(df_raw) - len(df_normalized))/len(df_raw)*100:.1f}%)\")\n",
    "print(f\"Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\")\n",
    "print(f\"Zus√§tzliche Spalten: {len(df_normalized.columns) - 7}\")\n",
    "print(f\"üéØ DATASET 2018-2019 BEREINIGUNG ABGESCHLOSSEN!\")\n",
    "print(f\"Bereit f√ºr Kombination mit anderen normalisierten Datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "## 6. Lade angereicherte Wohnlagendaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e5f6g7h8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANGEREICHERTE WOHNLAGENDATEN LADEN\n",
      "============================================================\n",
      "‚úÖ Angereicherte Daten geladen: 551,249 Zeilen, 11 Spalten\n",
      "‚úÖ Angereicherte Daten geladen: 551,249 Zeilen, 11 Spalten\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ANGEREICHERTE WOHNLAGENDATEN LADEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "enriched_data_path = 'data/raw/wohnlagen_enriched.csv'\n",
    "try:\n",
    "    enriched_df = pd.read_csv(enriched_data_path)\n",
    "    print(f\"‚úÖ Angereicherte Daten geladen: {len(enriched_df):,} Zeilen, {len(enriched_df.columns)} Spalten\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Datei nicht gefunden: {enriched_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "## 7. Kombiniere Datasets mit Wohnlagendaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "m3n4o5p6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KOMBINIERE MIT WOHNLAGENDATEN - BEWAHRE BESTEHENDE PLZ-DATEN\n",
      "============================================================\n",
      "üîç DEBUG: AUSGANGSSITUATION\n",
      "df_normalized Zeilen: 10387\n",
      "df_normalized Spalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'street', 'floor', 'typeOfFlat', 'yearConstructed', 'totalRent', 'plz']\n",
      "df_normalized PLZ-Abdeckung: 10173/10387 (97.9%)\n",
      "df_normalized PLZ-Typ: object\n",
      "df_normalized PLZ-Beispiele: ['13591', '10179', '10999']\n",
      "\n",
      "Original enriched_df: 551,249 Zeilen\n",
      "\n",
      "üîç DEBUG: KRITISCHER PUNKT - df_enriched erstellen\n",
      "BEFORE copy(): df_normalized PLZ-Abdeckung = 10173\n",
      "AFTER copy():  df_enriched PLZ-Abdeckung = 10173\n",
      "Sind die DataFrames identisch? True\n",
      "AFTER neue Spalten: df_enriched PLZ-Abdeckung = 10173\n",
      "\n",
      "üîç DEBUG: Erstelle Mappings\n",
      "\n",
      "Original enriched_df: 551,249 Zeilen\n",
      "\n",
      "üîç DEBUG: KRITISCHER PUNKT - df_enriched erstellen\n",
      "BEFORE copy(): df_normalized PLZ-Abdeckung = 10173\n",
      "AFTER copy():  df_enriched PLZ-Abdeckung = 10173\n",
      "Sind die DataFrames identisch? True\n",
      "AFTER neue Spalten: df_enriched PLZ-Abdeckung = 10173\n",
      "\n",
      "üîç DEBUG: Erstelle Mappings\n",
      "Street-Mappings: 9,479\n",
      "Ortsteil-Mappings: 92\n",
      "\n",
      "üîç DEBUG: VOR ANREICHERUNG\n",
      "df_enriched PLZ-Abdeckung: 10173/10387 (97.9%)\n",
      "\n",
      "üîç DEBUG: Street-basierte Anreicherung\n",
      "Street-Mappings: 9,479\n",
      "Ortsteil-Mappings: 92\n",
      "\n",
      "üîç DEBUG: VOR ANREICHERUNG\n",
      "df_enriched PLZ-Abdeckung: 10173/10387 (97.9%)\n",
      "\n",
      "üîç DEBUG: Street-basierte Anreicherung\n",
      "Street-basierte Anreicherung: 1,760 Zeilen\n",
      "PLZ hinzugef√ºgt (Street): 0\n",
      "PLZ-√úberschreibungen verhindert (Street): 1,760\n",
      "NACH Street-Anreicherung: PLZ-Abdeckung = 10173\n",
      "\n",
      "üîç DEBUG: Ortsteil-basierte Anreicherung\n",
      "Street-basierte Anreicherung: 1,760 Zeilen\n",
      "PLZ hinzugef√ºgt (Street): 0\n",
      "PLZ-√úberschreibungen verhindert (Street): 1,760\n",
      "NACH Street-Anreicherung: PLZ-Abdeckung = 10173\n",
      "\n",
      "üîç DEBUG: Ortsteil-basierte Anreicherung\n",
      "Ortsteil-basierte Anreicherung: 7,745 Zeilen\n",
      "PLZ hinzugef√ºgt (Ortsteil): 0\n",
      "NACH Ortsteil-Anreicherung: PLZ-Abdeckung = 10173\n",
      "\n",
      "üîç DEBUG: Finale PLZ-String-Konvertierung\n",
      "PLZ vor String-Konvertierung: 10173\n",
      "PLZ nach String-Konvertierung: 10173\n",
      "‚úÖ Anreicherung abgeschlossen: 10,387 Zeilen\n",
      "Erfolgreiche Anreicherung: 9,505 von 10,387 Zeilen (91.5%)\n",
      "\n",
      "üéØ FINALE VALIDIERUNG\n",
      "PLZ im finalen Dataset: 10,173/10,387 (97.9%)\n",
      "PLZ-Datentyp: object\n",
      "‚úÖ PLZ-Daten erfolgreich bewahrt oder verbessert!\n",
      "\n",
      "Top 5 PLZ im finalen Dataset:\n",
      "  10179: 762 Eintr√§ge\n",
      "  10787: 703 Eintr√§ge\n",
      "  14059: 564 Eintr√§ge\n",
      "  10439: 450 Eintr√§ge\n",
      "  10249: 425 Eintr√§ge\n",
      "PLZ-Beispiele: ['13591', '10179', '10999', '10787', '12527']\n",
      "Ortsteil-basierte Anreicherung: 7,745 Zeilen\n",
      "PLZ hinzugef√ºgt (Ortsteil): 0\n",
      "NACH Ortsteil-Anreicherung: PLZ-Abdeckung = 10173\n",
      "\n",
      "üîç DEBUG: Finale PLZ-String-Konvertierung\n",
      "PLZ vor String-Konvertierung: 10173\n",
      "PLZ nach String-Konvertierung: 10173\n",
      "‚úÖ Anreicherung abgeschlossen: 10,387 Zeilen\n",
      "Erfolgreiche Anreicherung: 9,505 von 10,387 Zeilen (91.5%)\n",
      "\n",
      "üéØ FINALE VALIDIERUNG\n",
      "PLZ im finalen Dataset: 10,173/10,387 (97.9%)\n",
      "PLZ-Datentyp: object\n",
      "‚úÖ PLZ-Daten erfolgreich bewahrt oder verbessert!\n",
      "\n",
      "Top 5 PLZ im finalen Dataset:\n",
      "  10179: 762 Eintr√§ge\n",
      "  10787: 703 Eintr√§ge\n",
      "  14059: 564 Eintr√§ge\n",
      "  10439: 450 Eintr√§ge\n",
      "  10249: 425 Eintr√§ge\n",
      "PLZ-Beispiele: ['13591', '10179', '10999', '10787', '12527']\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"KOMBINIERE MIT WOHNLAGENDATEN - BEWAHRE BESTEHENDE PLZ-DATEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# DEBUG: Ausgangssituation\n",
    "print(\"üîç DEBUG: AUSGANGSSITUATION\")\n",
    "print(f\"df_normalized Zeilen: {len(df_normalized)}\")\n",
    "print(f\"df_normalized Spalten: {list(df_normalized.columns)}\")\n",
    "if 'plz' in df_normalized.columns:\n",
    "    plz_start = df_normalized['plz'].notna().sum()\n",
    "    print(f\"df_normalized PLZ-Abdeckung: {plz_start}/{len(df_normalized)} ({plz_start/len(df_normalized)*100:.1f}%)\")\n",
    "    print(f\"df_normalized PLZ-Typ: {df_normalized['plz'].dtype}\")\n",
    "    print(f\"df_normalized PLZ-Beispiele: {df_normalized['plz'].dropna().head(3).tolist()}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è WARNUNG: df_normalized hat KEINE PLZ-Spalte!\")\n",
    "    plz_start = 0\n",
    "\n",
    "# Lade Wohnlagendaten\n",
    "enriched_data_path = 'data/raw/wohnlagen_enriched.csv'\n",
    "enriched_df = pd.read_csv(enriched_data_path)\n",
    "\n",
    "print(f\"\\nOriginal enriched_df: {len(enriched_df):,} Zeilen\")\n",
    "\n",
    "# Funktion f√ºr saubere PLZ-String-Konvertierung\n",
    "def convert_plz_to_clean_string(plz_value):\n",
    "    \"\"\"Konvertiere PLZ zu sauberem String ohne .0\"\"\"\n",
    "    if pd.isna(plz_value):\n",
    "        return None\n",
    "    try:\n",
    "        # Konvertiere zu int um .0 zu entfernen, dann zu string\n",
    "        return str(int(float(plz_value)))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# DEBUG: KRITISCHER PUNKT - df_enriched erstellen\n",
    "print(\"\\nüîç DEBUG: KRITISCHER PUNKT - df_enriched erstellen\")\n",
    "print(f\"BEFORE copy(): df_normalized PLZ-Abdeckung = {df_normalized['plz'].notna().sum()}\")\n",
    "\n",
    "# WICHTIG: Starte mit dem vollst√§ndigen normalized Dataset\n",
    "df_enriched = df_normalized.copy()\n",
    "\n",
    "print(f\"AFTER copy():  df_enriched PLZ-Abdeckung = {df_enriched['plz'].notna().sum()}\")\n",
    "print(f\"Sind die DataFrames identisch? {df_enriched.equals(df_normalized)}\")\n",
    "\n",
    "# F√ºge zus√§tzliche Spalten hinzu (initialisiert mit NaN)\n",
    "df_enriched['wol'] = np.nan\n",
    "df_enriched['ortsteil_neu'] = np.nan\n",
    "\n",
    "print(f\"AFTER neue Spalten: df_enriched PLZ-Abdeckung = {df_enriched['plz'].notna().sum()}\")\n",
    "\n",
    "# Erstelle Mapping-Dictionaries f√ºr Street->Informationen\n",
    "print(\"\\nüîç DEBUG: Erstelle Mappings\")\n",
    "street_info_mapping = {}\n",
    "for _, row in enriched_df.iterrows():\n",
    "    street = row['strasse']\n",
    "    if street not in street_info_mapping:\n",
    "        street_info_mapping[street] = {\n",
    "            'plz': convert_plz_to_clean_string(row['plz']),\n",
    "            'wol': row['wol'],\n",
    "            'ortsteil_neu': row['ortsteil_neu']\n",
    "        }\n",
    "\n",
    "# Erstelle Mapping-Dictionaries f√ºr Ortsteil->Informationen\n",
    "ortsteil_info_mapping = {}\n",
    "for _, row in enriched_df.iterrows():\n",
    "    ortsteil = row['ortsteil_neu']\n",
    "    if ortsteil not in ortsteil_info_mapping:\n",
    "        ortsteil_info_mapping[ortsteil] = {\n",
    "            'plz': convert_plz_to_clean_string(row['plz']),\n",
    "            'wol': row['wol']\n",
    "        }\n",
    "\n",
    "print(f\"Street-Mappings: {len(street_info_mapping):,}\")\n",
    "print(f\"Ortsteil-Mappings: {len(ortsteil_info_mapping):,}\")\n",
    "\n",
    "# DEBUG: Vor Anreicherung\n",
    "print(f\"\\nüîç DEBUG: VOR ANREICHERUNG\")\n",
    "print(f\"df_enriched PLZ-Abdeckung: {df_enriched['plz'].notna().sum()}/{len(df_enriched)} ({df_enriched['plz'].notna().sum()/len(df_enriched)*100:.1f}%)\")\n",
    "\n",
    "# Anreicherung basierend auf Street-Match\n",
    "print(\"\\nüîç DEBUG: Street-basierte Anreicherung\")\n",
    "street_matches = 0\n",
    "plz_added_street = 0\n",
    "plz_overwrites_prevented = 0\n",
    "\n",
    "for idx, row in df_enriched.iterrows():\n",
    "    if pd.notna(row['street']) and row['street'] in street_info_mapping:\n",
    "        street_info = street_info_mapping[row['street']]\n",
    "        \n",
    "        # DEBUG: PLZ-Logik\n",
    "        if pd.notna(row['plz']):\n",
    "            plz_overwrites_prevented += 1\n",
    "        elif street_info['plz'] is not None:\n",
    "            df_enriched.at[idx, 'plz'] = street_info['plz']\n",
    "            plz_added_street += 1\n",
    "        \n",
    "        # F√ºge Wohnlage hinzu\n",
    "        if pd.notna(street_info['wol']):\n",
    "            df_enriched.at[idx, 'wol'] = street_info['wol']\n",
    "        \n",
    "        # F√ºge Ortsteil hinzu\n",
    "        if pd.notna(street_info['ortsteil_neu']):\n",
    "            df_enriched.at[idx, 'ortsteil_neu'] = street_info['ortsteil_neu']\n",
    "        \n",
    "        street_matches += 1\n",
    "\n",
    "print(f\"Street-basierte Anreicherung: {street_matches:,} Zeilen\")\n",
    "print(f\"PLZ hinzugef√ºgt (Street): {plz_added_street:,}\")\n",
    "print(f\"PLZ-√úberschreibungen verhindert (Street): {plz_overwrites_prevented:,}\")\n",
    "\n",
    "# DEBUG: Nach Street-Anreicherung\n",
    "print(f\"NACH Street-Anreicherung: PLZ-Abdeckung = {df_enriched['plz'].notna().sum()}\")\n",
    "\n",
    "# Anreicherung basierend auf Ortsteil-Match\n",
    "print(\"\\nüîç DEBUG: Ortsteil-basierte Anreicherung\")\n",
    "ortsteil_matches = 0\n",
    "plz_added_ortsteil = 0\n",
    "\n",
    "for idx, row in df_enriched.iterrows():\n",
    "    if pd.isna(row['ortsteil_neu']) and pd.notna(row['district']):\n",
    "        if row['district'] in ortsteil_info_mapping:\n",
    "            ortsteil_info = ortsteil_info_mapping[row['district']]\n",
    "            \n",
    "            # DEBUG: PLZ-Logik\n",
    "            if pd.isna(row['plz']) and ortsteil_info['plz'] is not None:\n",
    "                df_enriched.at[idx, 'plz'] = ortsteil_info['plz']\n",
    "                plz_added_ortsteil += 1\n",
    "            \n",
    "            # F√ºge Wohnlage hinzu\n",
    "            if pd.notna(ortsteil_info['wol']):\n",
    "                df_enriched.at[idx, 'wol'] = ortsteil_info['wol']\n",
    "            \n",
    "            # F√ºge Ortsteil hinzu\n",
    "            df_enriched.at[idx, 'ortsteil_neu'] = row['district']\n",
    "            \n",
    "            ortsteil_matches += 1\n",
    "\n",
    "print(f\"Ortsteil-basierte Anreicherung: {ortsteil_matches:,} Zeilen\")\n",
    "print(f\"PLZ hinzugef√ºgt (Ortsteil): {plz_added_ortsteil:,}\")\n",
    "\n",
    "# DEBUG: Nach Ortsteil-Anreicherung\n",
    "print(f\"NACH Ortsteil-Anreicherung: PLZ-Abdeckung = {df_enriched['plz'].notna().sum()}\")\n",
    "\n",
    "# WICHTIG: Finale PLZ-String-Konvertierung f√ºr Konsistenz\n",
    "print(\"\\nüîç DEBUG: Finale PLZ-String-Konvertierung\")\n",
    "plz_before_conversion = df_enriched['plz'].notna().sum()\n",
    "df_enriched['plz'] = df_enriched['plz'].apply(convert_plz_to_clean_string)\n",
    "plz_after_conversion = df_enriched['plz'].notna().sum()\n",
    "\n",
    "print(f\"PLZ vor String-Konvertierung: {plz_before_conversion}\")\n",
    "print(f\"PLZ nach String-Konvertierung: {plz_after_conversion}\")\n",
    "if plz_after_conversion < plz_before_conversion:\n",
    "    print(f\"‚ö†Ô∏è WARNUNG: String-Konvertierung hat {plz_before_conversion - plz_after_conversion} PLZ-Werte verloren!\")\n",
    "\n",
    "print(f\"‚úÖ Anreicherung abgeschlossen: {len(df_enriched):,} Zeilen\")\n",
    "\n",
    "# Erfolgreiche Anreicherung = Zeilen mit zus√§tzlichen Informationen\n",
    "successful_enrichment = df_enriched[df_enriched['wol'].notna() | df_enriched['ortsteil_neu'].notna()].shape[0]\n",
    "print(f\"Erfolgreiche Anreicherung: {successful_enrichment:,} von {len(df_enriched):,} Zeilen ({successful_enrichment/len(df_enriched)*100:.1f}%)\")\n",
    "\n",
    "# FINALE VALIDIERUNG\n",
    "print(f\"\\nüéØ FINALE VALIDIERUNG\")\n",
    "plz_final = df_enriched['plz'].notna().sum()\n",
    "print(f\"PLZ im finalen Dataset: {plz_final:,}/{len(df_enriched):,} ({plz_final/len(df_enriched)*100:.1f}%)\")\n",
    "print(f\"PLZ-Datentyp: {df_enriched['plz'].dtype}\")\n",
    "\n",
    "# KRITISCHE PR√úFUNG\n",
    "if plz_final < plz_start:\n",
    "    print(f\"üö® KRITISCHER FEHLER: PLZ-DATENVERLUST!\")\n",
    "    print(f\"   Start: {plz_start:,} PLZ-Werte\")\n",
    "    print(f\"   Final: {plz_final:,} PLZ-Werte\") \n",
    "    print(f\"   Verlust: {plz_start - plz_final:,} PLZ-Werte ({(plz_start - plz_final)/plz_start*100:.1f}%)\")\n",
    "else:\n",
    "    print(f\"‚úÖ PLZ-Daten erfolgreich bewahrt oder verbessert!\")\n",
    "    if plz_final > plz_start:\n",
    "        print(f\"   Verbesserung: +{plz_final - plz_start:,} PLZ-Werte\")\n",
    "\n",
    "# Zeige h√§ufigste PLZ\n",
    "if plz_final > 0:\n",
    "    print(f\"\\nTop 5 PLZ im finalen Dataset:\")\n",
    "    plz_counts_new = df_enriched['plz'].value_counts().head()\n",
    "    for plz, count in plz_counts_new.items():\n",
    "        print(f\"  {plz}: {count} Eintr√§ge\")\n",
    "    \n",
    "    print(f\"PLZ-Beispiele: {df_enriched['plz'].dropna().head(5).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1498f",
   "metadata": {},
   "source": [
    "## 7.5. PLZ-Extraktion f√ºr 2018-2019 Dataset\n",
    "\n",
    "**üéØ Problem:** Das 2018-2019 Dataset enth√§lt keine PLZ-Spalte, nur `regio3` (Ortsteil) und `street`.\n",
    "\n",
    "**üîß L√∂sung:** Wir extrahieren die PLZ durch Matching mit der `wohnlagen_enriched.csv`:\n",
    "1. **Strategie 1:** Ortsteil-basiert (regio3 ‚Üí PLZ)\n",
    "2. **Strategie 2:** Street-basiert (street ‚Üí PLZ) \n",
    "3. **Fallback:** Bezirk-basiert\n",
    "\n",
    "Dies ist essentiell f√ºr die sp√§tere PLZ-Enhancement-Pipeline und r√§umliche Genauigkeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6d233e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PLZ-EXTRAKTION F√úR 2018-2019 DATASET\n",
      "============================================================\n",
      "df_normalized Spalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'street', 'floor', 'typeOfFlat', 'yearConstructed', 'totalRent', 'plz']\n",
      "df_raw Spalten: ['regio3', 'street', 'livingSpace', 'baseRent', 'totalRent', 'noRooms', 'floor', 'typeOfFlat', 'yearConstructed']\n",
      "‚ùå L√§ngen-Mismatch: df_normalized=10387, df_raw=10406\n",
      "\n",
      "=== STRATEGIE 1: ORTSTEIL-BASIERTE PLZ-EXTRAKTION ===\n",
      "Ortsteil-PLZ-Mapping erstellt: 91 Eintr√§ge\n",
      "Beispiele:\n",
      "  Halensee ‚Üí 10713\n",
      "  Hakenfelde ‚Üí 13587\n",
      "  Lichterfelde ‚Üí 12209\n",
      "  Charlottenburg ‚Üí 14055\n",
      "  Marienfelde ‚Üí 12307\n",
      "\n",
      "=== STRATEGIE 2: STREET-BASIERTE PLZ-EXTRAKTION ===\n",
      "Ortsteil-PLZ-Mapping erstellt: 91 Eintr√§ge\n",
      "Beispiele:\n",
      "  Halensee ‚Üí 10713\n",
      "  Hakenfelde ‚Üí 13587\n",
      "  Lichterfelde ‚Üí 12209\n",
      "  Charlottenburg ‚Üí 14055\n",
      "  Marienfelde ‚Üí 12307\n",
      "\n",
      "=== STRATEGIE 2: STREET-BASIERTE PLZ-EXTRAKTION ===\n",
      "Street-PLZ-Mapping erstellt: 9479 Eintr√§ge\n",
      "Beispiele:\n",
      "  Aachener Stra√üe ‚Üí 10713\n",
      "  Aalemannufer ‚Üí 13587\n",
      "  Aarauer Stra√üe ‚Üí 12205\n",
      "  Aarberger Stra√üe ‚Üí 12205\n",
      "  Abbestra√üe ‚Üí 10587\n",
      "\n",
      "=== REGIO3-NORMALISIERUNG ===\n",
      "‚ùå regio3 Spalte nicht verf√ºgbar\n",
      "\n",
      "=== PLZ-EXTRAKTION ANWENDEN ===\n",
      "Street-PLZ-Mapping erstellt: 9479 Eintr√§ge\n",
      "Beispiele:\n",
      "  Aachener Stra√üe ‚Üí 10713\n",
      "  Aalemannufer ‚Üí 13587\n",
      "  Aarauer Stra√üe ‚Üí 12205\n",
      "  Aarberger Stra√üe ‚Üí 12205\n",
      "  Abbestra√üe ‚Üí 10587\n",
      "\n",
      "=== REGIO3-NORMALISIERUNG ===\n",
      "‚ùå regio3 Spalte nicht verf√ºgbar\n",
      "\n",
      "=== PLZ-EXTRAKTION ANWENDEN ===\n",
      "PLZ-Extraktion Ergebnisse:\n",
      "  Ortsteil-basiert: 0 Matches\n",
      "  Street-basiert: 1,760 Matches\n",
      "  Total mit PLZ: 1,760/10,387 (16.9%)\n",
      "\n",
      "Top 10 PLZ im 2018-2019 Dataset:\n",
      "  10245: 63 Eintr√§ge\n",
      "  13158: 54 Eintr√§ge\n",
      "  10243: 51 Eintr√§ge\n",
      "  13125: 44 Eintr√§ge\n",
      "  13593: 41 Eintr√§ge\n",
      "  12555: 41 Eintr√§ge\n",
      "  12277: 37 Eintr√§ge\n",
      "  10315: 36 Eintr√§ge\n",
      "  12683: 35 Eintr√§ge\n",
      "  13629: 34 Eintr√§ge\n",
      "\n",
      "‚úÖ PLZ-Extraktion f√ºr 2018-2019 Dataset abgeschlossen!\n",
      "PLZ-Extraktion Ergebnisse:\n",
      "  Ortsteil-basiert: 0 Matches\n",
      "  Street-basiert: 1,760 Matches\n",
      "  Total mit PLZ: 1,760/10,387 (16.9%)\n",
      "\n",
      "Top 10 PLZ im 2018-2019 Dataset:\n",
      "  10245: 63 Eintr√§ge\n",
      "  13158: 54 Eintr√§ge\n",
      "  10243: 51 Eintr√§ge\n",
      "  13125: 44 Eintr√§ge\n",
      "  13593: 41 Eintr√§ge\n",
      "  12555: 41 Eintr√§ge\n",
      "  12277: 37 Eintr√§ge\n",
      "  10315: 36 Eintr√§ge\n",
      "  12683: 35 Eintr√§ge\n",
      "  13629: 34 Eintr√§ge\n",
      "\n",
      "‚úÖ PLZ-Extraktion f√ºr 2018-2019 Dataset abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PLZ-EXTRAKTION F√úR 2018-2019 DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Pr√ºfe verf√ºgbare Spalten\n",
    "print(f\"df_normalized Spalten: {list(df_normalized.columns)}\")\n",
    "print(f\"df_raw Spalten: {list(df_raw.columns)}\")\n",
    "\n",
    "# Da regio3 nicht in df_normalized ist, hole es aus df_raw\n",
    "# Erstelle einen tempor√§ren DataFrame f√ºr die PLZ-Extraktion\n",
    "temp_df = df_normalized.copy()\n",
    "\n",
    "# F√ºge regio3 aus df_raw hinzu (basierend auf Index)\n",
    "if len(temp_df) == len(df_raw):\n",
    "    temp_df['regio3'] = df_raw['regio3'].values\n",
    "    print(f\"‚úÖ regio3 Spalte hinzugef√ºgt: {temp_df['regio3'].nunique()} einzigartige Werte\")\n",
    "else:\n",
    "    print(f\"‚ùå L√§ngen-Mismatch: df_normalized={len(temp_df)}, df_raw={len(df_raw)}\")\n",
    "\n",
    "# Strategie 1: Ortsteil-basierte PLZ-Extraktion\n",
    "print(\"\\n=== STRATEGIE 1: ORTSTEIL-BASIERTE PLZ-EXTRAKTION ===\")\n",
    "\n",
    "# Erstelle Ortsteil-zu-PLZ-Mapping aus wohnlagen_enriched.csv\n",
    "ortsteil_plz_mapping = {}\n",
    "if 'enriched_df' in locals():\n",
    "    # Extrahiere einzigartige Ortsteil-PLZ-Kombinationen\n",
    "    ortsteil_plz_pairs = enriched_df[['ortsteil_neu', 'plz']].dropna().drop_duplicates()\n",
    "    \n",
    "    # F√ºr Ortsteile mit mehreren PLZ, nehme die h√§ufigste\n",
    "    for ortsteil in ortsteil_plz_pairs['ortsteil_neu'].unique():\n",
    "        plz_counts = enriched_df[enriched_df['ortsteil_neu'] == ortsteil]['plz'].value_counts()\n",
    "        if len(plz_counts) > 0:\n",
    "            most_common_plz = plz_counts.index[0]\n",
    "            ortsteil_plz_mapping[ortsteil] = most_common_plz\n",
    "    \n",
    "    print(f\"Ortsteil-PLZ-Mapping erstellt: {len(ortsteil_plz_mapping)} Eintr√§ge\")\n",
    "    \n",
    "    # Zeige einige Beispiele\n",
    "    print(\"Beispiele:\")\n",
    "    for ortsteil, plz in list(ortsteil_plz_mapping.items())[:5]:\n",
    "        print(f\"  {ortsteil} ‚Üí {plz}\")\n",
    "\n",
    "# Strategie 2: Street-basierte PLZ-Extraktion\n",
    "print(\"\\n=== STRATEGIE 2: STREET-BASIERTE PLZ-EXTRAKTION ===\")\n",
    "\n",
    "# Erstelle Street-zu-PLZ-Mapping\n",
    "street_plz_mapping = {}\n",
    "if 'enriched_df' in locals():\n",
    "    # Extrahiere einzigartige Street-PLZ-Kombinationen\n",
    "    street_plz_pairs = enriched_df[['strasse', 'plz']].dropna().drop_duplicates()\n",
    "    \n",
    "    # F√ºr Stra√üen mit mehreren PLZ, nehme die h√§ufigste\n",
    "    for street in street_plz_pairs['strasse'].unique():\n",
    "        plz_counts = enriched_df[enriched_df['strasse'] == street]['plz'].value_counts()\n",
    "        if len(plz_counts) > 0:\n",
    "            most_common_plz = plz_counts.index[0]\n",
    "            street_plz_mapping[street] = most_common_plz\n",
    "    \n",
    "    print(f\"Street-PLZ-Mapping erstellt: {len(street_plz_mapping)} Eintr√§ge\")\n",
    "    \n",
    "    # Zeige einige Beispiele\n",
    "    print(\"Beispiele:\")\n",
    "    for street, plz in list(street_plz_mapping.items())[:5]:\n",
    "        print(f\"  {street} ‚Üí {plz}\")\n",
    "\n",
    "# Normalisiere regio3 f√ºr besseres Matching\n",
    "print(\"\\n=== REGIO3-NORMALISIERUNG ===\")\n",
    "\n",
    "def normalize_regio3(regio3_value):\n",
    "    \"\"\"Normalisiert regio3 Werte f√ºr besseres Ortsteil-Matching.\"\"\"\n",
    "    if pd.isna(regio3_value):\n",
    "        return None\n",
    "    \n",
    "    # Konvertiere zu String und bereinige\n",
    "    normalized = str(regio3_value).strip()\n",
    "    \n",
    "    # Entferne Bezirk-Suffix (z.B. \"Prenzlauer_Berg_Prenzlauer_Berg\" ‚Üí \"Prenzlauer Berg\")\n",
    "    if '_' in normalized:\n",
    "        parts = normalized.split('_')\n",
    "        # Nehme den ersten Teil oder entferne Duplikate\n",
    "        if len(parts) == 2 and parts[0] == parts[1]:\n",
    "            normalized = parts[0]\n",
    "        else:\n",
    "            normalized = parts[0]\n",
    "    \n",
    "    # Ersetze Underscores durch Spaces\n",
    "    normalized = normalized.replace('_', ' ')\n",
    "    \n",
    "    # Spezielle Mappings f√ºr bekannte Varianten\n",
    "    mappings = {\n",
    "        'Neu Hohensch√∂nhausen': 'Neu-Hohensch√∂nhausen',\n",
    "        'Hohensch√∂nhausen': 'Alt-Hohensch√∂nhausen',\n",
    "        'Franz√∂sisch Buchholz': 'Franz√∂sisch Buchholz',\n",
    "        'Gr√ºnau': 'Gr√ºnau',\n",
    "        'K√∂penick': 'K√∂penick'\n",
    "    }\n",
    "    \n",
    "    return mappings.get(normalized, normalized)\n",
    "\n",
    "# Teste die Normalisierung (verwende temp_df mit regio3)\n",
    "if 'regio3' in temp_df.columns:\n",
    "    temp_df['regio3_normalized'] = temp_df['regio3'].apply(normalize_regio3)\n",
    "    \n",
    "    print(\"Regio3 Normalisierung - Beispiele:\")\n",
    "    regio3_examples = temp_df[['regio3', 'regio3_normalized']].drop_duplicates().head(10)\n",
    "    for _, row in regio3_examples.iterrows():\n",
    "        print(f\"  {row['regio3']} ‚Üí {row['regio3_normalized']}\")\n",
    "    \n",
    "    print(f\"\\nUnikat regio3 (original): {temp_df['regio3'].nunique()}\")\n",
    "    print(f\"Unikat regio3 (normalized): {temp_df['regio3_normalized'].nunique()}\")\n",
    "else:\n",
    "    print(\"‚ùå regio3 Spalte nicht verf√ºgbar\")\n",
    "\n",
    "# Wende PLZ-Extraktion an\n",
    "print(\"\\n=== PLZ-EXTRAKTION ANWENDEN ===\")\n",
    "\n",
    "# Initialisiere PLZ-Spalte\n",
    "temp_df['plz'] = None\n",
    "\n",
    "# Strategie 1: Ortsteil-basiert\n",
    "ortsteil_matches = 0\n",
    "if 'regio3_normalized' in temp_df.columns:\n",
    "    for idx, row in temp_df.iterrows():\n",
    "        normalized_ortsteil = row['regio3_normalized']\n",
    "        if normalized_ortsteil in ortsteil_plz_mapping:\n",
    "            temp_df.loc[idx, 'plz'] = ortsteil_plz_mapping[normalized_ortsteil]\n",
    "            ortsteil_matches += 1\n",
    "\n",
    "# Strategie 2: Street-basiert (f√ºr fehlende PLZ)\n",
    "street_matches = 0\n",
    "for idx, row in temp_df.iterrows():\n",
    "    if pd.isna(row['plz']) and row['street'] in street_plz_mapping:\n",
    "        temp_df.loc[idx, 'plz'] = street_plz_mapping[row['street']]\n",
    "        street_matches += 1\n",
    "\n",
    "# √úbertrage PLZ zur√ºck zu df_normalized\n",
    "df_normalized['plz'] = temp_df['plz']\n",
    "\n",
    "# Ergebnisse\n",
    "total_with_plz = df_normalized['plz'].notna().sum()\n",
    "print(f\"PLZ-Extraktion Ergebnisse:\")\n",
    "print(f\"  Ortsteil-basiert: {ortsteil_matches:,} Matches\")\n",
    "print(f\"  Street-basiert: {street_matches:,} Matches\")\n",
    "print(f\"  Total mit PLZ: {total_with_plz:,}/{len(df_normalized):,} ({total_with_plz/len(df_normalized)*100:.1f}%)\")\n",
    "\n",
    "# Zeige PLZ-Verteilung\n",
    "if total_with_plz > 0:\n",
    "    print(f\"\\nTop 10 PLZ im 2018-2019 Dataset:\")\n",
    "    plz_counts = df_normalized['plz'].value_counts().head(10)\n",
    "    for plz, count in plz_counts.items():\n",
    "        print(f\"  {plz}: {count:,} Eintr√§ge\")\n",
    "\n",
    "print(f\"\\n‚úÖ PLZ-Extraktion f√ºr 2018-2019 Dataset abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f89a0211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç ERWEITERTE PLZ-EXTRAKTION F√úR 2018-2019\n",
      "============================================================\n",
      "üß™ TESTE ERWEITERTE PLZ-EXTRAKTION\n",
      "==================================================\n",
      "Test der erweiterten PLZ-Extraktion:\n",
      "   'Unter den Linden' in 'Mitte' ‚Üí PLZ: 10117\n",
      "   'Alexanderplatz' in 'Mitte' ‚Üí PLZ: 10178\n",
      "   'Boxhagener Stra√üe' in 'Friedrichshain' ‚Üí PLZ: 10245\n",
      "   'Kurf√ºrstendamm' in 'Charlottenburg' ‚Üí PLZ: 10711\n",
      "   'Sonnenallee' in 'Neuk√∂lln' ‚Üí PLZ: 12437\n",
      "   'Hauptstra√üe' in 'Steglitz' ‚Üí PLZ: 13158\n",
      "   'M√ºllerstra√üe' in 'Wedding' ‚Üí PLZ: 12623\n",
      "\n",
      "üîÑ WENDE ERWEITERTE PLZ-EXTRAKTION AN\n",
      "==================================================\n",
      "Verf√ºgbare Spalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'street', 'floor', 'typeOfFlat', 'yearConstructed', 'totalRent', 'plz']\n",
      "‚úÖ Erweiterte PLZ-Extraktion Ergebnisse:\n",
      "   üìä Nur erweiterte Methode: 9,164 von 10,387 (88.2%)\n",
      "   üìä Kombiniert (alt + neu): 10,173 von 10,387 (97.9%)\n",
      "   üìà Verbesserung: +0 PLZ (0.0% mehr)\n",
      "   üéâ ZIEL ERREICHT: >80% PLZ-Abdeckung!\n",
      "\n",
      "üìã Verbesserte PLZ-Verteilung (Top 10):\n",
      "   10179: 762 Eintr√§ge\n",
      "   10787: 703 Eintr√§ge\n",
      "   14059: 564 Eintr√§ge\n",
      "   10439: 450 Eintr√§ge\n",
      "   10249: 425 Eintr√§ge\n",
      "   13351: 395 Eintr√§ge\n",
      "   12059: 366 Eintr√§ge\n",
      "   14197: 354 Eintr√§ge\n",
      "   14052: 320 Eintr√§ge\n",
      "   12559: 314 Eintr√§ge\n",
      "\n",
      "‚úÖ Erweiterte PLZ-Extraktion f√ºr 2018-2019 Dataset abgeschlossen!\n",
      "============================================================\n",
      "EXPORT ANGEREICHERTES DATASET MIT DEBUG-VALIDIERUNG\n",
      "============================================================\n",
      "üîç KRITISCHE DEBUG-PR√úFUNG VOR EXPORT\n",
      "df_enriched Variable existiert: True\n",
      "df_enriched Zeilen: 10387\n",
      "df_enriched Spalten: 15\n",
      "df_enriched PLZ-Abdeckung: 10173/10387 (97.9%)\n",
      "df_enriched PLZ-Datentyp: object\n",
      "\n",
      "üîß SICHERE FINALE PLZ-STRING-KONVERTIERUNG\n",
      "PLZ vor finaler Konvertierung: 10173\n",
      "PLZ nach finaler Konvertierung: 10173\n",
      "\n",
      "üì§ EXPORT NACH: data/processed/dataset_2018_2019_enriched.csv\n",
      "Zu exportierende Zeilen: 10387\n",
      "Zu exportierende Spalten: 15\n",
      "Spalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'street', 'floor', 'typeOfFlat', 'yearConstructed', 'totalRent', 'plz', 'wol', 'ortsteil_neu']\n",
      "‚úÖ Angereichertes Dataset exportiert: data/processed/dataset_2018_2019_enriched.csv\n",
      "\n",
      "üîç SOFORTIGE VALIDIERUNG NACH EXPORT\n",
      "‚úÖ Export-Validierung: 10387 Zeilen geladen\n",
      "PLZ im Export (raw): 10173/10387 (97.9%)\n",
      "PLZ-Datentyp im Export (raw): float64\n",
      "PLZ-Beispiele im Export (raw): [13591.0, 10179.0, 10999.0]\n",
      "PLZ im Export (string): 10173/10387 (97.9%)\n",
      "PLZ-Datentyp im Export (string): string\n",
      "PLZ-Beispiele im Export (string): ['13591', '10179', '10999']\n",
      "\n",
      "üéØ FINALE ZUSAMMENFASSUNG\n",
      "Dataset 2018-2019 ANREICHERUNG ABGESCHLOSSEN!\n",
      "Input (normalized): 10387 Zeilen\n",
      "Output (enriched): 10387 Zeilen\n",
      "PLZ-Abdeckung: 10,173 Zeilen (97.9%)\n",
      "Ortsteil-Abdeckung: 9,505 Zeilen (91.5%)\n",
      "Wohnlage-Abdeckung: 9,505 Zeilen (91.5%)\n",
      "üéØ Bereit f√ºr Kombination mit anderen enriched Datasets!\n",
      "\n",
      "‚úÖ PLZ-Daten erfolgreich bewahrt!\n",
      "‚úÖ Angereichertes Dataset exportiert: data/processed/dataset_2018_2019_enriched.csv\n",
      "\n",
      "üîç SOFORTIGE VALIDIERUNG NACH EXPORT\n",
      "‚úÖ Export-Validierung: 10387 Zeilen geladen\n",
      "PLZ im Export (raw): 10173/10387 (97.9%)\n",
      "PLZ-Datentyp im Export (raw): float64\n",
      "PLZ-Beispiele im Export (raw): [13591.0, 10179.0, 10999.0]\n",
      "PLZ im Export (string): 10173/10387 (97.9%)\n",
      "PLZ-Datentyp im Export (string): string\n",
      "PLZ-Beispiele im Export (string): ['13591', '10179', '10999']\n",
      "\n",
      "üéØ FINALE ZUSAMMENFASSUNG\n",
      "Dataset 2018-2019 ANREICHERUNG ABGESCHLOSSEN!\n",
      "Input (normalized): 10387 Zeilen\n",
      "Output (enriched): 10387 Zeilen\n",
      "PLZ-Abdeckung: 10,173 Zeilen (97.9%)\n",
      "Ortsteil-Abdeckung: 9,505 Zeilen (91.5%)\n",
      "Wohnlage-Abdeckung: 9,505 Zeilen (91.5%)\n",
      "üéØ Bereit f√ºr Kombination mit anderen enriched Datasets!\n",
      "\n",
      "‚úÖ PLZ-Daten erfolgreich bewahrt!\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# ERWEITERTE PLZ-EXTRAKTION F√úR 2018-2019 DATASET\n",
    "# ===================================================================\n",
    "print(\"\\nüîç ERWEITERTE PLZ-EXTRAKTION F√úR 2018-2019\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def extract_plz_advanced_2018_2019(street, district, ortsteil_mapping=None, street_mapping=None):\n",
    "    \"\"\"\n",
    "    Erweiterte PLZ-Extraktion f√ºr 2018-2019 Dataset mit mehreren Fallback-Strategien\n",
    "    \n",
    "    Args:\n",
    "        street: Stra√üenname\n",
    "        district: Bezirk\n",
    "        ortsteil_mapping: Ortsteil-zu-PLZ-Mapping\n",
    "        street_mapping: Street-zu-PLZ-Mapping\n",
    "    \n",
    "    Returns:\n",
    "        str: PLZ oder None\n",
    "    \"\"\"\n",
    "    # Strategie 1: Street-basierte PLZ-Extraktion (falls vorhanden)\n",
    "    if pd.notna(street) and street_mapping and street in street_mapping:\n",
    "        return str(street_mapping[street])\n",
    "    \n",
    "    # Strategie 2: Bezirk-zu-PLZ-Mapping (erweitert)\n",
    "    if pd.notna(district) and ortsteil_mapping:\n",
    "        district_str = str(district).strip()\n",
    "        \n",
    "        # Direkte Bezirks-Zuordnung\n",
    "        if district_str in ortsteil_mapping:\n",
    "            return str(ortsteil_mapping[district_str])\n",
    "        \n",
    "        # Erweiterte Bezirks-Aliases\n",
    "        district_aliases = {\n",
    "            # Hauptbezirke\n",
    "            'Mitte': 'Mitte',\n",
    "            'Friedrichshain-Kreuzberg': 'Friedrichshain',\n",
    "            'Pankow': 'Pankow',\n",
    "            'Charlottenburg-Wilmersdorf': 'Charlottenburg',\n",
    "            'Spandau': 'Spandau',\n",
    "            'Steglitz-Zehlendorf': 'Steglitz',\n",
    "            'Tempelhof-Sch√∂neberg': 'Tempelhof',\n",
    "            'Neuk√∂lln': 'Neuk√∂lln',\n",
    "            'Treptow-K√∂penick': 'Treptow',\n",
    "            'Marzahn-Hellersdorf': 'Marzahn',\n",
    "            'Lichtenberg': 'Lichtenberg',\n",
    "            'Reinickendorf': 'Reinickendorf',\n",
    "            # Einzelteile zusammengesetzter Bezirke\n",
    "            'Friedrichshain': 'Friedrichshain',\n",
    "            'Kreuzberg': 'Kreuzberg',\n",
    "            'Charlottenburg': 'Charlottenburg',\n",
    "            'Wilmersdorf': 'Wilmersdorf',\n",
    "            'Steglitz': 'Steglitz',\n",
    "            'Zehlendorf': 'Zehlendorf',\n",
    "            'Tempelhof': 'Tempelhof',\n",
    "            'Sch√∂neberg': 'Sch√∂neberg',\n",
    "            'Treptow': 'Treptow',\n",
    "            'K√∂penick': 'K√∂penick',\n",
    "            'Marzahn': 'Marzahn',\n",
    "            'Hellersdorf': 'Hellersdorf'\n",
    "        }\n",
    "        \n",
    "        # Pr√ºfe Aliases\n",
    "        for alias, canonical in district_aliases.items():\n",
    "            if alias.lower() in district_str.lower():\n",
    "                if canonical in ortsteil_mapping:\n",
    "                    return str(ortsteil_mapping[canonical])\n",
    "    \n",
    "    # Strategie 3: H√§ufige Stra√üenname-Patterns\n",
    "    if pd.notna(street):\n",
    "        street_str = str(street).strip()\n",
    "        \n",
    "        # Bekannte Stra√üen-zu-PLZ-Mapping (h√§ufigste Berliner Stra√üen)\n",
    "        common_streets = {\n",
    "            'Unter den Linden': '10117',\n",
    "            'Alexanderplatz': '10178',\n",
    "            'Potsdamer Platz': '10785',\n",
    "            'Kurf√ºrstendamm': '10719',\n",
    "            'Friedrichstra√üe': '10117',\n",
    "            'Hackescher Markt': '10178',\n",
    "            'Warschauer Stra√üe': '10243',\n",
    "            'Boxhagener Stra√üe': '10245',\n",
    "            'Kastanienallee': '10435',\n",
    "            'Oranienstra√üe': '10999',\n",
    "            'Bergmannstra√üe': '10961',\n",
    "            'Savignyplatz': '10623',\n",
    "            'Rosenthaler Stra√üe': '10119',\n",
    "            'Torstra√üe': '10119',\n",
    "            'Invalidenstra√üe': '10115',\n",
    "            'Chausseestra√üe': '10115',\n",
    "            'Brunnenstra√üe': '10119',\n",
    "            'Bernauer Stra√üe': '10119',\n",
    "            'Prenzlauer Allee': '10405',\n",
    "            'Karl-Marx-Allee': '10243',\n",
    "            'Frankfurter Allee': '10247',\n",
    "            'Sonnenallee': '12047',\n",
    "            'Hermannstra√üe': '12049',\n",
    "            'Kantstra√üe': '10623',\n",
    "            'Wilmersdorfer Stra√üe': '10627',\n",
    "            'Uhlandstra√üe': '10623',\n",
    "            'Ku\\'damm': '10719',\n",
    "            'Tauentzienstra√üe': '10789',\n",
    "            'Nollendorfplatz': '10777',\n",
    "            'Wittenbergplatz': '10789'\n",
    "        }\n",
    "        \n",
    "        # Pr√ºfe auf bekannte Stra√üen (auch Teilstrings)\n",
    "        for known_street, plz in common_streets.items():\n",
    "            if known_street.lower() in street_str.lower():\n",
    "                return plz\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Teste die erweiterte PLZ-Extraktion\n",
    "print(\"üß™ TESTE ERWEITERTE PLZ-EXTRAKTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_cases = [\n",
    "    (\"Unter den Linden\", \"Mitte\"),\n",
    "    (\"Alexanderplatz\", \"Mitte\"),\n",
    "    (\"Boxhagener Stra√üe\", \"Friedrichshain\"),\n",
    "    (\"Kurf√ºrstendamm\", \"Charlottenburg\"),\n",
    "    (\"Sonnenallee\", \"Neuk√∂lln\"),\n",
    "    (\"Hauptstra√üe\", \"Steglitz\"),\n",
    "    (\"M√ºllerstra√üe\", \"Wedding\")\n",
    "]\n",
    "\n",
    "print(\"Test der erweiterten PLZ-Extraktion:\")\n",
    "for street, district in test_cases:\n",
    "    plz = extract_plz_advanced_2018_2019(street, district, ortsteil_plz_mapping, street_plz_mapping)\n",
    "    print(f\"   '{street}' in '{district}' ‚Üí PLZ: {plz}\")\n",
    "\n",
    "# Erstelle erweiterte PLZ-Extraktion f√ºr alle Daten\n",
    "print(\"\\nüîÑ WENDE ERWEITERTE PLZ-EXTRAKTION AN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Verwende die verf√ºgbaren Spalten\n",
    "available_columns = df_normalized.columns.tolist()\n",
    "print(f\"Verf√ºgbare Spalten: {available_columns}\")\n",
    "\n",
    "# Wende erweiterte PLZ-Extraktion an\n",
    "df_normalized['plz_advanced'] = df_normalized.apply(\n",
    "    lambda row: extract_plz_advanced_2018_2019(\n",
    "        row.get('street', None),\n",
    "        row.get('district', None),\n",
    "        ortsteil_plz_mapping,\n",
    "        street_plz_mapping\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Kombiniere alte und neue PLZ-Extraktion\n",
    "df_normalized['plz_combined'] = df_normalized['plz_advanced'].fillna(df_normalized['plz'])\n",
    "\n",
    "# Ergebnisse der erweiterten PLZ-Extraktion\n",
    "plz_advanced_found = df_normalized['plz_advanced'].notna().sum()\n",
    "plz_combined_found = df_normalized['plz_combined'].notna().sum()\n",
    "total_rows = len(df_normalized)\n",
    "\n",
    "print(f\"‚úÖ Erweiterte PLZ-Extraktion Ergebnisse:\")\n",
    "print(f\"   üìä Nur erweiterte Methode: {plz_advanced_found:,} von {total_rows:,} ({plz_advanced_found/total_rows*100:.1f}%)\")\n",
    "print(f\"   üìä Kombiniert (alt + neu): {plz_combined_found:,} von {total_rows:,} ({plz_combined_found/total_rows*100:.1f}%)\")\n",
    "\n",
    "# Verbesserung berechnen\n",
    "old_plz_count = df_normalized['plz'].notna().sum()\n",
    "improvement = plz_combined_found - old_plz_count\n",
    "improvement_pct = (improvement / old_plz_count) * 100 if old_plz_count > 0 else 0\n",
    "\n",
    "print(f\"   üìà Verbesserung: +{improvement:,} PLZ ({improvement_pct:.1f}% mehr)\")\n",
    "\n",
    "if plz_combined_found / total_rows >= 0.8:\n",
    "    print(f\"   üéâ ZIEL ERREICHT: >80% PLZ-Abdeckung!\")\n",
    "elif improvement > 0:\n",
    "    print(f\"   üìà VERBESSERUNG: Erh√∂hte PLZ-Abdeckung\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  KEINE VERBESSERUNG: Alternative Strategie erforderlich\")\n",
    "\n",
    "# Aktualisiere die PLZ-Spalte mit dem kombinierten Ergebnis\n",
    "df_normalized['plz'] = df_normalized['plz_combined']\n",
    "\n",
    "# Zeige verbesserte PLZ-Verteilung\n",
    "if plz_combined_found > 0:\n",
    "    print(f\"\\nüìã Verbesserte PLZ-Verteilung (Top 10):\")\n",
    "    plz_counts_new = df_normalized['plz'].value_counts().head(10)\n",
    "    for plz, count in plz_counts_new.items():\n",
    "        print(f\"   {plz}: {count:,} Eintr√§ge\")\n",
    "\n",
    "# Bereinige tempor√§re Spalten\n",
    "df_normalized = df_normalized.drop(['plz_advanced', 'plz_combined'], axis=1)\n",
    "\n",
    "print(f\"\\n‚úÖ Erweiterte PLZ-Extraktion f√ºr 2018-2019 Dataset abgeschlossen!\")\n",
    "\n",
    "# ============================================================\n",
    "# EXPORT ANGEREICHERTES DATASET MIT DEBUG-VALIDIERUNG\n",
    "# ============================================================\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPORT ANGEREICHERTES DATASET MIT DEBUG-VALIDIERUNG\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# KRITISCHE DEBUG-PR√úFUNG VOR EXPORT\n",
    "print(\"üîç KRITISCHE DEBUG-PR√úFUNG VOR EXPORT\")\n",
    "print(f\"df_enriched Variable existiert: {'df_enriched' in locals()}\")\n",
    "print(f\"df_enriched Zeilen: {len(df_enriched)}\")\n",
    "print(f\"df_enriched Spalten: {len(df_enriched.columns)}\")\n",
    "print(f\"df_enriched PLZ-Abdeckung: {df_enriched['plz'].notna().sum()}/{len(df_enriched)} ({df_enriched['plz'].notna().sum()/len(df_enriched)*100:.1f}%)\")\n",
    "print(f\"df_enriched PLZ-Datentyp: {df_enriched['plz'].dtype}\")\n",
    "\n",
    "# Sichere finale PLZ-String-Konvertierung\n",
    "print(\"\\nüîß SICHERE FINALE PLZ-STRING-KONVERTIERUNG\")\n",
    "def convert_plz_to_clean_string(plz_value):\n",
    "    if pd.isna(plz_value):\n",
    "        return None\n",
    "    try:\n",
    "        return str(int(float(plz_value)))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Wende String-Konvertierung auf alle relevanten Spalten an\n",
    "plz_before_final_conversion = df_enriched['plz'].notna().sum()\n",
    "df_enriched['plz'] = df_enriched['plz'].apply(convert_plz_to_clean_string)\n",
    "plz_after_final_conversion = df_enriched['plz'].notna().sum()\n",
    "\n",
    "print(f\"PLZ vor finaler Konvertierung: {plz_before_final_conversion}\")\n",
    "print(f\"PLZ nach finaler Konvertierung: {plz_after_final_conversion}\")\n",
    "\n",
    "# Weitere String-Konvertierungen\n",
    "if 'ortsteil_neu' in df_enriched.columns:\n",
    "    df_enriched['ortsteil_neu'] = df_enriched['ortsteil_neu'].astype(str).replace('nan', '')\n",
    "    df_enriched.loc[df_enriched['ortsteil_neu'] == '', 'ortsteil_neu'] = np.nan\n",
    "\n",
    "if 'wol' in df_enriched.columns:\n",
    "    df_enriched['wol'] = df_enriched['wol'].astype(str).replace('nan', '')\n",
    "    df_enriched.loc[df_enriched['wol'] == '', 'wol'] = np.nan\n",
    "\n",
    "# Export path\n",
    "output_file_enriched = 'data/processed/dataset_2018_2019_enriched.csv'\n",
    "\n",
    "print(f\"\\nüì§ EXPORT NACH: {output_file_enriched}\")\n",
    "print(f\"Zu exportierende Zeilen: {len(df_enriched)}\")\n",
    "print(f\"Zu exportierende Spalten: {len(df_enriched.columns)}\")\n",
    "print(f\"Spalten: {list(df_enriched.columns)}\")\n",
    "\n",
    "# KRITISCHER EXPORT\n",
    "df_enriched.to_csv(output_file_enriched, index=False)\n",
    "print(f\"‚úÖ Angereichertes Dataset exportiert: {output_file_enriched}\")\n",
    "\n",
    "# SOFORTIGE VALIDIERUNG NACH EXPORT\n",
    "print(f\"\\nüîç SOFORTIGE VALIDIERUNG NACH EXPORT\")\n",
    "test_load_enriched = pd.read_csv(output_file_enriched)\n",
    "print(f\"‚úÖ Export-Validierung: {len(test_load_enriched)} Zeilen geladen\")\n",
    "\n",
    "# PLZ-Validierung (ohne dtype-Parameter)\n",
    "if 'plz' in test_load_enriched.columns:\n",
    "    plz_export_raw = test_load_enriched['plz'].notna().sum()\n",
    "    print(f\"PLZ im Export (raw): {plz_export_raw}/{len(test_load_enriched)} ({plz_export_raw/len(test_load_enriched)*100:.1f}%)\")\n",
    "    print(f\"PLZ-Datentyp im Export (raw): {test_load_enriched['plz'].dtype}\")\n",
    "    print(f\"PLZ-Beispiele im Export (raw): {test_load_enriched['plz'].dropna().head(3).tolist()}\")\n",
    "\n",
    "# PLZ-Validierung (mit dtype=string)\n",
    "test_load_string = pd.read_csv(output_file_enriched, dtype={'plz': 'string'})\n",
    "if 'plz' in test_load_string.columns:\n",
    "    plz_export_string = test_load_string['plz'].notna().sum()\n",
    "    print(f\"PLZ im Export (string): {plz_export_string}/{len(test_load_string)} ({plz_export_string/len(test_load_string)*100:.1f}%)\")\n",
    "    print(f\"PLZ-Datentyp im Export (string): {test_load_string['plz'].dtype}\")\n",
    "    print(f\"PLZ-Beispiele im Export (string): {test_load_string['plz'].dropna().head(3).tolist()}\")\n",
    "\n",
    "# FINALE ERFOLGSMELDUNG\n",
    "print(f\"\\nüéØ FINALE ZUSAMMENFASSUNG\")\n",
    "print(f\"Dataset 2018-2019 ANREICHERUNG ABGESCHLOSSEN!\")\n",
    "print(f\"Input (normalized): {len(df_normalized)} Zeilen\")\n",
    "print(f\"Output (enriched): {len(df_enriched)} Zeilen\")\n",
    "print(f\"PLZ-Abdeckung: {df_enriched['plz'].notna().sum():,} Zeilen ({df_enriched['plz'].notna().sum()/len(df_enriched)*100:.1f}%)\")\n",
    "\n",
    "if 'ortsteil_neu' in df_enriched.columns:\n",
    "    ortsteil_coverage = df_enriched['ortsteil_neu'].notna().sum()\n",
    "    print(f\"Ortsteil-Abdeckung: {ortsteil_coverage:,} Zeilen ({ortsteil_coverage/len(df_enriched)*100:.1f}%)\")\n",
    "\n",
    "if 'wol' in df_enriched.columns:\n",
    "    wol_coverage = df_enriched['wol'].notna().sum()\n",
    "    print(f\"Wohnlage-Abdeckung: {wol_coverage:,} Zeilen ({wol_coverage/len(df_enriched)*100:.1f}%)\")\n",
    "\n",
    "print(f\"üéØ Bereit f√ºr Kombination mit anderen enriched Datasets!\")\n",
    "\n",
    "# LETZTE WARNUNG\n",
    "if df_enriched['plz'].notna().sum() < df_normalized['plz'].notna().sum():\n",
    "    print(f\"\\n‚ö†Ô∏è WARNUNG: PLZ-Datenverlust erkannt!\")\n",
    "    print(f\"   Normalized: {df_normalized['plz'].notna().sum():,} PLZ\")\n",
    "    print(f\"   Enriched:   {df_enriched['plz'].notna().sum():,} PLZ\") \n",
    "    print(f\"   Verlust:    {df_normalized['plz'].notna().sum() - df_enriched['plz'].notna().sum():,} PLZ\")\n",
    "else:\n",
    "    print(f\"\\n‚úÖ PLZ-Daten erfolgreich bewahrt!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t0",
   "metadata": {},
   "source": [
    "## 8. Export des finalen angereicherten Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "u1v2w3x4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT FINALES ANGEREICHERTES DATASET\n",
      "============================================================\n",
      "‚úÖ Finales angereichertes Dataset exportiert: data/processed/dataset_2018_2019_enriched.csv\n",
      "Dateigr√∂√üe: 10,387 Zeilen x 15 Spalten\n",
      "‚úÖ Export-Validierung erfolgreich: 10,387 Zeilen geladen\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPORT FINALES ANGEREICHERTES DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Export\n",
    "output_file_enriched = 'data/processed/dataset_2018_2019_enriched.csv'\n",
    "df_enriched.to_csv(output_file_enriched, index=False)\n",
    "\n",
    "print(f\"‚úÖ Finales angereichertes Dataset exportiert: {output_file_enriched}\")\n",
    "print(f\"Dateigr√∂√üe: {len(df_enriched):,} Zeilen x {len(df_enriched.columns)} Spalten\")\n",
    "\n",
    "# Validierung durch Wiedereinlesen\n",
    "test_df_enriched = pd.read_csv(output_file_enriched)\n",
    "print(f\"‚úÖ Export-Validierung erfolgreich: {len(test_df_enriched):,} Zeilen geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39382f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß KRITISCHE PLZ-DATENTYP-REPARATUR VOR EXPORT\n",
      "============================================================\n",
      "PLZ-Datentyp vor Reparatur: object\n",
      "PLZ-Beispiele vor Reparatur: [np.int64(13591), np.int64(12527), np.int64(13053)]\n",
      "PLZ-Datentyp nach Reparatur: object\n",
      "PLZ-Beispiele nach Reparatur: ['13591', '12527', '13053']\n",
      "‚úÖ PLZ-Datentyp-Reparatur abgeschlossen!\n",
      "   ‚û°Ô∏è  PLZ wird jetzt als String gespeichert f√ºr korrekten Join\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# KRITISCHE PLZ-DATENTYP-REPARATUR VOR EXPORT\n",
    "# ===================================================================\n",
    "print(\"\\nüîß KRITISCHE PLZ-DATENTYP-REPARATUR VOR EXPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Stelle sicher, dass PLZ als STRING gespeichert wird (nicht als Integer/Float)\n",
    "# Dies ist kritisch f√ºr den sp√§teren Join in 04_Combine_Datasets.ipynb\n",
    "print(\"PLZ-Datentyp vor Reparatur:\", df_enriched['plz'].dtype)\n",
    "print(\"PLZ-Beispiele vor Reparatur:\", df_enriched['plz'].dropna().head(3).tolist())\n",
    "\n",
    "# Konvertiere PLZ zu String\n",
    "df_enriched['plz'] = df_enriched['plz'].apply(\n",
    "    lambda x: str(int(x)) if pd.notna(x) else None\n",
    ")\n",
    "\n",
    "print(\"PLZ-Datentyp nach Reparatur:\", df_enriched['plz'].dtype)  \n",
    "print(\"PLZ-Beispiele nach Reparatur:\", df_enriched['plz'].dropna().head(3).tolist())\n",
    "\n",
    "print(\"‚úÖ PLZ-Datentyp-Reparatur abgeschlossen!\")\n",
    "print(\"   ‚û°Ô∏è  PLZ wird jetzt als String gespeichert f√ºr korrekten Join\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
