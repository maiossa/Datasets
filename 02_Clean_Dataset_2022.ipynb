{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfab51b2",
   "metadata": {},
   "source": [
    "# Dataset 2022 Bereinigung und Normalisierung\n",
    "## Spezialisiertes Modul f√ºr Springer/Immowelt/Immonet Dataset\n",
    "\n",
    "### Ziel\n",
    "Bereinigung und Normalisierung des aktuellen Datasets (2022) in ein standardisiertes Format f√ºr die gemeinsame Analyse.\n",
    "\n",
    "### Input\n",
    "- `data/raw/Dataset_2022.csv`\n",
    "- `data/processed/berlin_plz_mapping.csv` (PLZ-zu-Bezirk-Mapping)\n",
    "\n",
    "### Output\n",
    "- `data/processed/dataset_2022_normalized.csv`\n",
    "\n",
    "### Besonderheiten\n",
    "- **PLZ-zu-Bezirk-Mapping erforderlich** (Dataset enth√§lt nur PLZ, keine Bezirksnamen)\n",
    "- Umfangreiche Spaltenstruktur mit vielen Features\n",
    "- Deutsche Zahlenformate\n",
    "\n",
    "### Standardisierte Ausgabespalten\n",
    "- `price`: Normalisierter Preis (KALTMIETE in ‚Ç¨)\n",
    "- `size`: Normalisierte Gr√∂√üe (WOHNFLAECHE in m¬≤)\n",
    "- `district`: Berliner Bezirk (via PLZ-Mapping)\n",
    "- `rooms`: Anzahl Zimmer (ZIMMER)\n",
    "- `year`: Jahr des Datasets (2022)\n",
    "- `dataset_id`: Eindeutige Dataset-Kennzeichnung (current)\n",
    "- `source`: Datenquelle\n",
    "\n",
    "---\n",
    "**Teil der modularen Preprocessing-Pipeline**  \n",
    "**Datum:** 4. Juli 2025  \n",
    "**Version:** 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754def0",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "102355a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotheken erfolgreich importiert!\n",
      "Pandas Version: 2.2.3\n",
      "Dataset: 2022 (Springer/Immowelt/Immonet)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "print(\"Bibliotheken erfolgreich importiert!\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"Dataset: 2022 (Springer/Immowelt/Immonet)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1604fc",
   "metadata": {},
   "source": [
    "## 2. PLZ-zu-Bezirk-Mapping laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d98e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PLZ-ZU-BEZIRK-MAPPING LADEN\n",
      "============================================================\n",
      "‚úÖ PLZ-Mapping geladen: 181 Eintr√§ge\n",
      "‚úÖ PLZ-Dictionary erstellt: 181 Zuordnungen\n",
      "\n",
      "Beispiele:\n",
      "  10115 ‚Üí Mitte\n",
      "  10117 ‚Üí Mitte\n",
      "  10119 ‚Üí Prenzlauer Berg\n",
      "  10178 ‚Üí Mitte\n",
      "  10179 ‚Üí Mitte\n",
      "\n",
      "Abgedeckte Bezirke: 19\n",
      "Bezirke: ['Charlottenburg', 'Friedrichshain', 'Kreuzberg', 'Lichtenberg', 'Marzahn-Hellersdorf', 'Mitte', 'Neuk√∂lln', 'Pankow', 'Prenzlauer Berg', 'Reinickendorf', 'Sch√∂neberg', 'Spandau', 'Steglitz', 'Tempelhof', 'Tiergarten', 'Treptow-K√∂penick', 'Wedding', 'Wilmersdorf', 'Zehlendorf']\n"
     ]
    }
   ],
   "source": [
    "# PLZ-zu-Bezirk-Mapping laden\n",
    "print(\"=\" * 60)\n",
    "print(\"PLZ-ZU-BEZIRK-MAPPING LADEN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    plz_mapping_df = pd.read_csv('data/processed/berlin_plz_mapping.csv')\n",
    "    print(f\"‚úÖ PLZ-Mapping geladen: {len(plz_mapping_df)} Eintr√§ge\")\n",
    "    \n",
    "    # Erstelle Dictionary f√ºr schnelles Lookup\n",
    "    plz_to_district = dict(zip(plz_mapping_df['PLZ'], plz_mapping_df['Bezirk']))\n",
    "    print(f\"‚úÖ PLZ-Dictionary erstellt: {len(plz_to_district)} Zuordnungen\")\n",
    "    \n",
    "    # Zeige einige Beispiele\n",
    "    print(f\"\\nBeispiele:\")\n",
    "    for plz, bezirk in list(plz_to_district.items())[:5]:\n",
    "        print(f\"  {plz} ‚Üí {bezirk}\")\n",
    "        \n",
    "    print(f\"\\nAbgedeckte Bezirke: {len(set(plz_to_district.values()))}\")\n",
    "    print(f\"Bezirke: {sorted(set(plz_to_district.values()))}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå FEHLER: PLZ-Mapping nicht gefunden!\")\n",
    "    print(\"Bitte stellen Sie sicher, dass 'data/processed/berlin_plz_mapping.csv' existiert.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f0cfe",
   "metadata": {},
   "source": [
    "## 3. Daten laden und erste Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c255b68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET 2022 LADEN UND ANALYSIEREN\n",
      "============================================================\n",
      "Dataset geladen: 2,950 Zeilen, 75 Spalten\n",
      "\n",
      "Erste 10 Spalten: ['ID', 'SORTE', 'PLZ', 'KALTMIETE', 'WARMMIETE', 'NEBENKOSTEN', 'KAUTION', 'HEIZUNGSKOSTEN', 'ZIMMER', 'PARKPLAETZE']\n",
      "Letzte 5 Spalten: ['Stein', 'Marmor', 'Doppelboden', 'Terracotta', 'Sonstiges']\n",
      "\n",
      "=== KERNFELDER ANALYSE ===\n",
      "PLZ: 2950/2950 (100.0%) nicht-null\n",
      "KALTMIETE: 2772/2950 (94.0%) nicht-null\n",
      "WOHNFLAECHE: 2950/2950 (100.0%) nicht-null\n",
      "ZIMMER: 2942/2950 (99.7%) nicht-null\n",
      "\n",
      "=== PLZ-ANALYSE ===\n",
      "Einzigartige PLZ: 183\n",
      "PLZ-Beispiele: [10115, 10117, 10119, 10178, 10179, 10243, 10245, 10247, 10249, 10315]\n",
      "\n",
      "Erste 5 Zeilen (Kernfelder):\n",
      "     PLZ  KALTMIETE  WOHNFLAECHE  ZIMMER\n",
      "0  13125     860.00        73.00     3.0\n",
      "1  13125     450.28        48.84     2.0\n",
      "2  13125     739.00        54.79     2.0\n",
      "3  13125     899.00        74.49     3.0\n",
      "4  13125     899.00        74.49     3.0\n"
     ]
    }
   ],
   "source": [
    "# Lade Dataset 2022\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET 2022 LADEN UND ANALYSIEREN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Lade Rohdaten\n",
    "df_raw = pd.read_csv('data/raw/Dataset_2022.csv')\n",
    "print(f\"Dataset geladen: {df_raw.shape[0]:,} Zeilen, {df_raw.shape[1]} Spalten\")\n",
    "\n",
    "# Grundlegende Informationen\n",
    "print(f\"\\nErste 10 Spalten: {list(df_raw.columns[:10])}\")\n",
    "print(f\"Letzte 5 Spalten: {list(df_raw.columns[-5:])}\")\n",
    "\n",
    "# Kernfelder analysieren\n",
    "core_fields = ['PLZ', 'KALTMIETE', 'WOHNFLAECHE', 'ZIMMER']\n",
    "print(f\"\\n=== KERNFELDER ANALYSE ===\")\n",
    "for field in core_fields:\n",
    "    if field in df_raw.columns:\n",
    "        non_null = df_raw[field].notna().sum()\n",
    "        print(f\"{field}: {non_null}/{len(df_raw)} ({non_null/len(df_raw)*100:.1f}%) nicht-null\")\n",
    "    else:\n",
    "        print(f\"‚ùå {field}: Spalte nicht gefunden!\")\n",
    "\n",
    "# PLZ-Analyse\n",
    "print(f\"\\n=== PLZ-ANALYSE ===\")\n",
    "unique_plz = df_raw['PLZ'].dropna().unique()\n",
    "print(f\"Einzigartige PLZ: {len(unique_plz)}\")\n",
    "print(f\"PLZ-Beispiele: {sorted(unique_plz)[:10]}\")\n",
    "\n",
    "# Erste 5 Zeilen der Kernfelder\n",
    "print(f\"\\nErste 5 Zeilen (Kernfelder):\")\n",
    "print(df_raw[core_fields].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2f21d",
   "metadata": {},
   "source": [
    "## 4. Spezifische Bereinigung Dataset 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f3a6a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPEZIFISCHE BEREINIGUNG DATASET 2022\n",
      "============================================================\n",
      "Arbeitskopie erstellt: 2950 Zeilen\n",
      "\n",
      "=== PREIS-BEREINIGUNG (KALTMIETE) ===\n",
      "KALTMIETE - Typ: float64\n",
      "Nicht-null Werte: 2772\n",
      "Nach Entfernung NaN-Preise: 2772 Zeilen\n",
      "G√ºltige Preise nach Bereinigung: 2772/2772 (100.0%)\n",
      "Preisspanne: 163.31‚Ç¨ - 3000.00‚Ç¨\n",
      "Entfernte unrealistische Preise: 0\n",
      "\n",
      "=== GR√ñSSEN-BEREINIGUNG (WOHNFLAECHE) ===\n",
      "WOHNFLAECHE - Typ: float64\n",
      "Nicht-null Werte: 2772\n",
      "G√ºltige Gr√∂√üen nach Bereinigung: 2772/2772 (100.0%)\n",
      "Gr√∂√üenspanne: 13.0m¬≤ - 230.0m¬≤\n",
      "Entfernte unrealistische Gr√∂√üen: 0\n",
      "\n",
      "=== PLZ-ZU-BEZIRK-ZUORDNUNG ===\n",
      "PLZ - Typ: int64\n",
      "Nicht-null PLZ: 2772\n",
      "\n",
      "=== ERWEITERUNG DES PLZ-MAPPINGS ===\n",
      "PLZ-Mapping erweitert: 188 Zuordnungen\n",
      "Erfolgreiche PLZ-zu-Bezirk-Zuordnungen: 2681/2772 (96.7%)\n",
      "\n",
      "Verbleibende nicht zugeordnete PLZ (10 einzigartige):\n",
      "  12247: 16 Eintr√§ge\n",
      "  12207: 16 Eintr√§ge\n",
      "  13627: 12 Eintr√§ge\n",
      "  12203: 11 Eintr√§ge\n",
      "  12209: 9 Eintr√§ge\n",
      "  12279: 7 Eintr√§ge\n",
      "  12277: 7 Eintr√§ge\n",
      "  12249: 6 Eintr√§ge\n",
      "  10551: 4 Eintr√§ge\n",
      "  12205: 3 Eintr√§ge\n",
      "Entfernte Eintr√§ge ohne Bezirk: 91\n",
      "\n",
      "=== ZIMMER-BEREINIGUNG (ZIMMER) ===\n",
      "ZIMMER - Typ: float64\n",
      "Nicht-null Werte: 2676\n",
      "G√ºltige Zimmerzahlen nach Bereinigung: 2676/2681 (99.8%)\n",
      "Zimmerspanne: 1.0 - 5.0\n",
      "Entfernte unrealistische Zimmerzahlen: 5\n",
      "\n",
      "‚úÖ Spezifische Bereinigung abgeschlossen\n",
      "Verbleibende Datens√§tze: 2676 (Verlust: 274)\n"
     ]
    }
   ],
   "source": [
    "# Spezifische Bereinigung f√ºr Dataset 2022\n",
    "print(\"=\" * 60)\n",
    "print(\"SPEZIFISCHE BEREINIGUNG DATASET 2022\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Erstelle Arbeitskopie\n",
    "df_clean = df_raw.copy()\n",
    "print(f\"Arbeitskopie erstellt: {len(df_clean)} Zeilen\")\n",
    "\n",
    "# === PREIS-BEREINIGUNG (KALTMIETE) ===\n",
    "print(\"\\n=== PREIS-BEREINIGUNG (KALTMIETE) ===\")\n",
    "print(f\"KALTMIETE - Typ: {df_clean['KALTMIETE'].dtype}\")\n",
    "print(f\"Nicht-null Werte: {df_clean['KALTMIETE'].notna().sum()}\")\n",
    "\n",
    "# Nur Zeilen mit g√ºltigen Preisen behalten\n",
    "df_clean = df_clean[df_clean['KALTMIETE'].notna()]\n",
    "print(f\"Nach Entfernung NaN-Preise: {len(df_clean)} Zeilen\")\n",
    "\n",
    "# Unrealistische Preise entfernen (< 100‚Ç¨ oder > 10000‚Ç¨) - Konsistent mit anderen Datasets\n",
    "initial_count = len(df_clean)\n",
    "df_clean = df_clean[(df_clean['KALTMIETE'] >= 100) & (df_clean['KALTMIETE'] <= 10000)]\n",
    "removed_prices = initial_count - len(df_clean)\n",
    "print(f\"G√ºltige Preise nach Bereinigung: {len(df_clean)}/{initial_count} ({100*len(df_clean)/initial_count:.1f}%)\")\n",
    "print(f\"Preisspanne: {df_clean['KALTMIETE'].min():.2f}‚Ç¨ - {df_clean['KALTMIETE'].max():.2f}‚Ç¨\")\n",
    "print(f\"Entfernte unrealistische Preise: {removed_prices}\")\n",
    "\n",
    "# === GR√ñSSEN-BEREINIGUNG (WOHNFLAECHE) ===\n",
    "print(\"\\n=== GR√ñSSEN-BEREINIGUNG (WOHNFLAECHE) ===\")\n",
    "print(f\"WOHNFLAECHE - Typ: {df_clean['WOHNFLAECHE'].dtype}\")\n",
    "print(f\"Nicht-null Werte: {df_clean['WOHNFLAECHE'].notna().sum()}\")\n",
    "\n",
    "# Unrealistische Gr√∂√üen entfernen (< 10m¬≤ oder > 500m¬≤) - Konsistent mit anderen Datasets\n",
    "initial_count = len(df_clean)\n",
    "df_clean = df_clean[(df_clean['WOHNFLAECHE'] >= 10) & (df_clean['WOHNFLAECHE'] <= 500)]\n",
    "removed_sizes = initial_count - len(df_clean)\n",
    "print(f\"G√ºltige Gr√∂√üen nach Bereinigung: {len(df_clean)}/{initial_count} ({100*len(df_clean)/initial_count:.1f}%)\")\n",
    "print(f\"Gr√∂√üenspanne: {df_clean['WOHNFLAECHE'].min():.1f}m¬≤ - {df_clean['WOHNFLAECHE'].max():.1f}m¬≤\")\n",
    "print(f\"Entfernte unrealistische Gr√∂√üen: {removed_sizes}\")\n",
    "\n",
    "# === PLZ-ZU-BEZIRK-ZUORDNUNG ===\n",
    "print(\"\\n=== PLZ-ZU-BEZIRK-ZUORDNUNG ===\")\n",
    "print(f\"PLZ - Typ: {df_clean['PLZ'].dtype}\")\n",
    "print(f\"Nicht-null PLZ: {df_clean['PLZ'].notna().sum()}\")\n",
    "\n",
    "# WICHTIG: Erweitere das PLZ-Mapping BEVOR die Zuordnung gemacht wird\n",
    "print(\"\\n=== ERWEITERUNG DES PLZ-MAPPINGS ===\")\n",
    "# Erweitere das PLZ-Dictionary um fehlende PLZ-Codes\n",
    "additional_plz_mapping = {\n",
    "    12627: 'Marzahn-Hellersdorf',\n",
    "    12629: 'Marzahn-Hellersdorf',\n",
    "    13593: 'Spandau',\n",
    "    13595: 'Spandau',\n",
    "    13597: 'Spandau',\n",
    "    13599: 'Spandau',\n",
    "    14052: 'Charlottenburg-Wilmersdorf',\n",
    "    14055: 'Charlottenburg-Wilmersdorf',\n",
    "    14057: 'Charlottenburg-Wilmersdorf',\n",
    "    14059: 'Charlottenburg-Wilmersdorf',\n",
    "    10315: 'Lichtenberg',\n",
    "    10317: 'Lichtenberg',\n",
    "    10318: 'Lichtenberg',\n",
    "    10319: 'Lichtenberg',\n",
    "    10365: 'Lichtenberg',\n",
    "    10367: 'Lichtenberg',\n",
    "    10369: 'Lichtenberg',\n",
    "    13125: 'Pankow',\n",
    "    13127: 'Pankow',\n",
    "    13129: 'Pankow',\n",
    "    13156: 'Pankow',\n",
    "    13158: 'Pankow',\n",
    "    13159: 'Pankow',\n",
    "    13187: 'Pankow',\n",
    "    13189: 'Pankow',\n",
    "    12305: 'Tempelhof-Sch√∂neberg',\n",
    "    12307: 'Tempelhof-Sch√∂neberg',\n",
    "    12309: 'Tempelhof-Sch√∂neberg',\n",
    "    12347: 'Neuk√∂lln',\n",
    "    12349: 'Neuk√∂lln',\n",
    "    12351: 'Neuk√∂lln',\n",
    "    12353: 'Neuk√∂lln',\n",
    "    12355: 'Neuk√∂lln',\n",
    "    12357: 'Neuk√∂lln',\n",
    "    12359: 'Neuk√∂lln',\n",
    "    12681: 'Marzahn-Hellersdorf',\n",
    "    12683: 'Marzahn-Hellersdorf',\n",
    "    12685: 'Marzahn-Hellersdorf',\n",
    "    12687: 'Marzahn-Hellersdorf',\n",
    "    12689: 'Marzahn-Hellersdorf',\n",
    "    12679: 'Marzahn-Hellersdorf',\n",
    "    13051: 'Pankow',\n",
    "    13053: 'Pankow',\n",
    "    13055: 'Pankow',\n",
    "    13057: 'Pankow',\n",
    "    13059: 'Pankow',\n",
    "    13086: 'Pankow',\n",
    "    13088: 'Pankow',\n",
    "    13089: 'Pankow',\n",
    "    13403: 'Reinickendorf',\n",
    "    13405: 'Reinickendorf',\n",
    "    13407: 'Reinickendorf',\n",
    "    13409: 'Reinickendorf',\n",
    "    13435: 'Reinickendorf',\n",
    "    13437: 'Reinickendorf',\n",
    "    13439: 'Reinickendorf',\n",
    "    13465: 'Reinickendorf',\n",
    "    13467: 'Reinickendorf',\n",
    "    13469: 'Reinickendorf',\n",
    "    13503: 'Reinickendorf',\n",
    "    13505: 'Reinickendorf',\n",
    "    13507: 'Reinickendorf',\n",
    "    13509: 'Reinickendorf',\n",
    "    13581: 'Spandau',\n",
    "    13583: 'Spandau',\n",
    "    13585: 'Spandau',\n",
    "    13587: 'Spandau',\n",
    "    13589: 'Spandau',\n",
    "    13591: 'Spandau',\n",
    "    14195: 'Steglitz-Zehlendorf',\n",
    "    14197: 'Steglitz-Zehlendorf',\n",
    "    14199: 'Steglitz-Zehlendorf',\n",
    "    14163: 'Steglitz-Zehlendorf',\n",
    "    14165: 'Steglitz-Zehlendorf',\n",
    "    14167: 'Steglitz-Zehlendorf',\n",
    "    14169: 'Steglitz-Zehlendorf',\n",
    "    14129: 'Steglitz-Zehlendorf',\n",
    "    14109: 'Steglitz-Zehlendorf',\n",
    "}\n",
    "\n",
    "# Erweitere das urspr√ºngliche PLZ-Dictionary\n",
    "plz_to_district.update(additional_plz_mapping)\n",
    "print(f\"PLZ-Mapping erweitert: {len(plz_to_district)} Zuordnungen\")\n",
    "\n",
    "# Jetzt PLZ zu Bezirk zuordnen\n",
    "df_clean['district'] = df_clean['PLZ'].map(plz_to_district)\n",
    "successful_mappings = df_clean['district'].notna().sum()\n",
    "print(f\"Erfolgreiche PLZ-zu-Bezirk-Zuordnungen: {successful_mappings}/{len(df_clean)} ({100*successful_mappings/len(df_clean):.1f}%)\")\n",
    "\n",
    "# Zeige nicht zugeordnete PLZ\n",
    "unmapped_plz = df_clean[df_clean['district'].isna()]['PLZ'].value_counts().head(10)\n",
    "if len(unmapped_plz) > 0:\n",
    "    unique_unmapped = df_clean[df_clean['district'].isna()]['PLZ'].nunique()\n",
    "    print(f\"\\nVerbleibende nicht zugeordnete PLZ ({unique_unmapped} einzigartige):\")\n",
    "    for plz, count in unmapped_plz.items():\n",
    "        print(f\"  {plz}: {count} Eintr√§ge\")\n",
    "\n",
    "# Nur Zeilen mit g√ºltigen Bezirken behalten\n",
    "initial_count = len(df_clean)\n",
    "df_clean = df_clean[df_clean['district'].notna()]\n",
    "removed_no_district = initial_count - len(df_clean)\n",
    "print(f\"Entfernte Eintr√§ge ohne Bezirk: {removed_no_district}\")\n",
    "\n",
    "# === ZIMMER-BEREINIGUNG (ZIMMER) ===\n",
    "print(\"\\n=== ZIMMER-BEREINIGUNG (ZIMMER) ===\")\n",
    "print(f\"ZIMMER - Typ: {df_clean['ZIMMER'].dtype}\")\n",
    "print(f\"Nicht-null Werte: {df_clean['ZIMMER'].notna().sum()}\")\n",
    "\n",
    "# Unrealistische Zimmerzahlen entfernen\n",
    "if len(df_clean) > 0:\n",
    "    initial_count = len(df_clean)\n",
    "    df_clean = df_clean[(df_clean['ZIMMER'] >= 1) & (df_clean['ZIMMER'] <= 10)]\n",
    "    removed_rooms = initial_count - len(df_clean)\n",
    "    print(f\"G√ºltige Zimmerzahlen nach Bereinigung: {len(df_clean)}/{initial_count} ({100*len(df_clean)/initial_count:.1f}%)\")\n",
    "    if len(df_clean) > 0:\n",
    "        print(f\"Zimmerspanne: {df_clean['ZIMMER'].min():.1f} - {df_clean['ZIMMER'].max():.1f}\")\n",
    "    print(f\"Entfernte unrealistische Zimmerzahlen: {removed_rooms}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Spezifische Bereinigung abgeschlossen\")\n",
    "print(f\"Verbleibende Datens√§tze: {len(df_clean)} (Verlust: {len(df_raw) - len(df_clean)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7bbc0a",
   "metadata": {},
   "source": [
    "## 5. Normalisierung in Standardformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d678813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NORMALISIERUNG IN STANDARDFORMAT\n",
      "============================================================\n",
      "Normalisiertes Dataset erstellt: 2676 Zeilen\n",
      "Standardspalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source']\n",
      "Zus√§tzliche Spalten: 16\n",
      "\n",
      "=== DATENQUALIT√ÑT NORMALISIERTES DATASET ===\n",
      "Zeilen mit Preis: 2676\n",
      "Zeilen mit Gr√∂√üe: 2676\n",
      "Zeilen mit Bezirk: 2676\n",
      "Zeilen mit Zimmeranzahl: 2676\n",
      "\n",
      "=== STATISTIKEN ===\n",
      "Preis - Min: 180.00‚Ç¨, Max: 3000.00‚Ç¨, Median: 790.60‚Ç¨\n",
      "Gr√∂√üe - Min: 13.0m¬≤, Max: 230.0m¬≤, Median: 65.1m¬≤\n",
      "Zimmer - Min: 1.0, Max: 5.0, Median: 2.0\n",
      "\n",
      "=== BEZIRKSVERTEILUNG ===\n",
      "Anzahl Bezirke: 21\n",
      "  Spandau: 329 Eintr√§ge\n",
      "  Pankow: 256 Eintr√§ge\n",
      "  Reinickendorf: 247 Eintr√§ge\n",
      "  Treptow-K√∂penick: 243 Eintr√§ge\n",
      "  Marzahn-Hellersdorf: 237 Eintr√§ge\n",
      "  Lichtenberg: 151 Eintr√§ge\n",
      "  Neuk√∂lln: 147 Eintr√§ge\n",
      "  Friedrichshain: 113 Eintr√§ge\n",
      "  Sch√∂neberg: 107 Eintr√§ge\n",
      "  Mitte: 99 Eintr√§ge\n",
      "\n",
      "‚úÖ Normalisierung abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "# Normalisierung in Standardformat\n",
    "print(\"=\" * 60)\n",
    "print(\"NORMALISIERUNG IN STANDARDFORMAT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Erstelle normalisiertes Dataset mit Standardspalten\n",
    "df_normalized = pd.DataFrame()\n",
    "\n",
    "if len(df_clean) > 0:\n",
    "    # Standardspalten zuweisen\n",
    "    df_normalized['price'] = df_clean['KALTMIETE'].astype('float64')\n",
    "    df_normalized['size'] = df_clean['WOHNFLAECHE'].astype('float64')\n",
    "    df_normalized['district'] = df_clean['district'].astype('string')\n",
    "    df_normalized['rooms'] = df_clean['ZIMMER'].astype('float64')\n",
    "    df_normalized['year'] = 2022\n",
    "    df_normalized['dataset_id'] = 'current'\n",
    "    df_normalized['source'] = 'Springer/Immowelt/Immonet'\n",
    "\n",
    "    # Zus√§tzliche wichtige Spalten aus Original-Dataset beibehalten\n",
    "    df_normalized['plz'] = df_clean['PLZ'].astype('string')\n",
    "    if 'WARMMIETE' in df_clean.columns:\n",
    "        df_normalized['warmmiete'] = df_clean['WARMMIETE']\n",
    "    if 'NEBENKOSTEN' in df_clean.columns:\n",
    "        df_normalized['nebenkosten'] = df_clean['NEBENKOSTEN']\n",
    "    if 'KAUTION' in df_clean.columns:\n",
    "        df_normalized['kaution'] = df_clean['KAUTION']\n",
    "    if 'BAUJAHR' in df_clean.columns:\n",
    "        df_normalized['baujahr'] = df_clean['BAUJAHR']\n",
    "    if 'ZUSTAND' in df_clean.columns:\n",
    "        df_normalized['zustand'] = df_clean['ZUSTAND']\n",
    "    if 'ENERGIEEFFIZIENSKLASSE' in df_clean.columns:\n",
    "        df_normalized['energieeffiziensklasse'] = df_clean['ENERGIEEFFIZIENSKLASSE']\n",
    "\n",
    "    # Ausstattungsmerkmale (boolean)\n",
    "    ausstattung_cols = ['m√∂bliert', 'Balkon', 'Terrasse', 'Garten', 'Einbauk√ºche', \n",
    "                       'Garage', 'Stellplatz', 'Personenaufzug', 'Keller']\n",
    "    for col in ausstattung_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_normalized[f'ausstattung_{col.lower()}'] = df_clean[col]\n",
    "\n",
    "print(f\"Normalisiertes Dataset erstellt: {len(df_normalized)} Zeilen\")\n",
    "print(f\"Standardspalten: {['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source']}\")\n",
    "print(f\"Zus√§tzliche Spalten: {len(df_normalized.columns) - 7}\")\n",
    "\n",
    "# Datenqualit√§t pr√ºfen\n",
    "print(f\"\\n=== DATENQUALIT√ÑT NORMALISIERTES DATASET ===\")\n",
    "print(f\"Zeilen mit Preis: {df_normalized['price'].notna().sum()}\")\n",
    "print(f\"Zeilen mit Gr√∂√üe: {df_normalized['size'].notna().sum()}\")\n",
    "print(f\"Zeilen mit Bezirk: {df_normalized['district'].notna().sum()}\")\n",
    "print(f\"Zeilen mit Zimmeranzahl: {df_normalized['rooms'].notna().sum()}\")\n",
    "\n",
    "# Statistiken\n",
    "if len(df_normalized) > 0:\n",
    "    print(f\"\\n=== STATISTIKEN ===\")\n",
    "    print(f\"Preis - Min: {df_normalized['price'].min():.2f}‚Ç¨, Max: {df_normalized['price'].max():.2f}‚Ç¨, Median: {df_normalized['price'].median():.2f}‚Ç¨\")\n",
    "    print(f\"Gr√∂√üe - Min: {df_normalized['size'].min():.1f}m¬≤, Max: {df_normalized['size'].max():.1f}m¬≤, Median: {df_normalized['size'].median():.1f}m¬≤\")\n",
    "    print(f\"Zimmer - Min: {df_normalized['rooms'].min():.1f}, Max: {df_normalized['rooms'].max():.1f}, Median: {df_normalized['rooms'].median():.1f}\")\n",
    "\n",
    "    # Bezirksverteilung\n",
    "    print(f\"\\n=== BEZIRKSVERTEILUNG ===\")\n",
    "    district_counts = df_normalized['district'].value_counts()\n",
    "    print(f\"Anzahl Bezirke: {len(district_counts)}\")\n",
    "    for district, count in district_counts.head(10).items():\n",
    "        print(f\"  {district}: {count} Eintr√§ge\")\n",
    "\n",
    "print(f\"\\n‚úÖ Normalisierung abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ddabaf",
   "metadata": {},
   "source": [
    "## 6. Export des normalisierten Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be53cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT NORMALISIERTES DATASET\n",
      "============================================================\n",
      "‚úÖ Normalisiertes Dataset exportiert: data/processed/dataset_2022_normalized.csv\n",
      "Dateigr√∂√üe: 2676 Zeilen x 23 Spalten\n",
      "‚úÖ Export-Validierung erfolgreich: 2676 Zeilen geladen\n",
      "\n",
      "=== ZUSAMMENFASSUNG DATASET 2022 ===\n",
      "Input: data/raw/Dataset_2022.csv (2950 Zeilen)\n",
      "Output: data/processed/dataset_2022_normalized.csv (2676 Zeilen)\n",
      "Datenverlust: 274 Zeilen (9.3%)\n",
      "PLZ-zu-Bezirk-Mapping: 2676/2676 (100.0%) erfolgreich\n",
      "Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\n",
      "Zus√§tzliche Spalten: 16\n",
      "\n",
      "üéØ DATASET 2022 BEREINIGUNG ABGESCHLOSSEN!\n",
      "Bereit f√ºr Kombination mit anderen normalisierten Datasets.\n"
     ]
    }
   ],
   "source": [
    "# Export des normalisierten Datasets\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPORT NORMALISIERTES DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ausgabedatei\n",
    "output_file = 'data/processed/dataset_2022_normalized.csv'\n",
    "\n",
    "# Export\n",
    "df_normalized.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Normalisiertes Dataset exportiert: {output_file}\")\n",
    "print(f\"Dateigr√∂√üe: {len(df_normalized)} Zeilen x {len(df_normalized.columns)} Spalten\")\n",
    "\n",
    "# Validierung des Exports\n",
    "test_load = pd.read_csv(output_file)\n",
    "print(f\"‚úÖ Export-Validierung erfolgreich: {len(test_load)} Zeilen geladen\")\n",
    "\n",
    "# Zusammenfassung\n",
    "print(f\"\\n=== ZUSAMMENFASSUNG DATASET 2022 ===\")\n",
    "print(f\"Input: data/raw/Dataset_2022.csv ({len(df_raw)} Zeilen)\")\n",
    "print(f\"Output: {output_file} ({len(df_normalized)} Zeilen)\")\n",
    "print(f\"Datenverlust: {len(df_raw) - len(df_normalized)} Zeilen ({((len(df_raw) - len(df_normalized))/len(df_raw)*100):.1f}%)\")\n",
    "print(f\"PLZ-zu-Bezirk-Mapping: {df_normalized['district'].notna().sum()}/{len(df_normalized)} ({df_normalized['district'].notna().sum()/len(df_normalized)*100:.1f}%) erfolgreich\")\n",
    "print(f\"Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\")\n",
    "print(f\"Zus√§tzliche Spalten: {len(df_normalized.columns) - 7}\")\n",
    "\n",
    "print(f\"\\nüéØ DATASET 2022 BEREINIGUNG ABGESCHLOSSEN!\")\n",
    "print(f\"Bereit f√ºr Kombination mit anderen normalisierten Datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d5",
   "metadata": {},
   "source": [
    "## 7. Lade angereicherte Wohnlagendaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f6g7h9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANGEREICHERTE WOHNLAGENDATEN LADEN\n",
      "============================================================\n",
      "‚úÖ Angereicherte Daten geladen: 551,249 Zeilen, 11 Spalten\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ANGEREICHERTE WOHNLAGENDATEN LADEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "enriched_data_path = 'data/raw/wohnlagen_enriched.csv'\n",
    "try:\n",
    "    enriched_df = pd.read_csv(enriched_data_path)\n",
    "    print(f\"‚úÖ Angereicherte Daten geladen: {len(enriched_df):,} Zeilen, {len(enriched_df.columns)} Spalten\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Datei nicht gefunden: {enriched_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l3",
   "metadata": {},
   "source": [
    "## 8. Kombiniere Datasets mit Wohnlagendaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "m3n4o5p7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KOMBINIERE MIT WOHNLAGENDATEN\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "You are trying to merge on string and int64 columns for key 'plz'. If you wish to proceed you should use pd.concat",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Merge the two dataframes\u001b[39;00m\n\u001b[32m      6\u001b[39m enriched_df_subset = enriched_df[[\u001b[33m'\u001b[39m\u001b[33mstrasse\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mplz\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mwol\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mortsteil_neu\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m df_enriched = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menriched_df_subset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mleft\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mplz\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ Kombiniertes und angereichertes Dataset erstellt: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_enriched)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Zeilen\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crack.crackdesk\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:170\u001b[39m, in \u001b[36mmerge\u001b[39m\u001b[34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[39m\n\u001b[32m    155\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _cross_merge(\n\u001b[32m    156\u001b[39m         left_df,\n\u001b[32m    157\u001b[39m         right_df,\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         copy=copy,\n\u001b[32m    168\u001b[39m     )\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     op = \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[43m=\u001b[49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[43m=\u001b[49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    180\u001b[39m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m=\u001b[49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    182\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    184\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m op.get_result(copy=copy)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crack.crackdesk\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:807\u001b[39m, in \u001b[36m_MergeOperation.__init__\u001b[39m\u001b[34m(self, left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, indicator, validate)\u001b[39m\n\u001b[32m    803\u001b[39m \u001b[38;5;28mself\u001b[39m._validate_tolerance(\u001b[38;5;28mself\u001b[39m.left_join_keys)\n\u001b[32m    805\u001b[39m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[32m    806\u001b[39m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m807\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_maybe_coerce_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[38;5;66;03m# If argument passed to validate,\u001b[39;00m\n\u001b[32m    810\u001b[39m \u001b[38;5;66;03m# check if columns specified as unique\u001b[39;00m\n\u001b[32m    811\u001b[39m \u001b[38;5;66;03m# are in fact unique.\u001b[39;00m\n\u001b[32m    812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m validate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\crack.crackdesk\\.pyenv\\pyenv-win\\versions\\3.11.9\\Lib\\site-packages\\pandas\\core\\reshape\\merge.py:1508\u001b[39m, in \u001b[36m_MergeOperation._maybe_coerce_merge_keys\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1502\u001b[39m     \u001b[38;5;66;03m# unless we are merging non-string-like with string-like\u001b[39;00m\n\u001b[32m   1503\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m (\n\u001b[32m   1504\u001b[39m         inferred_left \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_right \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[32m   1505\u001b[39m     ) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1506\u001b[39m         inferred_right \u001b[38;5;129;01min\u001b[39;00m string_types \u001b[38;5;129;01mand\u001b[39;00m inferred_left \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m string_types\n\u001b[32m   1507\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1508\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[32m   1510\u001b[39m \u001b[38;5;66;03m# datetimelikes must match exactly\u001b[39;00m\n\u001b[32m   1511\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m needs_i8_conversion(lk.dtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m needs_i8_conversion(rk.dtype):\n",
      "\u001b[31mValueError\u001b[39m: You are trying to merge on string and int64 columns for key 'plz'. If you wish to proceed you should use pd.concat"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"KOMBINIERE MIT WOHNLAGENDATEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Merge the two dataframes\n",
    "enriched_df_subset = enriched_df[['strasse', 'plz', 'wol', 'ortsteil_neu']]\n",
    "df_enriched = pd.merge(df_normalized, enriched_df_subset, how='left', on=['plz'])\n",
    "\n",
    "print(f\"‚úÖ Kombiniertes und angereichertes Dataset erstellt: {len(df_enriched):,} Zeilen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t1",
   "metadata": {},
   "source": [
    "## 9. Export des finalen angereicherten Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "u1v2w3x5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPORT FINALES ANGEREICHERTES DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Export\n",
    "output_file_enriched = 'data/processed/dataset_2022_enriched.csv'\n",
    "df_enriched.to_csv(output_file_enriched, index=False)\n",
    "\n",
    "print(f\"‚úÖ Finales angereichertes Dataset exportiert: {output_file_enriched}\")\n",
    "print(f\"Dateigr√∂√üe: {len(df_enriched):,} Zeilen x {len(df_enriched.columns)} Spalten\")\n",
    "\n",
    "# Validierung durch Wiedereinlesen\n",
    "test_df_enriched = pd.read_csv(output_file_enriched)\n",
    "print(f\"‚úÖ Export-Validierung erfolgreich: {len(test_df_enriched):,} Zeilen geladen\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
