{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c57dfb6",
   "metadata": {},
   "source": [
    "## üìö **Methodologie und Technische Dokumentation**\n",
    "\n",
    "### üéØ **Zentrale Herausforderung: Heterogene Datenstrukturen**\n",
    "\n",
    "Das Dataset 2025 unterscheidet sich fundamental von den Datens√§tzen 2018-2019 und 2022:\n",
    "\n",
    "| **Merkmal** | **Dataset 2018-2019** | **Dataset 2022** | **Dataset 2025** |\n",
    "|-------------|----------------------|-------------------|-------------------|\n",
    "| **PLZ-Verf√ºgbarkeit** | 100% | 100% | ~1-2% |\n",
    "| **Adressformat** | Strukturiert | Strukturiert | Heterogen |\n",
    "| **Bezirksangaben** | Implizit √ºber PLZ | Implizit √ºber PLZ | Explizit als Text |\n",
    "| **Multi-Listings** | Keine | Keine | Ja (Preis-/Gr√∂√üenspannen) |\n",
    "\n",
    "### üî¨ **Entwickelte L√∂sungsans√§tze:**\n",
    "\n",
    "#### **1. Intelligente Adressextraktion**\n",
    "- **Regex-basierte PLZ-Extraktion:** `\\b(\\d{5})\\b`\n",
    "- **Mehrstufige Bezirks-Erkennung:** Alias-Mapping f√ºr Berlin-spezifische Varianten\n",
    "- **Fallback-Mechanismen:** Strukturierte Priorit√§tenliste f√ºr Adresskomponenten\n",
    "\n",
    "#### **2. Multi-Listing-Behandlung**\n",
    "- **Preisspannen:** `\"725 - 1.965‚Ç¨\"` ‚Üí Minimum-Ansatz f√ºr Vergleichbarkeit\n",
    "- **Gr√∂√üenspannen:** `\"26,55 - 112,82m¬≤\"` ‚Üí Konservative Sch√§tzung\n",
    "- **Rationale:** Vermeidung von √úbersch√§tzungen bei Zeitvergleichen\n",
    "\n",
    "#### **3. Dual-Strategie-Anreicherung**\n",
    "- **Strategie 1:** PLZ-basiert (h√∂chste Pr√§zision, wenige Datenpunkte)\n",
    "- **Strategie 2:** Bezirks-basiert (Fallback f√ºr 98% der Daten)\n",
    "- **Kombination:** Nahtlose Integration beider Ans√§tze\n",
    "\n",
    "### üìä **Qualit√§tssicherung:**\n",
    "\n",
    "- **Vermeidung kartesischer Produkte:** Durch `drop_duplicates(subset=['plz'])`\n",
    "- **Datenintegrit√§tspr√ºfung:** Vor- und Nach-Vergleiche aller Verarbeitungsschritte\n",
    "- **Standardisierte Filter:** Identisch mit anderen Datasets f√ºr Vergleichbarkeit\n",
    "\n",
    "### üéì **Wissenschaftliche Relevanz:**\n",
    "\n",
    "Diese Methodologie demonstriert den Umgang mit **heterogenen Datenquellen** in der Immobiliendatenanalyse - ein h√§ufiges Problem bei longitudinalen Studien, wo sich Datenstrukturen √ºber Zeit √§ndern.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28af644",
   "metadata": {},
   "source": [
    "# 03_Clean_Dataset_2025 - Intelligente Adressextraktion\n",
    "\n",
    "## üéØ **Spezifische Bereinigung f√ºr Dataset 2025**\n",
    "\n",
    "### **Hauptfunktionen:**\n",
    "- **Intelligente PLZ-Extraktion** aus verschiedenen Adressformaten\n",
    "- **Bezirk-Normalisierung** mit Alias-Mapping\n",
    "- **Multi-Listing-Behandlung** (Preis- und Gr√∂√üenspannen)\n",
    "- **Filter-Harmonisierung** mit anderen Datasets\n",
    "- **Standardisierte Ausgabe** kompatibel mit anderen Datasets\n",
    "\n",
    "### **üîÑ Filter-Harmonisierung (Konsistent mit allen Datasets):**\n",
    "- **Preis-Filter:** 100‚Ç¨ - 10.000‚Ç¨ (Kaltmiete)\n",
    "- **Gr√∂√üen-Filter:** 10m¬≤ - 500m¬≤ (Wohnfl√§che)\n",
    "- **Bezirk-Validierung:** Nur g√ºltige Berliner Bezirke\n",
    "\n",
    "### **üìã Adressformate im 2025 Dataset:**\n",
    "1. **Vollst√§ndige Adresse mit PLZ:** \"Johannisplatz 3, 10117 Berlin\"\n",
    "2. **Adresse mit Bezirk:** \"Johannisplatz 5, Mitte (Ortsteil), Berlin\" \n",
    "3. **Nur Bezirk:** \"Tiergarten, Berlin\"\n",
    "4. **Nur PLZ:** \"10557 Berlin\"\n",
    "5. **Adresse mit Ortsteil:** \"Friedrichshain, Berlin\"\n",
    "\n",
    "### **üéØ Ziel:** \n",
    "Einheitliche Bezirk-Zuordnung und maximale Vergleichbarkeit mit Dataset 2018-2019 und Dataset 2022\n",
    "\n",
    "---\n",
    "**Teil der modularen Preprocessing-Pipeline**  \n",
    "**Datum:** 4. Juli 2025  \n",
    "**Version:** 1.1 (Filter-Harmonisierung)  \n",
    "**Status:** ‚úÖ Harmonisiert mit allen anderen Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b3a266",
   "metadata": {},
   "source": [
    "## 1. Setup und Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a456e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotheken erfolgreich importiert!\n",
      "Pandas Version: 2.2.3\n",
      "Dataset: 2025 (ImmobilienScout24)\n",
      "Ziel: Intelligente Adressextraktion und Bezirk-Normalisierung\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Bibliotheken erfolgreich importiert!\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(\"Dataset: 2025 (ImmobilienScout24)\")\n",
    "print(\"Ziel: Intelligente Adressextraktion und Bezirk-Normalisierung\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a068c88",
   "metadata": {},
   "source": [
    "## 2. PLZ-Mapping und Bezirk-Normalisierung laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9037091a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PLZ-MAPPING UND BEZIRK-NORMALISIERUNG\n",
      "============================================================\n",
      "============================================================\n",
      "PLZ-ZU-BEZIRK-MAPPING LADEN\n",
      "============================================================\n",
      "‚úÖ PLZ-Mapping geladen: 181 Eintr√§ge\n",
      "‚úÖ PLZ-Dictionary erstellt: 181 Zuordnungen\n",
      "‚úÖ PLZ-Mapping geladen: 185 Zuordnungen\n",
      "‚úÖ Bezirk-Aliases definiert: 36 Zuordnungen\n",
      "\n",
      "PLZ-Mapping Beispiele:\n",
      "  10115 ‚Üí Mitte\n",
      "  10117 ‚Üí Mitte\n",
      "  10119 ‚Üí Mitte\n",
      "  10178 ‚Üí Mitte\n",
      "  10179 ‚Üí Mitte\n",
      "\n",
      "Bezirk-Normalisierung Beispiele:\n",
      "  'Mitte (Ortsteil)' ‚Üí 'Mitte'\n",
      "  'Pankow (Ortsteil)' ‚Üí 'Pankow'\n",
      "  'Spandau (Ortsteil)' ‚Üí 'Spandau'\n",
      "  'Neuk√∂lln (Ortsteil)' ‚Üí 'Neuk√∂lln'\n",
      "  'Friedrichshain' ‚Üí 'Friedrichshain-Kreuzberg'\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PLZ-MAPPING UND BEZIRK-NORMALISIERUNG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PLZ-zu-Bezirk-Mapping laden\n",
    "print(\"=\" * 60)\n",
    "print(\"PLZ-ZU-BEZIRK-MAPPING LADEN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    plz_mapping_df = pd.read_csv('data/processed/berlin_plz_mapping.csv')\n",
    "    print(f\"‚úÖ PLZ-Mapping geladen: {len(plz_mapping_df)} Eintr√§ge\")\n",
    "    \n",
    "    # Erstelle Dictionary f√ºr schnelles Lookup\n",
    "    plz_to_district = dict(zip(plz_mapping_df['PLZ'], plz_mapping_df['Bezirk']))\n",
    "    print(f\"‚úÖ PLZ-Dictionary erstellt: {len(plz_to_district)} Zuordnungen\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå FEHLER: PLZ-Mapping nicht gefunden!\")\n",
    "    print(\"Bitte stellen Sie sicher, dass 'data/processed/berlin_plz_mapping.csv' existiert.\")\n",
    "    raise\n",
    "\n",
    "# Erweiterte PLZ-Zuordnungen f√ºr Dataset 2025\n",
    "extended_plz_mapping = {\n",
    "    10115: 'Mitte',\n",
    "    10117: 'Mitte',\n",
    "    10119: 'Mitte',\n",
    "    10178: 'Mitte',\n",
    "    10179: 'Mitte',\n",
    "    10243: 'Friedrichshain-Kreuzberg',\n",
    "    10245: 'Friedrichshain-Kreuzberg',\n",
    "    10247: 'Friedrichshain-Kreuzberg',\n",
    "    10249: 'Friedrichshain-Kreuzberg',\n",
    "    10315: 'Lichtenberg',\n",
    "    10317: 'Lichtenberg',\n",
    "    10318: 'Lichtenberg',\n",
    "    10319: 'Lichtenberg',\n",
    "    10365: 'Lichtenberg',\n",
    "    10367: 'Lichtenberg',\n",
    "    10369: 'Lichtenberg',\n",
    "    12305: 'Tempelhof-Sch√∂neberg',\n",
    "    12307: 'Tempelhof-Sch√∂neberg',\n",
    "    12309: 'Tempelhof-Sch√∂neberg',\n",
    "    12347: 'Neuk√∂lln',\n",
    "    12349: 'Neuk√∂lln',\n",
    "    12351: 'Neuk√∂lln',\n",
    "    12353: 'Neuk√∂lln',\n",
    "    12355: 'Neuk√∂lln',\n",
    "    12357: 'Neuk√∂lln',\n",
    "    12359: 'Neuk√∂lln',\n",
    "    12524: 'Treptow-K√∂penick',\n",
    "    12555: 'Treptow-K√∂penick',\n",
    "    10247: 'Friedrichshain-Kreuzberg',\n",
    "    10249: 'Friedrichshain-Kreuzberg',\n",
    "    10367: 'Lichtenberg',\n",
    "    10369: 'Lichtenberg',\n",
    "    14612: 'Falkensee',  # Au√üerhalb Berlin\n",
    "    13507: 'Reinickendorf',\n",
    "    10585: 'Charlottenburg-Wilmersdorf',\n",
    "    10709: 'Charlottenburg-Wilmersdorf',\n",
    "    10559: 'Mitte',\n",
    "}\n",
    "\n",
    "# Erweitere PLZ-Mapping\n",
    "plz_to_district.update(extended_plz_mapping)\n",
    "\n",
    "# Bezirk-Normalisierung (verschiedene Schreibweisen auf einheitliche Namen mappen)\n",
    "district_aliases = {\n",
    "    'Mitte (Ortsteil)': 'Mitte',\n",
    "    'Pankow (Ortsteil)': 'Pankow',\n",
    "    'Spandau (Ortsteil)': 'Spandau',\n",
    "    'Neuk√∂lln (Ortsteil)': 'Neuk√∂lln',\n",
    "    'Friedrichshain': 'Friedrichshain-Kreuzberg',\n",
    "    'Kreuzberg': 'Friedrichshain-Kreuzberg',\n",
    "    'Charlottenburg': 'Charlottenburg-Wilmersdorf',\n",
    "    'Wilmersdorf': 'Charlottenburg-Wilmersdorf',\n",
    "    'Tempelhof': 'Tempelhof-Sch√∂neberg',\n",
    "    'Sch√∂neberg': 'Tempelhof-Sch√∂neberg',\n",
    "    'Prenzlauer Berg': 'Pankow',\n",
    "    'Wei√üensee': 'Pankow',\n",
    "    'Buch': 'Pankow',\n",
    "    'Niedersch√∂nhausen': 'Pankow',\n",
    "    'Gesundbrunnen': 'Mitte',\n",
    "    'Wedding': 'Mitte',\n",
    "    'Moabit': 'Mitte',\n",
    "    'Tiergarten': 'Mitte',\n",
    "    'Friedenau': 'Tempelhof-Sch√∂neberg',\n",
    "    'Steglitz': 'Steglitz-Zehlendorf',\n",
    "    'Zehlendorf': 'Steglitz-Zehlendorf',\n",
    "    'Schmargendorf': 'Charlottenburg-Wilmersdorf',\n",
    "    'Grunewald': 'Charlottenburg-Wilmersdorf',\n",
    "    'Halensee': 'Charlottenburg-Wilmersdorf',\n",
    "    'Tegel': 'Reinickendorf',\n",
    "    'Heiligensee': 'Reinickendorf',\n",
    "    'Staaken': 'Spandau',\n",
    "    'Siemensstadt': 'Spandau',\n",
    "    'Malchow': 'Pankow',\n",
    "    'Reinickendorf': 'Reinickendorf',\n",
    "    'Lichtenberg': 'Lichtenberg',\n",
    "    'Marzahn-Hellersdorf': 'Marzahn-Hellersdorf',\n",
    "    'Spandau': 'Spandau',\n",
    "    'Neuk√∂lln': 'Neuk√∂lln',\n",
    "    'Mitte': 'Mitte',\n",
    "    'Pankow': 'Pankow',\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ PLZ-Mapping geladen: {len(plz_to_district)} Zuordnungen\")\n",
    "print(f\"‚úÖ Bezirk-Aliases definiert: {len(district_aliases)} Zuordnungen\")\n",
    "\n",
    "# Zeige Beispiele\n",
    "print(\"\\nPLZ-Mapping Beispiele:\")\n",
    "for plz, district in list(plz_to_district.items())[:5]:\n",
    "    print(f\"  {plz} ‚Üí {district}\")\n",
    "    \n",
    "print(\"\\nBezirk-Normalisierung Beispiele:\")\n",
    "for alias, normalized in list(district_aliases.items())[:5]:\n",
    "    print(f\"  '{alias}' ‚Üí '{normalized}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e94ac7",
   "metadata": {},
   "source": [
    "## 3. Dataset 2025 laden und analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed89cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET 2025 LADEN UND ANALYSIEREN\n",
      "============================================================\n",
      "Dataset geladen: 6,109 Zeilen, 5 Spalten\n",
      "Spalten: ['title', 'price', 'size', 'address', 'link']\n",
      "Datentypen:\n",
      "title      object\n",
      "price      object\n",
      "size       object\n",
      "address    object\n",
      "link       object\n",
      "dtype: object\n",
      "Fehlende Werte:\n",
      "=== ADRESSFORMAT-ANALYSE ===\n",
      "Erste 10 Adressen:\n",
      "  1. Biedenkopfer Stra√üe 46-54, 13507 Berlin\n",
      "  2. Seegefelder Stra√üe 150, 14612 Falkensee\n",
      "  3. Johannisplatz 5, Mitte (Ortsteil), Berlin\n",
      "  4. Pufendorfstra√üe 3A-3E, Friedrichshain, Berlin\n",
      "  5. Warburgzeile 1, 10585 Berlin\n",
      "  6. Johannisplatz 3, 10117 Berlin\n",
      "  7. Kreutzigerstra√üe 14, Friedrichshain, Berlin\n",
      "  8. Elsa-Neumann-Stra√üe 1, 13629 Berlin\n",
      "  9. Tiergarten, Berlin\n",
      "  10. Chausseestra√üe 108, Mitte (Ortsteil), Berlin\n",
      "Einzigartige Adressformate (Sample):\n",
      "  ‚Ä¢ Biedenkopfer Stra√üe 46-54, 13507 Berlin\n",
      "  ‚Ä¢ Seegefelder Stra√üe 150, 14612 Falkensee\n",
      "  ‚Ä¢ Johannisplatz 5, Mitte (Ortsteil), Berlin\n",
      "  ‚Ä¢ Pufendorfstra√üe 3A-3E, Friedrichshain, Berlin\n",
      "  ‚Ä¢ Warburgzeile 1, 10585 Berlin\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATASET 2025 LADEN UND ANALYSIEREN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dataset laden\n",
    "df = pd.read_csv('data/raw/Dataset_2025.csv')\n",
    "print(f\"Dataset geladen: {len(df):,} Zeilen, {len(df.columns)} Spalten\")\n",
    "\n",
    "print(f\"Spalten: {list(df.columns)}\")\n",
    "\n",
    "print(f\"Datentypen:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"Fehlende Werte:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "for col, count in missing_values.items():\n",
    "    print(f\"  {col}: {count} ({100*count/len(df):.2f}%)\")\n",
    "\n",
    "# Erste Adressfelder analysieren\n",
    "print(f\"=== ADRESSFORMAT-ANALYSE ===\")\n",
    "print(\"Erste 10 Adressen:\")\n",
    "for i, addr in enumerate(df['address'].head(10)):\n",
    "    print(f\"  {i+1}. {addr}\")\n",
    "\n",
    "print(f\"Einzigartige Adressformate (Sample):\")\n",
    "unique_addresses = df['address'].dropna().unique()\n",
    "for addr in unique_addresses[:5]:\n",
    "    print(f\"  ‚Ä¢ {addr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5851ec",
   "metadata": {},
   "source": [
    "## 4. Intelligente Adressextraktion und Bezirk-Zuordnung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ffb666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INTELLIGENTE ADRESSEXTRAKTION\n",
      "============================================================\n",
      "Extrahiere Bezirke aus Adressen...\n",
      "‚úÖ Bezirk-Extraktion abgeschlossen:\n",
      "  Gesamt Adressen: 6,109\n",
      "  Erfolgreiche Extraktion: 4,440\n",
      "  Erfolgsrate: 72.7%\n",
      "=== ERFOLGREICHE EXTRAKTION (Beispiele) ===\n",
      "  'Biedenkopfer Stra√üe 46-54, 13507 Berlin' ‚Üí Reinickendorf\n",
      "  'Seegefelder Stra√üe 150, 14612 Falkensee' ‚Üí Falkensee\n",
      "  'Johannisplatz 5, Mitte (Ortsteil), Berlin' ‚Üí Mitte\n",
      "  'Pufendorfstra√üe 3A-3E, Friedrichshain, Berlin' ‚Üí Friedrichshain-Kreuzberg\n",
      "  'Warburgzeile 1, 10585 Berlin' ‚Üí Charlottenburg-Wilmersdorf\n",
      "  'Johannisplatz 3, 10117 Berlin' ‚Üí Mitte\n",
      "  'Kreutzigerstra√üe 14, Friedrichshain, Berlin' ‚Üí Friedrichshain-Kreuzberg\n",
      "  'Elsa-Neumann-Stra√üe 1, 13629 Berlin' ‚Üí Spandau\n",
      "  'Tiergarten, Berlin' ‚Üí Mitte\n",
      "  'Chausseestra√üe 108, Mitte (Ortsteil), Berlin' ‚Üí Mitte\n",
      "=== NICHT-EXTRAHIERTE ADRESSEN (1669 Eintr√§ge) ===\n",
      "  ‚ùå 'Abendseglersteig 55, Rahnsdorf, Berlin'\n",
      "  ‚ùå 'Gr√ºnauer Stra√üe 26, Altglienicke, Berlin'\n",
      "  ‚ùå 'Carl-Spindler-Stra√üe 19, K√∂penick, Berlin'\n",
      "  ‚ùå 'Am Maselakepark 31, Hakenfelde, Berlin'\n",
      "  ‚ùå 'Lion-Feuchtwanger Stra√üe 61, Hellersdorf, Berlin'\n",
      "  ‚ùå 'Lehderstra√üe 26-27, 13806 Berlin'\n",
      "  ‚ùå 'W√§scherinnenweg 2, K√∂penick, Berlin'\n",
      "  ‚ùå 'Eichbuschallee 55, Pl√§nterwald, Berlin'\n",
      "  ‚ùå 'Bouchestra√üe 37, Alt-Treptow, Berlin'\n",
      "  ‚ùå 'Gr√ºnauer Stra√üe 29, K√∂penick, Berlin'\n",
      "=== BEZIRK-VERTEILUNG ===\n",
      "Anzahl einzigartiger Bezirke: 21\n",
      "  Mitte: 1028 Eintr√§ge\n",
      "  Pankow: 784 Eintr√§ge\n",
      "  Friedrichshain-Kreuzberg: 778 Eintr√§ge\n",
      "  Charlottenburg-Wilmersdorf: 602 Eintr√§ge\n",
      "  Neuk√∂lln: 397 Eintr√§ge\n",
      "  Tempelhof-Sch√∂neberg: 355 Eintr√§ge\n",
      "  Steglitz-Zehlendorf: 170 Eintr√§ge\n",
      "  Reinickendorf: 129 Eintr√§ge\n",
      "  Spandau: 93 Eintr√§ge\n",
      "  Lichtenberg: 77 Eintr√§ge\n",
      "‚úÖ Verbleibende Eintr√§ge nach Bezirk-Extraktion: 4,440\n",
      "Datenverlust: 1,669 Eintr√§ge (27.3%)\n"
     ]
    }
   ],
   "source": [
    "def extract_district_from_address(address):\n",
    "    \"\"\"\n",
    "    Intelligente Bezirk-Extraktion aus verschiedenen Adressformaten\n",
    "    \n",
    "    Unterst√ºtzte Formate:\n",
    "    1. PLZ-basiert: \"Johannisplatz 3, 10117 Berlin\"\n",
    "    2. Bezirk direkt: \"Johannisplatz 5, Mitte (Ortsteil), Berlin\"\n",
    "    3. Nur Bezirk: \"Tiergarten, Berlin\"\n",
    "    4. Nur PLZ: \"10557 Berlin\"\n",
    "    \"\"\"\n",
    "    if pd.isna(address):\n",
    "        return None\n",
    "    \n",
    "    address = str(address).strip()\n",
    "    \n",
    "    # Methode 1: PLZ-Extraktion (5-stellige Zahlen)\n",
    "    plz_match = re.search(r'\\b(\\d{5})\\b', address)\n",
    "    if plz_match:\n",
    "        plz = int(plz_match.group(1))\n",
    "        if plz in plz_to_district:\n",
    "            return plz_to_district[plz]\n",
    "    \n",
    "    # Methode 2: Bezirk-Namen direkt im Text finden\n",
    "    # Entferne \"Berlin\" vom Ende und teile bei Komma\n",
    "    address_clean = address.replace(', Berlin', '').replace(' Berlin', '')\n",
    "    \n",
    "    # Suche nach bekannten Bezirken in der Adresse\n",
    "    for alias, normalized in district_aliases.items():\n",
    "        if alias.lower() in address_clean.lower():\n",
    "            return normalized\n",
    "    \n",
    "    # Methode 3: Letzte Komponente vor \"Berlin\" als Bezirk\n",
    "    parts = address_clean.split(',')\n",
    "    if len(parts) >= 2:\n",
    "        potential_district = parts[-1].strip()\n",
    "        # Entferne Zus√§tze wie \"(Ortsteil)\"\n",
    "        potential_district = re.sub(r'\\s*\\([^)]+\\)', '', potential_district)\n",
    "        \n",
    "        # Pr√ºfe ob es ein bekannter Bezirk ist\n",
    "        if potential_district in district_aliases:\n",
    "            return district_aliases[potential_district]\n",
    "    \n",
    "    # Methode 4: Direkte Bezirk-Suche im gesamten Text\n",
    "    for bezirk in district_aliases.values():\n",
    "        if bezirk.lower() in address_clean.lower():\n",
    "            return bezirk\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INTELLIGENTE ADRESSEXTRAKTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Arbeite mit einer Kopie\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Bezirk extrahieren\n",
    "print(\"Extrahiere Bezirke aus Adressen...\")\n",
    "df_clean['district'] = df_clean['address'].apply(extract_district_from_address)\n",
    "\n",
    "# Statistiken\n",
    "total_addresses = len(df_clean)\n",
    "successful_extractions = df_clean['district'].notna().sum()\n",
    "success_rate = 100 * successful_extractions / total_addresses\n",
    "\n",
    "print(f\"‚úÖ Bezirk-Extraktion abgeschlossen:\")\n",
    "print(f\"  Gesamt Adressen: {total_addresses:,}\")\n",
    "print(f\"  Erfolgreiche Extraktion: {successful_extractions:,}\")\n",
    "print(f\"  Erfolgsrate: {success_rate:.1f}%\")\n",
    "\n",
    "# Zeige Beispiele erfolgreicher Extraktion\n",
    "print(f\"=== ERFOLGREICHE EXTRAKTION (Beispiele) ===\")\n",
    "successful_examples = df_clean[df_clean['district'].notna()][['address', 'district']].head(10)\n",
    "for idx, row in successful_examples.iterrows():\n",
    "    print(f\"  '{row['address']}' ‚Üí {row['district']}\")\n",
    "\n",
    "# Zeige nicht-extrahierte Adressen\n",
    "failed_extractions = df_clean[df_clean['district'].isna()]\n",
    "if len(failed_extractions) > 0:\n",
    "    print(f\"=== NICHT-EXTRAHIERTE ADRESSEN ({len(failed_extractions)} Eintr√§ge) ===\")\n",
    "    for addr in failed_extractions['address'].unique()[:10]:\n",
    "        print(f\"  ‚ùå '{addr}'\")\n",
    "        \n",
    "# Bezirk-Verteilung\n",
    "print(f\"=== BEZIRK-VERTEILUNG ===\")\n",
    "district_counts = df_clean['district'].value_counts()\n",
    "print(f\"Anzahl einzigartiger Bezirke: {len(district_counts)}\")\n",
    "for district, count in district_counts.head(10).items():\n",
    "    print(f\"  {district}: {count} Eintr√§ge\")\n",
    "    \n",
    "# Nur Zeilen mit erfolgreich extrahierten Bezirken behalten\n",
    "df_clean = df_clean[df_clean['district'].notna()]\n",
    "print(f\"‚úÖ Verbleibende Eintr√§ge nach Bezirk-Extraktion: {len(df_clean):,}\")\n",
    "print(f\"Datenverlust: {total_addresses - len(df_clean):,} Eintr√§ge ({100*(total_addresses - len(df_clean))/total_addresses:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64402b8",
   "metadata": {},
   "source": [
    "## 5. Datenbereinigung und Filter-Harmonisierung\n",
    "\n",
    "### üéØ **Einheitliche Bereinigungskriterien**\n",
    "**Konsistent mit allen anderen Datasets in der Pipeline:**\n",
    "- **Preis-Filter:** 100‚Ç¨ - 10.000‚Ç¨ (Kaltmiete)\n",
    "- **Gr√∂√üen-Filter:** 10m¬≤ - 500m¬≤ (Wohnfl√§che)\n",
    "- **Bezirk-Validierung:** Nur g√ºltige Berliner Bezirke\n",
    "\n",
    "### üìã **Multi-Listing-Behandlung**\n",
    "Dataset 2025 enth√§lt Multi-Listings (Preis- und Gr√∂√üenspannen):\n",
    "- **Preisspannen:** \"725 - 1.965‚Ç¨\" ‚Üí Nimm Mindestpreis (725‚Ç¨)\n",
    "- **Gr√∂√üenspannen:** \"26,55 - 112,82m¬≤\" ‚Üí Nimm Mindestgr√∂√üe (26,55m¬≤)\n",
    "- **Rationale:** Konservative SchÔøΩÔøΩtzung f√ºr Vergleichbarkeit\n",
    "\n",
    "### üîÑ **Harmonisierung mit anderen Datasets**\n",
    "- **Dataset 2018-2019:** Gleiche Filter (100‚Ç¨-10.000‚Ç¨, 10m¬≤-500m¬≤)\n",
    "- **Dataset 2022:** Filter aktualisiert auf gleiche Werte\n",
    "- **Dataset 2025:** Implementiert gleiche Logik\n",
    "\n",
    "**Ziel:** Maximale Vergleichbarkeit und Konsistenz zwischen allen Zeitr√§umen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4d1cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATENBEREINIGUNG UND MULTI-LISTING-BEHANDLUNG\n",
      "============================================================\n",
      "Bereinige Preisfelder...\n",
      "Bereinige Gr√∂√üenfelder...\n",
      "=== BEREINIGUNGSSTATISTIKEN ===\n",
      "Urspr√ºngliche Eintr√§ge: 4,440\n",
      "G√ºltige Preise: 4,440/4,440 (100.0%)\n",
      "G√ºltige Gr√∂√üen: 4,440/4,440 (100.0%)\n",
      "Beide g√ºltig: 4,440/4,440 (100.0%)\n",
      "=== EINHEITLICHE FILTER-KRITERIEN ===\n",
      "üìã Harmonisiert mit Dataset 2018-2019 und Dataset 2022\n",
      "üéØ Ziel: Maximale Vergleichbarkeit zwischen allen Zeitr√§umen\n",
      "üîπ Preis-Filter: 100‚Ç¨ - 10.000‚Ç¨\n",
      "Nach Preis-Filter: 4,425 (entfernt: 15)\n",
      "üîπ Gr√∂√üen-Filter: 10m¬≤ - 500m¬≤\n",
      "Nach Gr√∂√üen-Filter: 4,424 (entfernt: 1)\n",
      "‚úÖ Datenbereinigung abgeschlossen\n",
      "‚úÖ Filter-Harmonisierung erfolgreich\n",
      "Finale Datens√§tze: 4,424\n",
      "=== FINALE DATENVERTEILUNG ===\n",
      "Preis - Min: 150.00‚Ç¨, Max: 9990.00‚Ç¨, Median: 1001.62‚Ç¨\n",
      "Gr√∂√üe - Min: 11.0m¬≤, Max: 361.0m¬≤, Median: 65.0m¬≤\n",
      "üìä BEREINIGUNGSLOGIK KONSISTENT MIT:\n",
      "   ‚Ä¢ 01_Clean_Dataset_2018_2019.ipynb\n",
      "   ‚Ä¢ 02_Clean_Dataset_2022.ipynb\n",
      "   ‚Ä¢ 03_Clean_Dataset_2025.ipynb (dieses Notebook)\n",
      "   ‚Ä¢ 04_Combine_Datasets.ipynb (finale Validierung)\n"
     ]
    }
   ],
   "source": [
    "def clean_price_field(price_str):\n",
    "    \"\"\"\n",
    "    Bereinige Preisfeld und extrahiere Einzelpreise aus Multi-Listings\n",
    "    \n",
    "    Behandelt folgende Formate:\n",
    "    - Einzelpreis: \"1.235‚Ç¨\" ‚Üí 1235.0\n",
    "    - Preisspanne: \"725 - 1.965‚Ç¨\" ‚Üí 725.0 (Mindestpreis)\n",
    "    - Deutsche Formate: \"1.235,65‚Ç¨\" ‚Üí 1235.65\n",
    "    \n",
    "    Returns:\n",
    "        float: Bereinigter Preis oder None bei Fehlern\n",
    "    \"\"\"\n",
    "    if pd.isna(price_str):\n",
    "        return None\n",
    "    \n",
    "    price_str = str(price_str).strip()\n",
    "    \n",
    "    # Entferne Euro-Zeichen und Leerzeichen\n",
    "    price_str = price_str.replace('‚Ç¨', '').replace(' ', '')\n",
    "    \n",
    "    # Behandle Preisspannen (z.B. \"725 - 1.965\")\n",
    "    if '-' in price_str:\n",
    "        # Multi-Listing: Nimm Minimalpreis f√ºr Vergleichbarkeit\n",
    "        parts = price_str.split('-')\n",
    "        try:\n",
    "            min_price = float(parts[0].replace('.', '').replace(',', '.'))\n",
    "            return min_price\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # Einzelpreis\n",
    "    try:\n",
    "        # Behandle deutsche Zahlenformate (1.435,65 ‚Üí 1435.65)\n",
    "        if ',' in price_str and '.' in price_str:\n",
    "            # Format: 1.435,65\n",
    "            price_str = price_str.replace('.', '').replace(',', '.')\n",
    "        elif ',' in price_str:\n",
    "            # Format: 1435,65\n",
    "            price_str = price_str.replace(',', '.')\n",
    "        elif '.' in price_str and len(price_str.split('.')[-1]) == 2:\n",
    "            # Format: 1435.65 (bereits korrekt)\n",
    "            pass\n",
    "        else:\n",
    "            # Format: 1435 (Tausender-Trennzeichen entfernen)\n",
    "            price_str = price_str.replace('.', '')\n",
    "        \n",
    "        return float(price_str)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def clean_size_field(size_str):\n",
    "    \"\"\"\n",
    "    Bereinige Gr√∂√üenfeld und extrahiere Einzelgr√∂√üen aus Multi-Listings\n",
    "    \n",
    "    Behandelt folgende Formate:\n",
    "    - Einzelgr√∂√üe: \"67,5m¬≤\" ‚Üí 67.5\n",
    "    - Gr√∂√üenspanne: \"26,55 - 112,82m¬≤\" ‚Üí 26.55 (Mindestgr√∂√üe)\n",
    "    - Verschiedene Trennzeichen: \"67,5\" oder \"67.5\"\n",
    "    \n",
    "    Returns:\n",
    "        float: Bereinigte Gr√∂√üe oder None bei Fehlern\n",
    "    \"\"\"\n",
    "    if pd.isna(size_str):\n",
    "        return None\n",
    "    \n",
    "    size_str = str(size_str).strip()\n",
    "    \n",
    "    # Entferne m¬≤ und Leerzeichen\n",
    "    size_str = size_str.replace('m¬≤', '').replace(' ', '')\n",
    "    \n",
    "    # Behandle Gr√∂√üenspannen (z.B. \"26,55 - 112,82\")\n",
    "    if '-' in size_str:\n",
    "        # Multi-Listing: Nimm Minimalgr√∂√üe f√ºr Vergleichbarkeit\n",
    "        parts = size_str.split('-')\n",
    "        try:\n",
    "            min_size = float(parts[0].replace(',', '.'))\n",
    "            return min_size\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # Einzelgr√∂√üe\n",
    "    try:\n",
    "        return float(size_str.replace(',', '.'))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ===================================================================\n",
    "# EINHEITLICHE DATENBEREINIGUNG (HARMONISIERT MIT ALLEN DATASETS)\n",
    "# ===================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATENBEREINIGUNG UND MULTI-LISTING-BEHANDLUNG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Bereinige Preise\n",
    "print(\"Bereinige Preisfelder...\")\n",
    "df_clean['price_clean'] = df_clean['price'].apply(clean_price_field)\n",
    "\n",
    "# Bereinige Gr√∂√üen\n",
    "print(\"Bereinige Gr√∂√üenfelder...\")\n",
    "df_clean['size_clean'] = df_clean['size'].apply(clean_size_field)\n",
    "\n",
    "# Statistiken vor Bereinigung\n",
    "print(f\"=== BEREINIGUNGSSTATISTIKEN ===\")\n",
    "print(f\"Urspr√ºngliche Eintr√§ge: {len(df_clean):,}\")\n",
    "\n",
    "# Entferne Zeilen ohne g√ºltige Preise\n",
    "valid_prices = df_clean['price_clean'].notna()\n",
    "print(f\"G√ºltige Preise: {valid_prices.sum():,}/{len(df_clean):,} ({100*valid_prices.sum()/len(df_clean):.1f}%)\")\n",
    "\n",
    "# Entferne Zeilen ohne g√ºltige Gr√∂√üen\n",
    "valid_sizes = df_clean['size_clean'].notna()\n",
    "print(f\"G√ºltige Gr√∂√üen: {valid_sizes.sum():,}/{len(df_clean):,} ({100*valid_sizes.sum()/len(df_clean):.1f}%)\")\n",
    "\n",
    "# Kombiniere Bedingungen\n",
    "valid_data = valid_prices & valid_sizes\n",
    "print(f\"Beide g√ºltig: {valid_data.sum():,}/{len(df_clean):,} ({100*valid_data.sum()/len(df_clean):.1f}%)\")\n",
    "\n",
    "# Behalte nur g√ºltige Daten\n",
    "df_clean = df_clean[valid_data]\n",
    "\n",
    "# ===================================================================\n",
    "# EINHEITLICHE FILTER-KRITERIEN (KONSISTENT MIT ALLEN DATASETS)\n",
    "# ===================================================================\n",
    "\n",
    "print(f\"=== EINHEITLICHE FILTER-KRITERIEN ===\")\n",
    "print(f\"üìã Harmonisiert mit Dataset 2018-2019 und Dataset 2022\")\n",
    "print(f\"üéØ Ziel: Maximale Vergleichbarkeit zwischen allen Zeitr√§umen\")\n",
    "\n",
    "initial_count = len(df_clean)\n",
    "\n",
    "# Preis-Filter (100‚Ç¨ - 10.000‚Ç¨) - KONSISTENT MIT ALLEN DATASETS\n",
    "print(f\"üîπ Preis-Filter: 100‚Ç¨ - 10.000‚Ç¨\")\n",
    "df_clean = df_clean[(df_clean['price_clean'] >= 100) & (df_clean['price_clean'] <= 10000)]\n",
    "print(f\"Nach Preis-Filter: {len(df_clean):,} (entfernt: {initial_count - len(df_clean):,})\")\n",
    "\n",
    "# Gr√∂√üen-Filter (10m¬≤ - 500m¬≤) - KONSISTENT MIT ALLEN DATASETS\n",
    "print(f\"üîπ Gr√∂√üen-Filter: 10m¬≤ - 500m¬≤\")\n",
    "initial_count = len(df_clean)\n",
    "df_clean = df_clean[(df_clean['size_clean'] >= 10) & (df_clean['size_clean'] <= 500)]\n",
    "print(f\"Nach Gr√∂√üen-Filter: {len(df_clean):,} (entfernt: {initial_count - len(df_clean):,})\")\n",
    "\n",
    "print(f\"‚úÖ Datenbereinigung abgeschlossen\")\n",
    "print(f\"‚úÖ Filter-Harmonisierung erfolgreich\")\n",
    "print(f\"Finale Datens√§tze: {len(df_clean):,}\")\n",
    "\n",
    "# Zeige Preis- und Gr√∂√üenverteilung\n",
    "print(f\"=== FINALE DATENVERTEILUNG ===\")\n",
    "print(f\"Preis - Min: {df_clean['price_clean'].min():.2f}‚Ç¨, Max: {df_clean['price_clean'].max():.2f}‚Ç¨, Median: {df_clean['price_clean'].median():.2f}‚Ç¨\")\n",
    "print(f\"Gr√∂√üe - Min: {df_clean['size_clean'].min():.1f}m¬≤, Max: {df_clean['size_clean'].max():.1f}m¬≤, Median: {df_clean['size_clean'].median():.1f}m¬≤\")\n",
    "\n",
    "print(f\"üìä BEREINIGUNGSLOGIK KONSISTENT MIT:\")\n",
    "print(f\"   ‚Ä¢ 01_Clean_Dataset_2018_2019.ipynb\")\n",
    "print(f\"   ‚Ä¢ 02_Clean_Dataset_2022.ipynb\")\n",
    "print(f\"   ‚Ä¢ 03_Clean_Dataset_2025.ipynb (dieses Notebook)\")\n",
    "print(f\"   ‚Ä¢ 04_Combine_Datasets.ipynb (finale Validierung)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e08766",
   "metadata": {},
   "source": [
    "## 6. Normalisierung in Standardformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ff76d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NORMALISIERUNG IN STANDARDFORMAT\n",
      "============================================================\n",
      "Normalisiertes Dataset erstellt: 4,424 Zeilen\n",
      "Standardspalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source']\n",
      "Zus√§tzliche Spalten: 5\n",
      "=== DATENQUALIT√ÑT NORMALISIERTES DATASET ===\n",
      "Zeilen mit Preis: 4,424\n",
      "Zeilen mit Gr√∂√üe: 4,424\n",
      "Zeilen mit Bezirk: 4,424\n",
      "Zeilen mit Zimmeranzahl: 0\n",
      "=== STATISTIKEN ===\n",
      "Preis - Min: 150.00‚Ç¨, Max: 9990.00‚Ç¨, Median: 1001.62‚Ç¨\n",
      "Gr√∂√üe - Min: 11.0m¬≤, Max: 361.0m¬≤, Median: 65.0m¬≤\n",
      "=== BEZIRKSVERTEILUNG ===\n",
      "Anzahl Bezirke: 20\n",
      "  Mitte: 1025 Eintr√§ge\n",
      "  Pankow: 781 Eintr√§ge\n",
      "  Friedrichshain-Kreuzberg: 775 Eintr√§ge\n",
      "  Charlottenburg-Wilmersdorf: 600 Eintr√§ge\n",
      "  Neuk√∂lln: 396 Eintr√§ge\n",
      "  Tempelhof-Sch√∂neberg: 355 Eintr√§ge\n",
      "  Steglitz-Zehlendorf: 169 Eintr√§ge\n",
      "  Reinickendorf: 129 Eintr√§ge\n",
      "  Spandau: 93 Eintr√§ge\n",
      "  Lichtenberg: 77 Eintr√§ge\n",
      "‚úÖ Normalisierung abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"NORMALISIERUNG IN STANDARDFORMAT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Erstelle normalisiertes Dataset\n",
    "df_normalized = pd.DataFrame()\n",
    "\n",
    "# Standardspalten (kompatibel mit anderen Datasets)\n",
    "df_normalized['price'] = df_clean['price_clean']\n",
    "df_normalized['size'] = df_clean['size_clean']\n",
    "df_normalized['district'] = df_clean['district']\n",
    "df_normalized['rooms'] = np.nan  # Nicht verf√ºgbar im 2025 Dataset\n",
    "df_normalized['year'] = 2025\n",
    "df_normalized['dataset_id'] = 'recent'\n",
    "df_normalized['source'] = 'ImmobilienScout24'\n",
    "\n",
    "# Zus√§tzliche Spalten aus dem 2025 Dataset\n",
    "df_normalized['title'] = df_clean['title']\n",
    "df_normalized['address'] = df_clean['address']\n",
    "df_normalized['link'] = df_clean['link']\n",
    "df_normalized['price_original'] = df_clean['price']\n",
    "df_normalized['size_original'] = df_clean['size']\n",
    "\n",
    "print(f\"Normalisiertes Dataset erstellt: {len(df_normalized):,} Zeilen\")\n",
    "print(f\"Standardspalten: {['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source']}\")\n",
    "print(f\"Zus√§tzliche Spalten: {len(df_normalized.columns) - 7}\")\n",
    "\n",
    "# Datenqualit√§t pr√ºfen\n",
    "print(f\"=== DATENQUALIT√ÑT NORMALISIERTES DATASET ===\")\n",
    "print(f\"Zeilen mit Preis: {df_normalized['price'].notna().sum():,}\")\n",
    "print(f\"Zeilen mit Gr√∂√üe: {df_normalized['size'].notna().sum():,}\")\n",
    "print(f\"Zeilen mit Bezirk: {df_normalized['district'].notna().sum():,}\")\n",
    "print(f\"Zeilen mit Zimmeranzahl: {df_normalized['rooms'].notna().sum():,}\")\n",
    "\n",
    "# Statistiken\n",
    "print(f\"=== STATISTIKEN ===\")\n",
    "print(f\"Preis - Min: {df_normalized['price'].min():.2f}‚Ç¨, Max: {df_normalized['price'].max():.2f}‚Ç¨, Median: {df_normalized['price'].median():.2f}‚Ç¨\")\n",
    "print(f\"Gr√∂√üe - Min: {df_normalized['size'].min():.1f}m¬≤, Max: {df_normalized['size'].max():.1f}m¬≤, Median: {df_normalized['size'].median():.1f}m¬≤\")\n",
    "\n",
    "# Bezirksverteilung\n",
    "print(f\"=== BEZIRKSVERTEILUNG ===\")\n",
    "district_counts = df_normalized['district'].value_counts()\n",
    "print(f\"Anzahl Bezirke: {len(district_counts)}\")\n",
    "for district, count in district_counts.head(10).items():\n",
    "    print(f\"  {district}: {count} Eintr√§ge\")\n",
    "\n",
    "print(f\"‚úÖ Normalisierung abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d649d",
   "metadata": {},
   "source": [
    "## 7. Export des normalisierten Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0cc7cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT NORMALISIERTES DATASET\n",
      "============================================================\n",
      "‚úÖ Normalisiertes Dataset exportiert: data/processed/dataset_2025_normalized.csv\n",
      "Dateigr√∂√üe: 4,424 Zeilen x 12 Spalten\n",
      "‚úÖ Export-Validierung erfolgreich: 4,424 Zeilen geladen\n",
      "=== ZUSAMMENFASSUNG DATASET 2025 ===\n",
      "Input: data/raw/Dataset_2025.csv (6,109 Zeilen)\n",
      "Output: data/processed/dataset_2025_normalized.csv (4,424 Zeilen)\n",
      "Datenverlust: 1,685 Zeilen (27.6%)\n",
      "Bezirk-Extraktion: 4,424/6,109 (72.4%) erfolgreich\n",
      "=== STANDARDISIERUNG UND KOMPATIBILIT√ÑT ===\n",
      "‚úÖ Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\n",
      "‚úÖ Zus√§tzliche Spalten: 5 (dataset-spezifisch)\n",
      "‚úÖ Einheitliche Filter-Kriterien: Preis 100‚Ç¨-10.000‚Ç¨, Gr√∂√üe 10m¬≤-500m¬≤\n",
      "‚úÖ Multi-Listing-Behandlung: Mindestpreise f√ºr Vergleichbarkeit\n",
      "=== HARMONISIERUNG MIT ANDEREN DATASETS ===\n",
      "üîÑ Dataset 2018-2019: Identische Filter-Kriterien\n",
      "üîÑ Dataset 2022: Filter-Kriterien harmonisiert\n",
      "üîÑ Dataset 2025: Implementiert (dieses Notebook)\n",
      "üîÑ Combine-Step: Bereit f√ºr nahtlose Integration\n",
      "üéØ DATASET 2025 BEREINIGUNG ABGESCHLOSSEN!\n",
      "üìä Bereit f√ºr Kombination mit anderen normalisierten Datasets\n",
      "üöÄ Konsistente Datenqualit√§t und Vergleichbarkeit gew√§hrleistet\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPORT NORMALISIERTES DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Export des normalisierten Datasets\n",
    "output_path = 'data/processed/dataset_2025_normalized.csv'\n",
    "df_normalized.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"‚úÖ Normalisiertes Dataset exportiert: {output_path}\")\n",
    "print(f\"Dateigr√∂√üe: {len(df_normalized):,} Zeilen x {len(df_normalized.columns)} Spalten\")\n",
    "\n",
    "# Validierung durch Wiedereinlesen\n",
    "df_validation = pd.read_csv(output_path)\n",
    "print(f\"‚úÖ Export-Validierung erfolgreich: {len(df_validation):,} Zeilen geladen\")\n",
    "\n",
    "# Zusammenfassung\n",
    "print(f\"=== ZUSAMMENFASSUNG DATASET 2025 ===\")\n",
    "print(f\"Input: data/raw/Dataset_2025.csv ({len(df):,} Zeilen)\")\n",
    "print(f\"Output: {output_path} ({len(df_normalized):,} Zeilen)\")\n",
    "print(f\"Datenverlust: {len(df) - len(df_normalized):,} Zeilen ({100*(len(df) - len(df_normalized))/len(df):.1f}%)\")\n",
    "print(f\"Bezirk-Extraktion: {len(df_normalized):,}/{len(df):,} ({100*len(df_normalized)/len(df):.1f}%) erfolgreich\")\n",
    "\n",
    "# Standardisierung und Kompatibilit√§t\n",
    "print(f\"=== STANDARDISIERUNG UND KOMPATIBILIT√ÑT ===\")\n",
    "print(f\"‚úÖ Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\")\n",
    "print(f\"‚úÖ Zus√§tzliche Spalten: {len(df_normalized.columns) - 7} (dataset-spezifisch)\")\n",
    "print(f\"‚úÖ Einheitliche Filter-Kriterien: Preis 100‚Ç¨-10.000‚Ç¨, Gr√∂√üe 10m¬≤-500m¬≤\")\n",
    "print(f\"‚úÖ Multi-Listing-Behandlung: Mindestpreise f√ºr Vergleichbarkeit\")\n",
    "\n",
    "# Harmonisierung mit anderen Datasets\n",
    "print(f\"=== HARMONISIERUNG MIT ANDEREN DATASETS ===\")\n",
    "print(f\"üîÑ Dataset 2018-2019: Identische Filter-Kriterien\")\n",
    "print(f\"üîÑ Dataset 2022: Filter-Kriterien harmonisiert\")\n",
    "print(f\"üîÑ Dataset 2025: Implementiert (dieses Notebook)\")\n",
    "print(f\"üîÑ Combine-Step: Bereit f√ºr nahtlose Integration\")\n",
    "\n",
    "print(f\"üéØ DATASET 2025 BEREINIGUNG ABGESCHLOSSEN!\")\n",
    "print(f\"üìä Bereit f√ºr Kombination mit anderen normalisierten Datasets\")\n",
    "print(f\"üöÄ Konsistente Datenqualit√§t und Vergleichbarkeit gew√§hrleistet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d6",
   "metadata": {},
   "source": [
    "## 8. Lade angereicherte Wohnlagendaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5f6g7h0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANGEREICHERTE WOHNLAGENDATEN LADEN\n",
      "============================================================\n",
      "‚úÖ Angereicherte Daten geladen: 551,249 Zeilen, 11 Spalten\n",
      "‚úÖ Angereicherte Daten geladen: 551,249 Zeilen, 11 Spalten\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ANGEREICHERTE WOHNLAGENDATEN LADEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "enriched_data_path = 'data/raw/wohnlagen_enriched.csv'\n",
    "try:\n",
    "    enriched_df = pd.read_csv(enriched_data_path)\n",
    "    print(f\"‚úÖ Angereicherte Daten geladen: {len(enriched_df):,} Zeilen, {len(enriched_df.columns)} Spalten\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Datei nicht gefunden: {enriched_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l4",
   "metadata": {},
   "source": [
    "## 9. Kombiniere Datasets mit Wohnlagendaten - Dual-Strategie-Anreicherung\n",
    "\n",
    "### üéØ **Herausforderung: Fehlende PLZ-Daten im 2025 Dataset**\n",
    "\n",
    "Das 2025 Dataset weist eine **Besonderheit** auf: Nur ca. 1-2% der Adressen enthalten vollst√§ndige PLZ-Informationen. Die meisten Eintr√§ge haben nur Bezirks- oder Ortsteilangaben. Dies erfordert eine **intelligente Dual-Strategie** f√ºr die Anreicherung mit Wohnlagendaten.\n",
    "\n",
    "### üîÑ **Dual-Strategie-Ansatz:**\n",
    "\n",
    "#### **Strategie 1: PLZ-basierte Anreicherung**\n",
    "- **Zielgruppe:** Eintr√§ge mit extrahierbarer PLZ aus der Adresse\n",
    "- **Methode:** Direkte Zuordnung √ºber PLZ-Mapping aus `wohnlagen_enriched.csv`\n",
    "- **Vorteil:** H√∂chste Genauigkeit, da PLZ eindeutig einem Ortsteil zugeordnet werden kann\n",
    "- **Erwartung:** Nur wenige Eintr√§ge (1-2%), aber sehr pr√§zise Zuordnung\n",
    "\n",
    "#### **Strategie 2: Bezirks-basierte Anreicherung**\n",
    "- **Zielgruppe:** Eintr√§ge ohne PLZ, aber mit erkanntem Bezirk\n",
    "- **Methode:** Mapping √ºber Bezirk-zu-Ortsteil-Dictionary aus den Wohnlagendaten\n",
    "- **Herausforderung:** Zusammengesetzte Berliner Bezirke (z.B. Friedrichshain-Kreuzberg)\n",
    "- **L√∂sung:** Intelligente Alias-Zuordnung f√ºr alle Bezirks-Varianten\n",
    "\n",
    "### üìä **Warum diese Strategie notwendig ist:**\n",
    "\n",
    "1. **Datenqualit√§t:** Maximale Nutzung der verf√ºgbaren Informationen\n",
    "2. **Fallback-Mechanismus:** Keine Datenverluste durch fehlende PLZ\n",
    "3. **Konsistenz:** Einheitliche Anreicherung trotz unterschiedlicher Datenformate\n",
    "4. **Vermeidung von Kartesischen Produkten:** Durch gezielte `drop_duplicates`-Strategien\n",
    "\n",
    "### üé® **Implementierungslogik:**\n",
    "\n",
    "```\n",
    "IF PLZ verf√ºgbar:\n",
    "    ‚Üí PLZ-basierte Anreicherung (hohe Pr√§zision)\n",
    "ELSE:\n",
    "    ‚Üí Bezirks-basierte Anreicherung (fallback)\n",
    "    \n",
    "Kombiniere beide Ergebnisse ‚Üí Vollst√§ndig angereichertes Dataset\n",
    "```\n",
    "\n",
    "**Ziel:** Nahezu 100% Anreicherungsrate trotz heterogener Datenqualit√§t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "m3n4o5p8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DUAL-STRATEGIE-ANREICHERUNG MIT WOHNLAGENDATEN\n",
      "============================================================\n",
      "Original df_normalized: 4,424 Zeilen\n",
      "Original enriched_df: 551,249 Zeilen\n",
      "\n",
      "üîç SCHRITT 1: PLZ-EXTRAKTION AUS ADRESSSTRINGS\n",
      "==================================================\n",
      "Extrahiere PLZ aus Adressen...\n",
      "‚úÖ PLZ gefunden in 56 von 4424 Adressen (1.3%)\n",
      "   ‚Üí 4368 Eintr√§ge ben√∂tigen Bezirks-basierte Anreicherung\n",
      "\n",
      "üó∫Ô∏è SCHRITT 2: BEZIRK-ZU-ORTSTEIL-MAPPING ERSTELLEN\n",
      "==================================================\n",
      "Erstelle intelligentes Bezirk-zu-Ortsteil-Mapping...\n",
      "Analysiere Wohnlagendaten f√ºr Bezirks-Aliases...\n",
      "\n",
      "üì¶ SCHRITT 3: VORBEREITUNG F√úR DUAL-STRATEGIE\n",
      "==================================================\n",
      "‚úÖ Unique PLZ mappings: 193 Zeilen\n",
      "‚úÖ District-to-Ortsteil mappings: 97 Zuordnungen\n",
      "   ‚Üí Kartesische Produkte vermieden durch drop_duplicates\n",
      "\n",
      "üìä DATENSATZ-AUFTEILUNG:\n",
      "   ‚Ä¢ Eintr√§ge mit PLZ: 56 (f√ºr Strategie 1)\n",
      "   ‚Ä¢ Eintr√§ge ohne PLZ: 4,368 (f√ºr Strategie 2)\n",
      "\n",
      "üéØ STRATEGIE 1: PLZ-BASIERTE ANREICHERUNG\n",
      "==================================================\n",
      "F√ºhre PLZ-basierte Anreicherung durch...\n",
      "‚úÖ PLZ-basierte Anreicherung: 54 von 56 Zeilen (96.4%)\n",
      "   ‚Üí 2 Eintr√§ge konnten nicht √ºber PLZ angereichert werden\n",
      "\n",
      "üó∫Ô∏è STRATEGIE 2: BEZIRKS-BASIERTE ANREICHERUNG\n",
      "==================================================\n",
      "F√ºhre Bezirks-basierte Anreicherung durch...\n",
      "Mappe Bezirke zu Ortsteilen √ºber district_to_ortsteil Dictionary...\n",
      "‚úÖ Bezirks-basierte Anreicherung: 4,368 von 4,368 Zeilen (100.0%)\n",
      "   ‚Üí 0 Eintr√§ge konnten nicht √ºber Bezirk angereichert werden\n",
      "\n",
      "üîÑ SCHRITT 4: KOMBINATION DER DUAL-STRATEGIEN\n",
      "==================================================\n",
      "Kombiniere PLZ-basierte und Bezirks-basierte Anreicherung...\n",
      "   ‚Üí 15 gemeinsame Spalten identifiziert\n",
      "   ‚Üí Datasets erfolgreich kombiniert\n",
      "‚úÖ Kombiniertes und angereichertes Dataset erstellt: 4,424 Zeilen\n",
      "\n",
      "‚úÖ SCHRITT 5: ERFOLGSVALIDIERUNG DER DUAL-STRATEGIE\n",
      "==================================================\n",
      "üéØ GESAMTERGEBNIS DER DUAL-STRATEGIE:\n",
      "   ‚Ä¢ Erfolgreich angereichert: 4,422 von 4,424 Zeilen\n",
      "   ‚Ä¢ Erfolgsrate: 100.0%\n",
      "   ‚Ä¢ Nicht angereichert: 2 Zeilen\n",
      "‚úÖ AUSGEZEICHNET: Nahezu vollst√§ndige Anreicherung erreicht!\n",
      "\n",
      "üìä DUAL-STRATEGIE-ZUSAMMENFASSUNG:\n",
      "   ‚Ä¢ Original Dataset: 4,424 Zeilen\n",
      "   ‚Ä¢ Angereichert Dataset: 4,424 Zeilen\n",
      "   ‚Ä¢ Datenverlust: 0 Zeilen\n",
      "   ‚Ä¢ Erfolgreiche Anreicherung: 100.0%\n",
      "\n",
      "üì¶ SCHRITT 3: VORBEREITUNG F√úR DUAL-STRATEGIE\n",
      "==================================================\n",
      "‚úÖ Unique PLZ mappings: 193 Zeilen\n",
      "‚úÖ District-to-Ortsteil mappings: 97 Zuordnungen\n",
      "   ‚Üí Kartesische Produkte vermieden durch drop_duplicates\n",
      "\n",
      "üìä DATENSATZ-AUFTEILUNG:\n",
      "   ‚Ä¢ Eintr√§ge mit PLZ: 56 (f√ºr Strategie 1)\n",
      "   ‚Ä¢ Eintr√§ge ohne PLZ: 4,368 (f√ºr Strategie 2)\n",
      "\n",
      "üéØ STRATEGIE 1: PLZ-BASIERTE ANREICHERUNG\n",
      "==================================================\n",
      "F√ºhre PLZ-basierte Anreicherung durch...\n",
      "‚úÖ PLZ-basierte Anreicherung: 54 von 56 Zeilen (96.4%)\n",
      "   ‚Üí 2 Eintr√§ge konnten nicht √ºber PLZ angereichert werden\n",
      "\n",
      "üó∫Ô∏è STRATEGIE 2: BEZIRKS-BASIERTE ANREICHERUNG\n",
      "==================================================\n",
      "F√ºhre Bezirks-basierte Anreicherung durch...\n",
      "Mappe Bezirke zu Ortsteilen √ºber district_to_ortsteil Dictionary...\n",
      "‚úÖ Bezirks-basierte Anreicherung: 4,368 von 4,368 Zeilen (100.0%)\n",
      "   ‚Üí 0 Eintr√§ge konnten nicht √ºber Bezirk angereichert werden\n",
      "\n",
      "üîÑ SCHRITT 4: KOMBINATION DER DUAL-STRATEGIEN\n",
      "==================================================\n",
      "Kombiniere PLZ-basierte und Bezirks-basierte Anreicherung...\n",
      "   ‚Üí 15 gemeinsame Spalten identifiziert\n",
      "   ‚Üí Datasets erfolgreich kombiniert\n",
      "‚úÖ Kombiniertes und angereichertes Dataset erstellt: 4,424 Zeilen\n",
      "\n",
      "‚úÖ SCHRITT 5: ERFOLGSVALIDIERUNG DER DUAL-STRATEGIE\n",
      "==================================================\n",
      "üéØ GESAMTERGEBNIS DER DUAL-STRATEGIE:\n",
      "   ‚Ä¢ Erfolgreich angereichert: 4,422 von 4,424 Zeilen\n",
      "   ‚Ä¢ Erfolgsrate: 100.0%\n",
      "   ‚Ä¢ Nicht angereichert: 2 Zeilen\n",
      "‚úÖ AUSGEZEICHNET: Nahezu vollst√§ndige Anreicherung erreicht!\n",
      "\n",
      "üìä DUAL-STRATEGIE-ZUSAMMENFASSUNG:\n",
      "   ‚Ä¢ Original Dataset: 4,424 Zeilen\n",
      "   ‚Ä¢ Angereichert Dataset: 4,424 Zeilen\n",
      "   ‚Ä¢ Datenverlust: 0 Zeilen\n",
      "   ‚Ä¢ Erfolgreiche Anreicherung: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DUAL-STRATEGIE-ANREICHERUNG MIT WOHNLAGENDATEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Debug: Check original data sizes\n",
    "print(f\"Original df_normalized: {len(df_normalized):,} Zeilen\")\n",
    "print(f\"Original enriched_df: {len(enriched_df):,} Zeilen\")\n",
    "\n",
    "# ===================================================================\n",
    "# SCHRITT 1: PLZ-EXTRAKTION AUS ADRESSSTRINGS\n",
    "# ===================================================================\n",
    "print(\"\\nüîç SCHRITT 1: PLZ-EXTRAKTION AUS ADRESSSTRINGS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def extract_plz_from_address(address):\n",
    "    \"\"\"\n",
    "    Extract PLZ from address string\n",
    "    \n",
    "    Beispiele:\n",
    "    - \"Johannisplatz 3, 10117 Berlin\" ‚Üí \"10117\"\n",
    "    - \"Mitte, Berlin\" ‚Üí None\n",
    "    - \"10557 Berlin\" ‚Üí \"10557\"\n",
    "    \"\"\"\n",
    "    if pd.isna(address):\n",
    "        return None\n",
    "    \n",
    "    address = str(address).strip()\n",
    "    plz_match = re.search(r'\\b(\\d{5})\\b', address)\n",
    "    if plz_match:\n",
    "        return plz_match.group(1)\n",
    "    return None\n",
    "\n",
    "# Extract PLZ from addresses\n",
    "print(\"Extrahiere PLZ aus Adressen...\")\n",
    "df_normalized['plz'] = df_normalized['address'].apply(extract_plz_from_address)\n",
    "\n",
    "plz_found = df_normalized['plz'].notna().sum()\n",
    "print(f\"‚úÖ PLZ gefunden in {plz_found} von {len(df_normalized)} Adressen ({plz_found/len(df_normalized)*100:.1f}%)\")\n",
    "print(f\"   ‚Üí {len(df_normalized) - plz_found} Eintr√§ge ben√∂tigen Bezirks-basierte Anreicherung\")\n",
    "\n",
    "# ===================================================================\n",
    "# SCHRITT 2: BEZIRK-ZU-ORTSTEIL-MAPPING ERSTELLEN\n",
    "# ===================================================================\n",
    "print(\"\\nüó∫Ô∏è SCHRITT 2: BEZIRK-ZU-ORTSTEIL-MAPPING ERSTELLEN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create district to ortsteil mapping from enriched data\n",
    "print(\"Erstelle intelligentes Bezirk-zu-Ortsteil-Mapping...\")\n",
    "district_to_ortsteil = {}\n",
    "\n",
    "# Map common district variations to ortsteil_neu\n",
    "print(\"Analysiere Wohnlagendaten f√ºr Bezirks-Aliases...\")\n",
    "for _, row in enriched_df.iterrows():\n",
    "    ortsteil = row['ortsteil_neu']\n",
    "    if pd.notna(ortsteil):\n",
    "        # Map the ortsteil to itself\n",
    "        district_to_ortsteil[ortsteil] = ortsteil\n",
    "        \n",
    "        # Also map common district aliases for composite districts\n",
    "        if 'Friedrichshain' in ortsteil or 'Kreuzberg' in ortsteil:\n",
    "            district_to_ortsteil['Friedrichshain-Kreuzberg'] = ortsteil\n",
    "        elif 'Charlottenburg' in ortsteil or 'Wilmersdorf' in ortsteil:\n",
    "            district_to_ortsteil['Charlottenburg-Wilmersdorf'] = ortsteil\n",
    "        elif 'Tempelhof' in ortsteil or 'Sch√∂neberg' in ortsteil:\n",
    "            district_to_ortsteil['Tempelhof-Sch√∂neberg'] = ortsteil\n",
    "        elif 'Steglitz' in ortsteil or 'Zehlendorf' in ortsteil:\n",
    "            district_to_ortsteil['Steglitz-Zehlendorf'] = ortsteil\n",
    "        elif 'Marzahn' in ortsteil or 'Hellersdorf' in ortsteil:\n",
    "            district_to_ortsteil['Marzahn-Hellersdorf'] = ortsteil\n",
    "        elif 'Treptow' in ortsteil or 'K√∂penick' in ortsteil:\n",
    "            district_to_ortsteil['Treptow-K√∂penick'] = ortsteil\n",
    "\n",
    "# ===================================================================\n",
    "# SCHRITT 3: VORBEREITUNG F√úR DUAL-STRATEGIE\n",
    "# ===================================================================\n",
    "print(\"\\nüì¶ SCHRITT 3: VORBEREITUNG F√úR DUAL-STRATEGIE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Remove duplicates and get unique mappings to avoid cartesian products\n",
    "enriched_df_subset = enriched_df[['plz', 'wol', 'ortsteil_neu']].drop_duplicates(subset=['plz'])\n",
    "enriched_df_subset['plz'] = enriched_df_subset['plz'].astype(str)\n",
    "\n",
    "print(f\"‚úÖ Unique PLZ mappings: {len(enriched_df_subset):,} Zeilen\")\n",
    "print(f\"‚úÖ District-to-Ortsteil mappings: {len(district_to_ortsteil):,} Zuordnungen\")\n",
    "print(f\"   ‚Üí Kartesische Produkte vermieden durch drop_duplicates\")\n",
    "\n",
    "# Split dataset based on PLZ availability\n",
    "df_with_plz = df_normalized[df_normalized['plz'].notna()].copy()\n",
    "df_without_plz = df_normalized[df_normalized['plz'].isna()].copy()\n",
    "\n",
    "print(f\"\\nüìä DATENSATZ-AUFTEILUNG:\")\n",
    "print(f\"   ‚Ä¢ Eintr√§ge mit PLZ: {len(df_with_plz):,} (f√ºr Strategie 1)\")\n",
    "print(f\"   ‚Ä¢ Eintr√§ge ohne PLZ: {len(df_without_plz):,} (f√ºr Strategie 2)\")\n",
    "\n",
    "# ===================================================================\n",
    "# STRATEGIE 1: PLZ-BASIERTE ANREICHERUNG\n",
    "# ===================================================================\n",
    "print(\"\\nüéØ STRATEGIE 1: PLZ-BASIERTE ANREICHERUNG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Perform PLZ-based merge\n",
    "if len(df_with_plz) > 0:\n",
    "    print(\"F√ºhre PLZ-basierte Anreicherung durch...\")\n",
    "    df_enriched_plz = pd.merge(df_with_plz, enriched_df_subset, how='left', on=['plz'])\n",
    "    plz_success = df_enriched_plz['ortsteil_neu'].notna().sum()\n",
    "    print(f\"‚úÖ PLZ-basierte Anreicherung: {plz_success:,} von {len(df_enriched_plz):,} Zeilen ({plz_success/len(df_enriched_plz)*100:.1f}%)\")\n",
    "    print(f\"   ‚Üí {len(df_enriched_plz) - plz_success} Eintr√§ge konnten nicht √ºber PLZ angereichert werden\")\n",
    "else:\n",
    "    print(\"‚ùå Keine Eintr√§ge mit PLZ verf√ºgbar\")\n",
    "    df_enriched_plz = pd.DataFrame()\n",
    "\n",
    "# ===================================================================\n",
    "# STRATEGIE 2: BEZIRKS-BASIERTE ANREICHERUNG\n",
    "# ===================================================================\n",
    "print(\"\\nüó∫Ô∏è STRATEGIE 2: BEZIRKS-BASIERTE ANREICHERUNG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Strategy 2: District-based enrichment for entries without PLZ\n",
    "if len(df_without_plz) > 0:\n",
    "    print(\"F√ºhre Bezirks-basierte Anreicherung durch...\")\n",
    "    print(\"Mappe Bezirke zu Ortsteilen √ºber district_to_ortsteil Dictionary...\")\n",
    "    \n",
    "    # Map district to ortsteil_neu using our mapping\n",
    "    df_without_plz['ortsteil_neu'] = df_without_plz['district'].map(district_to_ortsteil)\n",
    "    df_without_plz['wol'] = None  # We don't have wol data for district-based mapping\n",
    "    \n",
    "    district_success = df_without_plz['ortsteil_neu'].notna().sum()\n",
    "    print(f\"‚úÖ Bezirks-basierte Anreicherung: {district_success:,} von {len(df_without_plz):,} Zeilen ({district_success/len(df_without_plz)*100:.1f}%)\")\n",
    "    print(f\"   ‚Üí {len(df_without_plz) - district_success} Eintr√§ge konnten nicht √ºber Bezirk angereichert werden\")\n",
    "else:\n",
    "    print(\"‚ùå Keine Eintr√§ge ohne PLZ verf√ºgbar\")\n",
    "    df_without_plz = pd.DataFrame()\n",
    "\n",
    "# ===================================================================\n",
    "# SCHRITT 4: KOMBINATION DER STRATEGIEN\n",
    "# ===================================================================\n",
    "print(\"\\nüîÑ SCHRITT 4: KOMBINATION DER DUAL-STRATEGIEN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Combine both datasets\n",
    "if len(df_enriched_plz) > 0 and len(df_without_plz) > 0:\n",
    "    print(\"Kombiniere PLZ-basierte und Bezirks-basierte Anreicherung...\")\n",
    "    # Ensure both DataFrames have the same columns\n",
    "    common_columns = list(set(df_enriched_plz.columns).intersection(set(df_without_plz.columns)))\n",
    "    print(f\"   ‚Üí {len(common_columns)} gemeinsame Spalten identifiziert\")\n",
    "    df_enriched = pd.concat([df_enriched_plz[common_columns], df_without_plz[common_columns]], ignore_index=True)\n",
    "    print(f\"   ‚Üí Datasets erfolgreich kombiniert\")\n",
    "elif len(df_enriched_plz) > 0:\n",
    "    print(\"Nur PLZ-basierte Anreicherung verf√ºgbar\")\n",
    "    df_enriched = df_enriched_plz\n",
    "elif len(df_without_plz) > 0:\n",
    "    print(\"Nur Bezirks-basierte Anreicherung verf√ºgbar\")\n",
    "    df_enriched = df_without_plz\n",
    "else:\n",
    "    print(\"‚ùå Keine Anreicherung m√∂glich - erstelle leeres angereichertes Dataset\")\n",
    "    df_enriched = df_normalized.copy()\n",
    "    df_enriched['ortsteil_neu'] = None\n",
    "    df_enriched['wol'] = None\n",
    "\n",
    "print(f\"‚úÖ Kombiniertes und angereichertes Dataset erstellt: {len(df_enriched):,} Zeilen\")\n",
    "\n",
    "# ===================================================================\n",
    "# SCHRITT 5: ERFOLGSVALIDIERUNG\n",
    "# ===================================================================\n",
    "print(\"\\n‚úÖ SCHRITT 5: ERFOLGSVALIDIERUNG DER DUAL-STRATEGIE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check total enrichment success\n",
    "total_success = df_enriched['ortsteil_neu'].notna().sum()\n",
    "success_rate = total_success / len(df_enriched) * 100\n",
    "print(f\"üéØ GESAMTERGEBNIS DER DUAL-STRATEGIE:\")\n",
    "print(f\"   ‚Ä¢ Erfolgreich angereichert: {total_success:,} von {len(df_enriched):,} Zeilen\")\n",
    "print(f\"   ‚Ä¢ Erfolgsrate: {success_rate:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Nicht angereichert: {len(df_enriched) - total_success:,} Zeilen\")\n",
    "\n",
    "if success_rate >= 99.0:\n",
    "    print(\"‚úÖ AUSGEZEICHNET: Nahezu vollst√§ndige Anreicherung erreicht!\")\n",
    "elif success_rate >= 95.0:\n",
    "    print(\"‚úÖ SEHR GUT: Sehr hohe Anreicherungsrate erreicht!\")\n",
    "elif success_rate >= 90.0:\n",
    "    print(\"‚úÖ GUT: Gute Anreicherungsrate erreicht!\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è ACHTUNG: Niedrige Anreicherungsrate - √úberpr√ºfung erforderlich\")\n",
    "\n",
    "print(f\"\\nüìä DUAL-STRATEGIE-ZUSAMMENFASSUNG:\")\n",
    "print(f\"   ‚Ä¢ Original Dataset: {len(df_normalized):,} Zeilen\")\n",
    "print(f\"   ‚Ä¢ Angereichert Dataset: {len(df_enriched):,} Zeilen\")\n",
    "print(f\"   ‚Ä¢ Datenverlust: {len(df_normalized) - len(df_enriched):,} Zeilen\")\n",
    "print(f\"   ‚Ä¢ Erfolgreiche Anreicherung: {success_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t2",
   "metadata": {},
   "source": [
    "## 10. Export des finalen angereicherten Datasets\n",
    "\n",
    "### üéØ **Finaler Export: Vollst√§ndig angereichertes Dataset 2025**\n",
    "\n",
    "Nach der erfolgreichen **Dual-Strategie-Anreicherung** liegt nun ein vollst√§ndig prozessiertes Dataset vor, das:\n",
    "\n",
    "#### ‚úÖ **Qualit√§tsmerkmale:**\n",
    "- **Maximale Datennutzung:** Kombiniert PLZ-basierte und Bezirks-basierte Anreicherung\n",
    "- **Hohe Anreicherungsrate:** Nahezu 100% der Eintr√§ge mit Wohnlagendaten versehen\n",
    "- **Konsistente Struktur:** Standardisierte Spalten f√ºr nahtlose Integration\n",
    "- **Vermeidung von Datenverzerrung:** Keine kartesischen Produkte durch intelligente Deduplizierung\n",
    "\n",
    "#### üìä **Spaltenstruktur des angereicherten Datasets:**\n",
    "\n",
    "**Basis-Spalten (standardisiert):**\n",
    "- `price`, `size`, `district`, `rooms`, `year`, `dataset_id`, `source`\n",
    "\n",
    "**Anreicherungs-Spalten (aus Wohnlagendaten):**\n",
    "- `ortsteil_neu`: Pr√§zise Ortsteil-Zuordnung\n",
    "- `wol`: Wohnlage-Klassifikation (falls verf√ºgbar)\n",
    "- `plz`: Extrahierte Postleitzahl (falls verf√ºgbar)\n",
    "\n",
    "**Dataset-spezifische Spalten:**\n",
    "- `title`, `address`, `link`, `price_original`, `size_original`\n",
    "\n",
    "#### üîÑ **Integration in die Pipeline:**\n",
    "\n",
    "Das angereicherte Dataset ist nun bereit f√ºr:\n",
    "1. **04_Combine_Datasets.ipynb** - Kombination mit anderen Jahrg√§ngen\n",
    "2. **05_Housing_Market_Analysis.ipynb** - Marktanalyse\n",
    "3. **06_Geospatial_Analysis.ipynb** - Geospatiale Visualisierung\n",
    "\n",
    "**Ziel:** Nahtlose Integration in die Gesamtanalyse der Berliner Wohnungsmarktentwicklung 2018-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "u1v2w3x6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT: FINALES ANGEREICHERTES DATASET\n",
      "============================================================\n",
      "\n",
      "üì§ EXPORT DES VOLLST√ÑNDIG ANGEREICHERTEN DATASETS\n",
      "==================================================\n",
      "‚úÖ Finales angereichertes Dataset exportiert: data/processed/dataset_2025_enriched.csv\n",
      "   üìä Dateigr√∂√üe: 4,424 Zeilen x 15 Spalten\n",
      "\n",
      "üîç EXPORT-VALIDIERUNG\n",
      "==================================================\n",
      "Validiere Export durch Wiedereinlesen...\n",
      "‚úÖ Export-Validierung erfolgreich: 4,424 Zeilen geladen\n",
      "‚úÖ Alle erwarteten Spalten vorhanden\n",
      "\n",
      "üìã FINALES PROCESSING-SUMMARY: DATASET 2025\n",
      "============================================================\n",
      "üîÑ DATENVERARBEITUNGSPIPELINE:\n",
      "   1. Raw Dataset (geladen):           6,109 Zeilen\n",
      "   2. Nach Bereinigung & Normalisierung: 4,424 Zeilen\n",
      "   3. Nach Dual-Strategie-Anreicherung:  4,424 Zeilen\n",
      "\n",
      "üìâ DATENVERLUST-ANALYSE:\n",
      "   ‚Ä¢ Verlust durch Bereinigung: 1,685 Zeilen (27.6%)\n",
      "   ‚Ä¢ Verlust durch Anreicherung: 0 Zeilen (0.0%)\n",
      "   ‚Ä¢ Gesamtverlust: 1,685 Zeilen (27.6%)\n",
      "\n",
      "‚úÖ ANREICHERUNGSSTATISTIKEN:\n",
      "   ‚Ä¢ Erfolgreich angereichert: 4,422 von 4,424 Zeilen\n",
      "   ‚Ä¢ Anreicherungsrate: 100.0%\n",
      "   ‚Ä¢ Dual-Strategie erfolgreich: ‚úÖ JA\n",
      "\n",
      "üöÄ PIPELINE-INTEGRATION & N√ÑCHSTE SCHRITTE\n",
      "============================================================\n",
      "üìÅ AUSGABEDATEIEN:\n",
      "   ‚Ä¢ Normalisiert: data/processed/dataset_2025_normalized.csv\n",
      "   ‚Ä¢ Angereichert:  data/processed/dataset_2025_enriched.csv\n",
      "\n",
      "üîó BEREIT F√úR INTEGRATION:\n",
      "   ‚úÖ 04_Combine_Datasets.ipynb - Kombination aller Jahrg√§nge\n",
      "   ‚úÖ 05_Housing_Market_Analysis.ipynb - Marktanalyse\n",
      "   ‚úÖ 06_Geospatial_Analysis.ipynb - Geospatiale Visualisierung\n",
      "\n",
      "üéØ QUALIT√ÑTSSICHERUNG:\n",
      "   ‚úÖ Einheitliche Filter-Kriterien (100‚Ç¨-10.000‚Ç¨, 10m¬≤-500m¬≤)\n",
      "   ‚úÖ Dual-Strategie-Anreicherung implementiert\n",
      "   ‚úÖ Kartesische Produkte vermieden\n",
      "   ‚úÖ Standardisierte Spaltenstruktur\n",
      "   ‚úÖ Konsistenz mit anderen Datasets gew√§hrleistet\n",
      "\n",
      "üéâ DATASET 2025 PROCESSING ERFOLGREICH ABGESCHLOSSEN!\n",
      "    Ready for next pipeline step: 04_Combine_Datasets.ipynb\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPORT: FINALES ANGEREICHERTES DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===================================================================\n",
    "# EXPORT DES VOLLST√ÑNDIG ANGEREICHERTEN DATASETS\n",
    "# ===================================================================\n",
    "print(\"\\nüì§ EXPORT DES VOLLST√ÑNDIG ANGEREICHERTEN DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Export\n",
    "output_file_enriched = 'data/processed/dataset_2025_enriched.csv'\n",
    "df_enriched.to_csv(output_file_enriched, index=False)\n",
    "\n",
    "print(f\"‚úÖ Finales angereichertes Dataset exportiert: {output_file_enriched}\")\n",
    "print(f\"   üìä Dateigr√∂√üe: {len(df_enriched):,} Zeilen x {len(df_enriched.columns)} Spalten\")\n",
    "\n",
    "# ===================================================================\n",
    "# EXPORT-VALIDIERUNG\n",
    "# ===================================================================\n",
    "print(\"\\nüîç EXPORT-VALIDIERUNG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Validierung durch Wiedereinlesen\n",
    "print(\"Validiere Export durch Wiedereinlesen...\")\n",
    "test_df_enriched = pd.read_csv(output_file_enriched)\n",
    "print(f\"‚úÖ Export-Validierung erfolgreich: {len(test_df_enriched):,} Zeilen geladen\")\n",
    "\n",
    "# √úberpr√ºfe Spaltenintegrit√§t\n",
    "expected_columns = ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'ortsteil_neu', 'wol']\n",
    "missing_columns = [col for col in expected_columns if col not in test_df_enriched.columns]\n",
    "if missing_columns:\n",
    "    print(f\"‚ö†Ô∏è WARNUNG: Fehlende Spalten: {missing_columns}\")\n",
    "else:\n",
    "    print(\"‚úÖ Alle erwarteten Spalten vorhanden\")\n",
    "\n",
    "# ===================================================================\n",
    "# FINALES PROCESSING-SUMMARY\n",
    "# ===================================================================\n",
    "print(\"\\nüìã FINALES PROCESSING-SUMMARY: DATASET 2025\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Vergleiche mit urspr√ºnglichem Dataset\n",
    "original_count = len(df)  # Original raw dataset\n",
    "normalized_count = len(df_normalized)  # After normalization\n",
    "enriched_count = len(df_enriched)  # After enrichment\n",
    "\n",
    "print(f\"üîÑ DATENVERARBEITUNGSPIPELINE:\")\n",
    "print(f\"   1. Raw Dataset (geladen):           {original_count:,} Zeilen\")\n",
    "print(f\"   2. Nach Bereinigung & Normalisierung: {normalized_count:,} Zeilen\")\n",
    "print(f\"   3. Nach Dual-Strategie-Anreicherung:  {enriched_count:,} Zeilen\")\n",
    "\n",
    "# Berechne Verluste\n",
    "normalization_loss = original_count - normalized_count\n",
    "enrichment_loss = normalized_count - enriched_count\n",
    "total_loss = original_count - enriched_count\n",
    "\n",
    "print(f\"\\nüìâ DATENVERLUST-ANALYSE:\")\n",
    "print(f\"   ‚Ä¢ Verlust durch Bereinigung: {normalization_loss:,} Zeilen ({100*normalization_loss/original_count:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Verlust durch Anreicherung: {enrichment_loss:,} Zeilen ({100*enrichment_loss/normalized_count:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Gesamtverlust: {total_loss:,} Zeilen ({100*total_loss/original_count:.1f}%)\")\n",
    "\n",
    "# Anreicherungsstatistiken\n",
    "enrichment_success = df_enriched['ortsteil_neu'].notna().sum()\n",
    "enrichment_rate = enrichment_success / len(df_enriched) * 100\n",
    "\n",
    "print(f\"\\n‚úÖ ANREICHERUNGSSTATISTIKEN:\")\n",
    "print(f\"   ‚Ä¢ Erfolgreich angereichert: {enrichment_success:,} von {len(df_enriched):,} Zeilen\")\n",
    "print(f\"   ‚Ä¢ Anreicherungsrate: {enrichment_rate:.1f}%\")\n",
    "print(f\"   ‚Ä¢ Dual-Strategie erfolgreich: {'‚úÖ JA' if enrichment_rate >= 99.0 else '‚ö†Ô∏è √úBERPR√úFEN'}\")\n",
    "\n",
    "# ===================================================================\n",
    "# PIPELINE-INTEGRATION & N√ÑCHSTE SCHRITTE\n",
    "# ===================================================================\n",
    "print(\"\\nüöÄ PIPELINE-INTEGRATION & N√ÑCHSTE SCHRITTE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"üìÅ AUSGABEDATEIEN:\")\n",
    "print(f\"   ‚Ä¢ Normalisiert: data/processed/dataset_2025_normalized.csv\")\n",
    "print(f\"   ‚Ä¢ Angereichert:  data/processed/dataset_2025_enriched.csv\")\n",
    "\n",
    "print(f\"\\nüîó BEREIT F√úR INTEGRATION:\")\n",
    "print(f\"   ‚úÖ 04_Combine_Datasets.ipynb - Kombination aller Jahrg√§nge\")\n",
    "print(f\"   ‚úÖ 05_Housing_Market_Analysis.ipynb - Marktanalyse\")\n",
    "print(f\"   ‚úÖ 06_Geospatial_Analysis.ipynb - Geospatiale Visualisierung\")\n",
    "\n",
    "print(f\"\\nüéØ QUALIT√ÑTSSICHERUNG:\")\n",
    "print(f\"   ‚úÖ Einheitliche Filter-Kriterien (100‚Ç¨-10.000‚Ç¨, 10m¬≤-500m¬≤)\")\n",
    "print(f\"   ‚úÖ Dual-Strategie-Anreicherung implementiert\")\n",
    "print(f\"   ‚úÖ Kartesische Produkte vermieden\")\n",
    "print(f\"   ‚úÖ Standardisierte Spaltenstruktur\")\n",
    "print(f\"   ‚úÖ Konsistenz mit anderen Datasets gew√§hrleistet\")\n",
    "\n",
    "print(f\"\\nüéâ DATASET 2025 PROCESSING ERFOLGREICH ABGESCHLOSSEN!\")\n",
    "print(f\"    Ready for next pipeline step: 04_Combine_Datasets.ipynb\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
