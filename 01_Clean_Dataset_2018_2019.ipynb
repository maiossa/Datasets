{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70f42e6",
   "metadata": {},
   "source": [
    "# Dataset 2018-2019 Bereinigung und Normalisierung\n",
    "## Spezialisiertes Modul für Kaggle/Immobilienscout24 Dataset\n",
    "\n",
    "### Ziel\n",
    "Bereinigung und Normalisierung des historischen Datasets (2018-2019) in ein standardisiertes Format für die gemeinsame Analyse.\n",
    "\n",
    "### Input\n",
    "- `data/raw/Dataset_2018_2019.csv`\n",
    "\n",
    "### Output\n",
    "- `data/processed/dataset_2018_2019_normalized.csv`\n",
    "\n",
    "### Standardisierte Ausgabespalten\n",
    "- `price`: Normalisierter Preis (Kaltmiete in €)\n",
    "- `size`: Normalisierte Größe (m²)\n",
    "- `district`: Berliner Bezirk (standardisiert)\n",
    "- `rooms`: Anzahl Zimmer\n",
    "- `year`: Jahr des Datasets (2019)\n",
    "- `dataset_id`: Eindeutige Dataset-Kennzeichnung (historical)\n",
    "- `source`: Datenquelle\n",
    "\n",
    "---\n",
    "**Teil der modularen Preprocessing-Pipeline**  \n",
    "**Datum:** 4. Juli 2025  \n",
    "**Version:** 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ef7a1",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3a55584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotheken erfolgreich importiert!\n",
      "Pandas Version: 2.3.0\n",
      "Dataset: 2018-2019 (Kaggle/Immobilienscout24)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "print(\"Bibliotheken erfolgreich importiert!\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"Dataset: 2018-2019 (Kaggle/Immobilienscout24)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e7e8f",
   "metadata": {},
   "source": [
    "## 2. Daten laden und erste Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee850b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET 2018-2019 LADEN UND ANALYSIEREN\n",
      "============================================================\n",
      "Dataset geladen: 10,406 Zeilen, 9 Spalten\n",
      "\n",
      "Spalten: ['regio3', 'street', 'livingSpace', 'baseRent', 'totalRent', 'noRooms', 'floor', 'typeOfFlat', 'yearConstructed']\n",
      "\n",
      "Datentypen:\n",
      "regio3              object\n",
      "street              object\n",
      "livingSpace        float64\n",
      "baseRent           float64\n",
      "totalRent          float64\n",
      "noRooms            float64\n",
      "floor              float64\n",
      "typeOfFlat          object\n",
      "yearConstructed    float64\n",
      "dtype: object\n",
      "\n",
      "Fehlende Werte:\n",
      "  totalRent: 662 (6.36%)\n",
      "  floor: 1100 (10.57%)\n",
      "  typeOfFlat: 804 (7.73%)\n",
      "  yearConstructed: 1425 (13.69%)\n",
      "\n",
      "Erste 5 Zeilen:\n",
      "            regio3                      street  livingSpace  baseRent  totalRent  noRooms  floor    typeOfFlat  yearConstructed\n",
      "0  Staaken_Spandau           Metropolitan Park        77.00    820.00    1140.00      3.0    0.0  ground_floor              NaN\n",
      "1        Weißensee      B&ouml;rnestra&szlig;e        62.63    808.00     955.00      2.0    0.0  ground_floor           1918.0\n",
      "2            Mitte  Stallschreiberstra&szlig;e        46.40   1150.00    1300.00      2.0    3.0     apartment           2019.0\n",
      "3        Kreuzberg      Hallesche Stra&szlig;e        67.00   1200.00    1428.78      2.5    6.0     apartment           2017.0\n",
      "4       Tiergarten           Heidestra&szlig;e        73.54   1338.43    1559.05      2.0    0.0  ground_floor           2019.0\n"
     ]
    }
   ],
   "source": [
    "# Lade Dataset 2018-2019\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET 2018-2019 LADEN UND ANALYSIEREN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Lade Rohdaten\n",
    "df_raw = pd.read_csv('data/raw/Dataset_2018_2019.csv')\n",
    "print(f\"Dataset geladen: {df_raw.shape[0]:,} Zeilen, {df_raw.shape[1]} Spalten\")\n",
    "\n",
    "# Grundlegende Informationen\n",
    "print(f\"\\nSpalten: {list(df_raw.columns)}\")\n",
    "print(f\"\\nDatentypen:\")\n",
    "print(df_raw.dtypes)\n",
    "\n",
    "# Fehlende Werte\n",
    "print(f\"\\nFehlende Werte:\")\n",
    "missing_values = df_raw.isnull().sum()\n",
    "missing_pct = (missing_values / len(df_raw) * 100).round(2)\n",
    "for col in missing_values[missing_values > 0].index:\n",
    "    print(f\"  {col}: {missing_values[col]} ({missing_pct[col]}%)\")\n",
    "\n",
    "# Erste 5 Zeilen\n",
    "print(f\"\\nErste 5 Zeilen:\")\n",
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fadae8",
   "metadata": {},
   "source": [
    "## 3. Spezifische Bereinigung Dataset 2018-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f6ec445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPEZIFISCHE BEREINIGUNG DATASET 2018-2019\n",
      "============================================================\n",
      "Arbeitskopie erstellt: 10406 Zeilen\n",
      "\n",
      "=== PREIS-BEREINIGUNG ===\n",
      "baseRent - Statistik vor Bereinigung:\n",
      "  Typ: float64\n",
      "  Nicht-null Werte: 10406\n",
      "  Min: 0.0, Max: 20000.0\n",
      "Entfernte unrealistische Preise: 11\n",
      "\n",
      "=== GRÖSSEN-BEREINIGUNG ===\n",
      "livingSpace - Statistik vor Bereinigung:\n",
      "  Typ: float64\n",
      "  Nicht-null Werte: 10395\n",
      "  Min: 3.0, Max: 542.53\n",
      "Entfernte unrealistische Größen: 7\n",
      "\n",
      "=== BEZIRKS-NORMALISIERUNG ===\n",
      "regio3 - Einzigartige Werte: 79\n",
      "Bezirke: ['Adlershof_Treptow', 'Alt_Hohenschönhausen_Hohenschönhausen', 'Altglienicke_Treptow', 'Baumschulenweg_Treptow', 'Biesdorf_Marzahn', 'Blankenburg_Weißensee', 'Bohnsdorf_Treptow', 'Britz_Neukölln', 'Buch_Pankow', 'Buckow_Neukölln', 'Charlottenburg', 'Dahlem_Zehlendorf', 'Falkenberg_Hohenschönhausen', 'Französisch_Buchholz_Pankow', 'Friedenau_Schöneberg', 'Friedrichsfelde_Lichtenberg', 'Friedrichshagen_Köpenick', 'Friedrichshain', 'Frohnau_Reinickendorf', 'Gatow_Spandau', 'Grunewald_Wilmersdorf', 'Grünau_Köpenick', 'Haselhorst_Spandau', 'Heiligensee_Reinickendorf', 'Heinersdorf_Weißensee', 'Hellersdorf', 'Hermsdorf_Reinickendorf', 'Johannisthal_Treptow', 'Karlshorst_Lichtenberg', 'Karow_Weißensee', 'Kaulsdorf_Hellersdorf', 'Kladow_Spandau', 'Konradshöhe_Reinickendorf', 'Kreuzberg', 'Köpenick', 'Lankwitz_Steglitz', 'Lichtenberg', 'Lichtenrade_Tempelhof', 'Lichterfelde_Steglitz', 'Lübars_Reinickendorf', 'Mahlsdorf_Hellersdorf', 'Malchow_Hohenschönhausen', 'Mariendorf_Tempelhof', 'Marienfelde_Tempelhof', 'Marzahn', 'Mitte', 'Müggelheim_Köpenick', 'Neu_Hohenschönhausen_Hohenschönhausen', 'Neukölln', 'Niederschöneweide_Treptow', 'Niederschönhausen_Pankow', 'Nikolassee_Zehlendorf', 'Oberschöneweide_Köpenick', 'Pankow', 'Plänterwald_Treptow', 'Prenzlauer_Berg_Prenzlauer_Berg', 'Rahnsdorf_Köpenick', 'Reinickendorf', 'Rosenthal_Pankow', 'Rudow_Neukölln', 'Rummelsburg_Lichtenberg', 'Schmargendorf_Wilmersdorf', 'Schmöckwitz_Köpenick', 'Schöneberg', 'Siemensstadt_Spandau', 'Spandau', 'Staaken_Spandau', 'Steglitz', 'Tegel_Reinickendorf', 'Tempelhof', 'Tiergarten', 'Treptow', 'Waidmannslust_Reinickendorf', 'Wannsee_Zehlendorf', 'Wedding', 'Weißensee', 'Wilmersdorf', 'Wittenau_Reinickendorf', 'Zehlendorf']\n",
      "Normalisierte Bezirke: ['Adlershof', 'Alt', 'Altglienicke', 'Baumschulenweg', 'Biesdorf', 'Blankenburg', 'Bohnsdorf', 'Britz', 'Buch', 'Buckow', 'Charlottenburg', 'Dahlem', 'Falkenberg', 'Französisch', 'Friedenau', 'Friedrichsfelde', 'Friedrichshagen', 'Friedrichshain', 'Frohnau', 'Gatow', 'Grunewald', 'Grünau', 'Haselhorst', 'Heiligensee', 'Heinersdorf', 'Hellersdorf', 'Hermsdorf', 'Johannisthal', 'Karlshorst', 'Karow', 'Kaulsdorf', 'Kladow', 'Konradshöhe', 'Kreuzberg', 'Köpenick', 'Lankwitz', 'Lichtenberg', 'Lichtenrade', 'Lichterfelde', 'Lübars', 'Mahlsdorf', 'Malchow', 'Mariendorf', 'Marienfelde', 'Marzahn', 'Mitte', 'Müggelheim', 'Neu', 'Neukölln', 'Niederschöneweide', 'Niederschönhausen', 'Nikolassee', 'Oberschöneweide', 'Pankow', 'Plänterwald', 'Prenzlauer', 'Rahnsdorf', 'Reinickendorf', 'Rosenthal', 'Rudow', 'Rummelsburg', 'Schmargendorf', 'Schmöckwitz', 'Schöneberg', 'Siemensstadt', 'Spandau', 'Staaken', 'Steglitz', 'Tegel', 'Tempelhof', 'Tiergarten', 'Treptow', 'Waidmannslust', 'Wannsee', 'Wedding', 'Weißensee', 'Wilmersdorf', 'Wittenau', 'Zehlendorf']\n",
      "Anzahl normalisierte Bezirke: 79\n",
      "\n",
      "=== ZIMMER-BEREINIGUNG ===\n",
      "noRooms - Statistik:\n",
      "  Typ: float64\n",
      "  Nicht-null Werte: 10388\n",
      "  Einzigartige Werte: [np.float64(1.0), np.float64(1.1), np.float64(1.5), np.float64(2.0), np.float64(2.1), np.float64(2.2), np.float64(2.5), np.float64(3.0), np.float64(3.5), np.float64(4.0), np.float64(4.5), np.float64(5.0), np.float64(5.5), np.float64(6.0), np.float64(6.5), np.float64(7.0), np.float64(7.5), np.float64(8.0), np.float64(8.5), np.float64(9.0), np.float64(10.0), np.float64(11.0)]\n",
      "Entfernte unrealistische Zimmeranzahlen: 1\n",
      "Spezifische Bereinigung abgeschlossen\n",
      "Verbleibende Datensätze: 10387 (Verlust: 19)\n"
     ]
    }
   ],
   "source": [
    "# Spezifische Bereinigung für Dataset 2018-2019\n",
    "print(\"=\" * 60)\n",
    "print(\"SPEZIFISCHE BEREINIGUNG DATASET 2018-2019\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Erstelle Arbeitskopie\n",
    "df = df_raw.copy()\n",
    "print(f\"Arbeitskopie erstellt: {len(df)} Zeilen\")\n",
    "\n",
    "# 1. Preis-Bereinigung (baseRent)\n",
    "print(f\"\\n=== PREIS-BEREINIGUNG ===\")\n",
    "print(f\"baseRent - Statistik vor Bereinigung:\")\n",
    "print(f\"  Typ: {df['baseRent'].dtype}\")\n",
    "print(f\"  Nicht-null Werte: {df['baseRent'].notna().sum()}\")\n",
    "print(f\"  Min: {df['baseRent'].min()}, Max: {df['baseRent'].max()}\")\n",
    "\n",
    "# Preis ist bereits numerisch, nur Plausibilitätsprüfung\n",
    "# Entferne unrealistische Preise (< 100€ oder > 10.000€)\n",
    "original_count = len(df)\n",
    "df = df[(df['baseRent'] >= 100) & (df['baseRent'] <= 10000)]\n",
    "removed_price = original_count - len(df)\n",
    "print(f\"Entfernte unrealistische Preise: {removed_price}\")\n",
    "\n",
    "# 2. Größen-Bereinigung (livingSpace)\n",
    "print(f\"\\n=== GRÖSSEN-BEREINIGUNG ===\")\n",
    "print(f\"livingSpace - Statistik vor Bereinigung:\")\n",
    "print(f\"  Typ: {df['livingSpace'].dtype}\")\n",
    "print(f\"  Nicht-null Werte: {df['livingSpace'].notna().sum()}\")\n",
    "print(f\"  Min: {df['livingSpace'].min()}, Max: {df['livingSpace'].max()}\")\n",
    "\n",
    "# Größe ist bereits numerisch, nur Plausibilitätsprüfung\n",
    "# Entferne unrealistische Größen (< 10m² oder > 500m²)\n",
    "original_count = len(df)\n",
    "df = df[(df['livingSpace'] >= 10) & (df['livingSpace'] <= 500)]\n",
    "removed_size = original_count - len(df)\n",
    "print(f\"Entfernte unrealistische Größen: {removed_size}\")\n",
    "\n",
    "# 3. Bezirks-Normalisierung (regio3)\n",
    "print(f\"\\n=== BEZIRKS-NORMALISIERUNG ===\")\n",
    "print(f\"regio3 - Einzigartige Werte: {df['regio3'].nunique()}\")\n",
    "print(f\"Bezirke: {sorted(df['regio3'].unique())}\")\n",
    "\n",
    "# Bezirk-Normalisierung (entferne _Suffix)\n",
    "def normalize_district_2018_2019(district):\n",
    "    \"\"\"Normalisiert Bezirksnamen für Dataset 2018-2019\"\"\"\n",
    "    if pd.isna(district):\n",
    "        return None\n",
    "    \n",
    "    # Entferne Suffix nach Unterstrich\n",
    "    if '_' in str(district):\n",
    "        return str(district).split('_')[0]\n",
    "    \n",
    "    return str(district)\n",
    "\n",
    "df['district_normalized'] = df['regio3'].apply(normalize_district_2018_2019)\n",
    "\n",
    "print(f\"Normalisierte Bezirke: {sorted(df['district_normalized'].unique())}\")\n",
    "print(f\"Anzahl normalisierte Bezirke: {df['district_normalized'].nunique()}\")\n",
    "\n",
    "# 4. Zimmer-Bereinigung (noRooms)\n",
    "print(f\"\\n=== ZIMMER-BEREINIGUNG ===\")\n",
    "print(f\"noRooms - Statistik:\")\n",
    "print(f\"  Typ: {df['noRooms'].dtype}\")\n",
    "print(f\"  Nicht-null Werte: {df['noRooms'].notna().sum()}\")\n",
    "print(f\"  Einzigartige Werte: {sorted(df['noRooms'].dropna().unique())}\")\n",
    "\n",
    "# Zimmeranzahl ist bereits numerisch\n",
    "# Plausibilitätsprüfung (0.5 bis 10 Zimmer)\n",
    "original_count = len(df)\n",
    "df = df[(df['noRooms'] >= 0.5) & (df['noRooms'] <= 10)]\n",
    "removed_rooms = original_count - len(df)\n",
    "print(f\"Entfernte unrealistische Zimmeranzahlen: {removed_rooms}\")\n",
    "\n",
    "print(f\"Spezifische Bereinigung abgeschlossen\")\n",
    "print(f\"Verbleibende Datensätze: {len(df)} (Verlust: {len(df_raw) - len(df)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47813a15",
   "metadata": {},
   "source": [
    "## 4. Normalisierung in Standardformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86dc1e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NORMALISIERUNG IN STANDARDFORMAT\n",
      "============================================================\n",
      "Normalisiertes Dataset erstellt: 10387 Zeilen\n",
      "Standardspalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source']\n",
      "Zusätzliche Spalten: ['street', 'floor', 'typeOfFlat', 'yearConstructed', 'totalRent']\n",
      "=== DATENQUALITÄT NORMALISIERTES DATASET ===\n",
      "Zeilen mit Preis: 10387\n",
      "Zeilen mit Größe: 10387\n",
      "Zeilen mit Bezirk: 10387\n",
      "Zeilen mit Zimmeranzahl: 10387\n",
      "=== STATISTIKEN ===\n",
      "Preis - Min: 178.16€, Max: 9500.00€, Median: 945.00€\n",
      "Größe - Min: 10.0m², Max: 482.0m², Median: 72.0m²\n",
      "Zimmer - Min: 1.0, Max: 10.0, Median: 2.0\n",
      "=== BEZIRKSVERTEILUNG ===\n",
      "Anzahl Bezirke: 79\n",
      "  Mitte: 799 Einträge\n",
      "  Tiergarten: 768 Einträge\n",
      "  Charlottenburg: 701 Einträge\n",
      "  Friedrichshain: 553 Einträge\n",
      "  Prenzlauer: 473 Einträge\n",
      "  Spandau: 415 Einträge\n",
      "  Wedding: 397 Einträge\n",
      "  Wilmersdorf: 370 Einträge\n",
      "  Neukölln: 361 Einträge\n",
      "  Köpenick: 351 Einträge\n",
      "Normalisierung abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "# Normalisierung in Standardformat\n",
    "print(\"=\" * 60)\n",
    "print(\"NORMALISIERUNG IN STANDARDFORMAT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Erstelle normalisiertes Dataset mit Standardspalten\n",
    "df_normalized = pd.DataFrame()\n",
    "\n",
    "# Standardspalten zuweisen\n",
    "df_normalized['price'] = df['baseRent'].astype('float64')\n",
    "df_normalized['size'] = df['livingSpace'].astype('float64')\n",
    "df_normalized['district'] = df['district_normalized'].astype('string')\n",
    "df_normalized['rooms'] = df['noRooms'].astype('float64')\n",
    "df_normalized['year'] = 2019\n",
    "df_normalized['dataset_id'] = 'historical'\n",
    "df_normalized['source'] = 'Kaggle/Immobilienscout24'\n",
    "\n",
    "# Zusätzliche Spalten aus Original-Dataset beibehalten\n",
    "df_normalized['street'] = df['street']\n",
    "df_normalized['floor'] = df['floor']\n",
    "df_normalized['typeOfFlat'] = df['typeOfFlat']\n",
    "df_normalized['yearConstructed'] = df['yearConstructed']\n",
    "df_normalized['totalRent'] = df['totalRent']\n",
    "\n",
    "print(f\"Normalisiertes Dataset erstellt: {len(df_normalized)} Zeilen\")\n",
    "print(f\"Standardspalten: {['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source']}\")\n",
    "print(f\"Zusätzliche Spalten: {list(df_normalized.columns[7:])}\")\n",
    "\n",
    "# Datenqualität prüfen\n",
    "print(f\"=== DATENQUALITÄT NORMALISIERTES DATASET ===\")\n",
    "print(f\"Zeilen mit Preis: {df_normalized['price'].notna().sum()}\")\n",
    "print(f\"Zeilen mit Größe: {df_normalized['size'].notna().sum()}\")\n",
    "print(f\"Zeilen mit Bezirk: {df_normalized['district'].notna().sum()}\")\n",
    "print(f\"Zeilen mit Zimmeranzahl: {df_normalized['rooms'].notna().sum()}\")\n",
    "\n",
    "# Statistiken\n",
    "print(f\"=== STATISTIKEN ===\")\n",
    "print(f\"Preis - Min: {df_normalized['price'].min():.2f}€, Max: {df_normalized['price'].max():.2f}€, Median: {df_normalized['price'].median():.2f}€\")\n",
    "print(f\"Größe - Min: {df_normalized['size'].min():.1f}m², Max: {df_normalized['size'].max():.1f}m², Median: {df_normalized['size'].median():.1f}m²\")\n",
    "print(f\"Zimmer - Min: {df_normalized['rooms'].min():.1f}, Max: {df_normalized['rooms'].max():.1f}, Median: {df_normalized['rooms'].median():.1f}\")\n",
    "\n",
    "# Bezirksverteilung\n",
    "print(f\"=== BEZIRKSVERTEILUNG ===\")\n",
    "district_counts = df_normalized['district'].value_counts()\n",
    "print(f\"Anzahl Bezirke: {len(district_counts)}\")\n",
    "for district, count in district_counts.head(10).items():\n",
    "    print(f\"  {district}: {count} Einträge\")\n",
    "\n",
    "print(f\"Normalisierung abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db94d3",
   "metadata": {},
   "source": [
    "## 5. Export des normalisierten Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c728c108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT NORMALISIERTES DATASET\n",
      "============================================================\n",
      "✅ Normalisiertes Dataset exportiert: data/processed/dataset_2018_2019_normalized.csv\n",
      "Dateigröße: 10387 Zeilen x 12 Spalten\n",
      "✅ Export-Validierung erfolgreich: 10387 Zeilen geladen\n",
      "=== ZUSAMMENFASSUNG DATASET 2018-2019 ===\n",
      "Input: data/raw/Dataset_2018_2019.csv (10406 Zeilen)\n",
      "Output: data/processed/dataset_2018_2019_normalized.csv (10387 Zeilen)\n",
      "Datenverlust: 19 Zeilen (0.2%)\n",
      "Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\n",
      "Zusätzliche Spalten: 5\n",
      "🎯 DATASET 2018-2019 BEREINIGUNG ABGESCHLOSSEN!\n",
      "Bereit für Kombination mit anderen normalisierten Datasets.\n"
     ]
    }
   ],
   "source": [
    "# Export des normalisierten Datasets\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPORT NORMALISIERTES DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ausgabedatei\n",
    "output_file = 'data/processed/dataset_2018_2019_normalized.csv'\n",
    "\n",
    "# Export\n",
    "df_normalized.to_csv(output_file, index=False)\n",
    "print(f\"✅ Normalisiertes Dataset exportiert: {output_file}\")\n",
    "print(f\"Dateigröße: {len(df_normalized)} Zeilen x {len(df_normalized.columns)} Spalten\")\n",
    "\n",
    "# Validierung des Exports\n",
    "test_load = pd.read_csv(output_file)\n",
    "print(f\"✅ Export-Validierung erfolgreich: {len(test_load)} Zeilen geladen\")\n",
    "\n",
    "# Zusammenfassung\n",
    "print(f\"=== ZUSAMMENFASSUNG DATASET 2018-2019 ===\")\n",
    "print(f\"Input: data/raw/Dataset_2018_2019.csv ({len(df_raw)} Zeilen)\")\n",
    "print(f\"Output: {output_file} ({len(df_normalized)} Zeilen)\")\n",
    "print(f\"Datenverlust: {len(df_raw) - len(df_normalized)} Zeilen ({((len(df_raw) - len(df_normalized))/len(df_raw)*100):.1f}%)\")\n",
    "print(f\"Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\")\n",
    "print(f\"Zusätzliche Spalten: {len(df_normalized.columns) - 7}\")\n",
    "\n",
    "print(f\"🎯 DATASET 2018-2019 BEREINIGUNG ABGESCHLOSSEN!\")\n",
    "print(f\"Bereit für Kombination mit anderen normalisierten Datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "## 6. Lade angereicherte Wohnlagendaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f6g7h8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANGEREICHERTE WOHNLAGENDATEN LADEN\n",
      "============================================================\n",
      "✅ Angereicherte Daten geladen: 551,249 Zeilen, 11 Spalten\n",
      "✅ Angereicherte Daten geladen: 551,249 Zeilen, 11 Spalten\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ANGEREICHERTE WOHNLAGENDATEN LADEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "enriched_data_path = 'data/raw/wohnlagen_enriched.csv'\n",
    "try:\n",
    "    enriched_df = pd.read_csv(enriched_data_path)\n",
    "    print(f\"✅ Angereicherte Daten geladen: {len(enriched_df):,} Zeilen, {len(enriched_df.columns)} Spalten\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Datei nicht gefunden: {enriched_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "## 7. Kombiniere Datasets mit Wohnlagendaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "m3n4o5p6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KOMBINIERE MIT WOHNLAGENDATEN\n",
      "============================================================\n",
      "Original df_normalized: 10,387 Zeilen\n",
      "Original enriched_df: 551,249 Zeilen\n",
      "PLZ verfügbar: 9,505/10,387 (91.5%)\n",
      "Unique street mappings: 9,479 Zeilen\n",
      "Street overlap: 533 von 3267 Straßen im Dataset\n",
      "✅ Kombiniertes und angereichertes Dataset erstellt: 10,387 Zeilen\n",
      "Erfolgreiche Anreicherung: 1,760 von 10,387 Zeilen (16.9%)\n",
      "\n",
      "Finale Spalten im angereicherten Dataset:\n",
      "  ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'street', 'floor', 'typeOfFlat', 'yearConstructed', 'totalRent', 'plz', 'wol', 'ortsteil_neu']\n",
      "PLZ im finalen Dataset: 9,505/10,387 (91.5%)\n",
      "Top 5 PLZ im finalen Dataset:\n",
      "  10179: 762 Einträge\n",
      "  10557: 721 Einträge\n",
      "  14055: 567 Einträge\n",
      "  12555: 498 Einträge\n",
      "  10247: 424 Einträge\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"KOMBINIERE MIT WOHNLAGENDATEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Debug: Check original data sizes\n",
    "print(f\"Original df_normalized: {len(df_normalized):,} Zeilen\")\n",
    "print(f\"Original enriched_df: {len(enriched_df):,} Zeilen\")\n",
    "\n",
    "# Stelle sicher, dass PLZ-Spalte vorhanden ist\n",
    "if 'plz' not in df_normalized.columns:\n",
    "    print(\"❌ PLZ-Spalte fehlt! Führe PLZ-Extraktion zuerst aus.\")\n",
    "    df_normalized['plz'] = None\n",
    "else:\n",
    "    plz_available = df_normalized['plz'].notna().sum()\n",
    "    print(f\"PLZ verfügbar: {plz_available:,}/{len(df_normalized):,} ({plz_available/len(df_normalized)*100:.1f}%)\")\n",
    "\n",
    "# Create a unique mapping to avoid cartesian product - use only unique street names\n",
    "enriched_df_subset = enriched_df[['strasse', 'wol', 'ortsteil_neu']].drop_duplicates(subset=['strasse'])\n",
    "print(f\"Unique street mappings: {len(enriched_df_subset):,} Zeilen\")\n",
    "\n",
    "# Debug: Check street overlap\n",
    "df_streets_unique = set(df_normalized['street'].unique())\n",
    "enriched_streets_unique = set(enriched_df_subset['strasse'].unique())\n",
    "overlap = df_streets_unique.intersection(enriched_streets_unique)\n",
    "print(f\"Street overlap: {len(overlap)} von {len(df_streets_unique)} Straßen im Dataset\")\n",
    "\n",
    "# Perform the merge (mit PLZ)\n",
    "df_enriched = pd.merge(df_normalized, enriched_df_subset, how='left', left_on=['street'], right_on=['strasse'])\n",
    "\n",
    "# Bereinige überflüssige Spalten\n",
    "if 'strasse' in df_enriched.columns:\n",
    "    df_enriched = df_enriched.drop('strasse', axis=1)\n",
    "\n",
    "print(f\"✅ Kombiniertes und angereichertes Dataset erstellt: {len(df_enriched):,} Zeilen\")\n",
    "\n",
    "# Check merge success\n",
    "successful_enrichment = df_enriched['ortsteil_neu'].notna().sum()\n",
    "print(f\"Erfolgreiche Anreicherung: {successful_enrichment:,} von {len(df_enriched):,} Zeilen ({successful_enrichment/len(df_enriched)*100:.1f}%)\")\n",
    "\n",
    "# Finale Spalten-Zusammenfassung\n",
    "print(f\"\\nFinale Spalten im angereicherten Dataset:\")\n",
    "print(f\"  {list(df_enriched.columns)}\")\n",
    "\n",
    "# PLZ-Qualität prüfen\n",
    "if 'plz' in df_enriched.columns:\n",
    "    final_plz_count = df_enriched['plz'].notna().sum()\n",
    "    print(f\"PLZ im finalen Dataset: {final_plz_count:,}/{len(df_enriched):,} ({final_plz_count/len(df_enriched)*100:.1f}%)\")\n",
    "    \n",
    "    if final_plz_count > 0:\n",
    "        print(\"Top 5 PLZ im finalen Dataset:\")\n",
    "        for plz, count in df_enriched['plz'].value_counts().head(5).items():\n",
    "            print(f\"  {plz}: {count:,} Einträge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1498f",
   "metadata": {},
   "source": [
    "## 7.5. PLZ-Extraktion für 2018-2019 Dataset\n",
    "\n",
    "**🎯 Problem:** Das 2018-2019 Dataset enthält keine PLZ-Spalte, nur `regio3` (Ortsteil) und `street`.\n",
    "\n",
    "**🔧 Lösung:** Wir extrahieren die PLZ durch Matching mit der `wohnlagen_enriched.csv`:\n",
    "1. **Strategie 1:** Ortsteil-basiert (regio3 → PLZ)\n",
    "2. **Strategie 2:** Street-basiert (street → PLZ) \n",
    "3. **Fallback:** Bezirk-basiert\n",
    "\n",
    "Dies ist essentiell für die spätere PLZ-Enhancement-Pipeline und räumliche Genauigkeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6d233e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PLZ-EXTRAKTION FÜR 2018-2019 DATASET\n",
      "============================================================\n",
      "df_normalized Spalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'street', 'floor', 'typeOfFlat', 'yearConstructed', 'totalRent', 'plz']\n",
      "df_raw Spalten: ['regio3', 'street', 'livingSpace', 'baseRent', 'totalRent', 'noRooms', 'floor', 'typeOfFlat', 'yearConstructed']\n",
      "❌ Längen-Mismatch: df_normalized=10387, df_raw=10406\n",
      "\n",
      "=== STRATEGIE 1: ORTSTEIL-BASIERTE PLZ-EXTRAKTION ===\n",
      "Ortsteil-PLZ-Mapping erstellt: 91 Einträge\n",
      "Beispiele:\n",
      "  Halensee → 10713\n",
      "  Hakenfelde → 13587\n",
      "  Lichterfelde → 12209\n",
      "  Charlottenburg → 14055\n",
      "  Marienfelde → 12307\n",
      "\n",
      "=== STRATEGIE 2: STREET-BASIERTE PLZ-EXTRAKTION ===\n",
      "Ortsteil-PLZ-Mapping erstellt: 91 Einträge\n",
      "Beispiele:\n",
      "  Halensee → 10713\n",
      "  Hakenfelde → 13587\n",
      "  Lichterfelde → 12209\n",
      "  Charlottenburg → 14055\n",
      "  Marienfelde → 12307\n",
      "\n",
      "=== STRATEGIE 2: STREET-BASIERTE PLZ-EXTRAKTION ===\n",
      "Street-PLZ-Mapping erstellt: 9479 Einträge\n",
      "Beispiele:\n",
      "  Aachener Straße → 10713\n",
      "  Aalemannufer → 13587\n",
      "  Aarauer Straße → 12205\n",
      "  Aarberger Straße → 12205\n",
      "  Abbestraße → 10587\n",
      "\n",
      "=== REGIO3-NORMALISIERUNG ===\n",
      "❌ regio3 Spalte nicht verfügbar\n",
      "\n",
      "=== PLZ-EXTRAKTION ANWENDEN ===\n",
      "Street-PLZ-Mapping erstellt: 9479 Einträge\n",
      "Beispiele:\n",
      "  Aachener Straße → 10713\n",
      "  Aalemannufer → 13587\n",
      "  Aarauer Straße → 12205\n",
      "  Aarberger Straße → 12205\n",
      "  Abbestraße → 10587\n",
      "\n",
      "=== REGIO3-NORMALISIERUNG ===\n",
      "❌ regio3 Spalte nicht verfügbar\n",
      "\n",
      "=== PLZ-EXTRAKTION ANWENDEN ===\n",
      "PLZ-Extraktion Ergebnisse:\n",
      "  Ortsteil-basiert: 0 Matches\n",
      "  Street-basiert: 1,760 Matches\n",
      "  Total mit PLZ: 1,760/10,387 (16.9%)\n",
      "\n",
      "Top 10 PLZ im 2018-2019 Dataset:\n",
      "  10245: 63 Einträge\n",
      "  13158: 54 Einträge\n",
      "  10243: 51 Einträge\n",
      "  13125: 44 Einträge\n",
      "  13593: 41 Einträge\n",
      "  12555: 41 Einträge\n",
      "  12277: 37 Einträge\n",
      "  10315: 36 Einträge\n",
      "  12683: 35 Einträge\n",
      "  13629: 34 Einträge\n",
      "\n",
      "✅ PLZ-Extraktion für 2018-2019 Dataset abgeschlossen!\n",
      "PLZ-Extraktion Ergebnisse:\n",
      "  Ortsteil-basiert: 0 Matches\n",
      "  Street-basiert: 1,760 Matches\n",
      "  Total mit PLZ: 1,760/10,387 (16.9%)\n",
      "\n",
      "Top 10 PLZ im 2018-2019 Dataset:\n",
      "  10245: 63 Einträge\n",
      "  13158: 54 Einträge\n",
      "  10243: 51 Einträge\n",
      "  13125: 44 Einträge\n",
      "  13593: 41 Einträge\n",
      "  12555: 41 Einträge\n",
      "  12277: 37 Einträge\n",
      "  10315: 36 Einträge\n",
      "  12683: 35 Einträge\n",
      "  13629: 34 Einträge\n",
      "\n",
      "✅ PLZ-Extraktion für 2018-2019 Dataset abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PLZ-EXTRAKTION FÜR 2018-2019 DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Prüfe verfügbare Spalten\n",
    "print(f\"df_normalized Spalten: {list(df_normalized.columns)}\")\n",
    "print(f\"df_raw Spalten: {list(df_raw.columns)}\")\n",
    "\n",
    "# Da regio3 nicht in df_normalized ist, hole es aus df_raw\n",
    "# Erstelle einen temporären DataFrame für die PLZ-Extraktion\n",
    "temp_df = df_normalized.copy()\n",
    "\n",
    "# Füge regio3 aus df_raw hinzu (basierend auf Index)\n",
    "if len(temp_df) == len(df_raw):\n",
    "    temp_df['regio3'] = df_raw['regio3'].values\n",
    "    print(f\"✅ regio3 Spalte hinzugefügt: {temp_df['regio3'].nunique()} einzigartige Werte\")\n",
    "else:\n",
    "    print(f\"❌ Längen-Mismatch: df_normalized={len(temp_df)}, df_raw={len(df_raw)}\")\n",
    "\n",
    "# Strategie 1: Ortsteil-basierte PLZ-Extraktion\n",
    "print(\"\\n=== STRATEGIE 1: ORTSTEIL-BASIERTE PLZ-EXTRAKTION ===\")\n",
    "\n",
    "# Erstelle Ortsteil-zu-PLZ-Mapping aus wohnlagen_enriched.csv\n",
    "ortsteil_plz_mapping = {}\n",
    "if 'enriched_df' in locals():\n",
    "    # Extrahiere einzigartige Ortsteil-PLZ-Kombinationen\n",
    "    ortsteil_plz_pairs = enriched_df[['ortsteil_neu', 'plz']].dropna().drop_duplicates()\n",
    "    \n",
    "    # Für Ortsteile mit mehreren PLZ, nehme die häufigste\n",
    "    for ortsteil in ortsteil_plz_pairs['ortsteil_neu'].unique():\n",
    "        plz_counts = enriched_df[enriched_df['ortsteil_neu'] == ortsteil]['plz'].value_counts()\n",
    "        if len(plz_counts) > 0:\n",
    "            most_common_plz = plz_counts.index[0]\n",
    "            ortsteil_plz_mapping[ortsteil] = most_common_plz\n",
    "    \n",
    "    print(f\"Ortsteil-PLZ-Mapping erstellt: {len(ortsteil_plz_mapping)} Einträge\")\n",
    "    \n",
    "    # Zeige einige Beispiele\n",
    "    print(\"Beispiele:\")\n",
    "    for ortsteil, plz in list(ortsteil_plz_mapping.items())[:5]:\n",
    "        print(f\"  {ortsteil} → {plz}\")\n",
    "\n",
    "# Strategie 2: Street-basierte PLZ-Extraktion\n",
    "print(\"\\n=== STRATEGIE 2: STREET-BASIERTE PLZ-EXTRAKTION ===\")\n",
    "\n",
    "# Erstelle Street-zu-PLZ-Mapping\n",
    "street_plz_mapping = {}\n",
    "if 'enriched_df' in locals():\n",
    "    # Extrahiere einzigartige Street-PLZ-Kombinationen\n",
    "    street_plz_pairs = enriched_df[['strasse', 'plz']].dropna().drop_duplicates()\n",
    "    \n",
    "    # Für Straßen mit mehreren PLZ, nehme die häufigste\n",
    "    for street in street_plz_pairs['strasse'].unique():\n",
    "        plz_counts = enriched_df[enriched_df['strasse'] == street]['plz'].value_counts()\n",
    "        if len(plz_counts) > 0:\n",
    "            most_common_plz = plz_counts.index[0]\n",
    "            street_plz_mapping[street] = most_common_plz\n",
    "    \n",
    "    print(f\"Street-PLZ-Mapping erstellt: {len(street_plz_mapping)} Einträge\")\n",
    "    \n",
    "    # Zeige einige Beispiele\n",
    "    print(\"Beispiele:\")\n",
    "    for street, plz in list(street_plz_mapping.items())[:5]:\n",
    "        print(f\"  {street} → {plz}\")\n",
    "\n",
    "# Normalisiere regio3 für besseres Matching\n",
    "print(\"\\n=== REGIO3-NORMALISIERUNG ===\")\n",
    "\n",
    "def normalize_regio3(regio3_value):\n",
    "    \"\"\"Normalisiert regio3 Werte für besseres Ortsteil-Matching.\"\"\"\n",
    "    if pd.isna(regio3_value):\n",
    "        return None\n",
    "    \n",
    "    # Konvertiere zu String und bereinige\n",
    "    normalized = str(regio3_value).strip()\n",
    "    \n",
    "    # Entferne Bezirk-Suffix (z.B. \"Prenzlauer_Berg_Prenzlauer_Berg\" → \"Prenzlauer Berg\")\n",
    "    if '_' in normalized:\n",
    "        parts = normalized.split('_')\n",
    "        # Nehme den ersten Teil oder entferne Duplikate\n",
    "        if len(parts) == 2 and parts[0] == parts[1]:\n",
    "            normalized = parts[0]\n",
    "        else:\n",
    "            normalized = parts[0]\n",
    "    \n",
    "    # Ersetze Underscores durch Spaces\n",
    "    normalized = normalized.replace('_', ' ')\n",
    "    \n",
    "    # Spezielle Mappings für bekannte Varianten\n",
    "    mappings = {\n",
    "        'Neu Hohenschönhausen': 'Neu-Hohenschönhausen',\n",
    "        'Hohenschönhausen': 'Alt-Hohenschönhausen',\n",
    "        'Französisch Buchholz': 'Französisch Buchholz',\n",
    "        'Grünau': 'Grünau',\n",
    "        'Köpenick': 'Köpenick'\n",
    "    }\n",
    "    \n",
    "    return mappings.get(normalized, normalized)\n",
    "\n",
    "# Teste die Normalisierung (verwende temp_df mit regio3)\n",
    "if 'regio3' in temp_df.columns:\n",
    "    temp_df['regio3_normalized'] = temp_df['regio3'].apply(normalize_regio3)\n",
    "    \n",
    "    print(\"Regio3 Normalisierung - Beispiele:\")\n",
    "    regio3_examples = temp_df[['regio3', 'regio3_normalized']].drop_duplicates().head(10)\n",
    "    for _, row in regio3_examples.iterrows():\n",
    "        print(f\"  {row['regio3']} → {row['regio3_normalized']}\")\n",
    "    \n",
    "    print(f\"\\nUnikat regio3 (original): {temp_df['regio3'].nunique()}\")\n",
    "    print(f\"Unikat regio3 (normalized): {temp_df['regio3_normalized'].nunique()}\")\n",
    "else:\n",
    "    print(\"❌ regio3 Spalte nicht verfügbar\")\n",
    "\n",
    "# Wende PLZ-Extraktion an\n",
    "print(\"\\n=== PLZ-EXTRAKTION ANWENDEN ===\")\n",
    "\n",
    "# Initialisiere PLZ-Spalte\n",
    "temp_df['plz'] = None\n",
    "\n",
    "# Strategie 1: Ortsteil-basiert\n",
    "ortsteil_matches = 0\n",
    "if 'regio3_normalized' in temp_df.columns:\n",
    "    for idx, row in temp_df.iterrows():\n",
    "        normalized_ortsteil = row['regio3_normalized']\n",
    "        if normalized_ortsteil in ortsteil_plz_mapping:\n",
    "            temp_df.loc[idx, 'plz'] = ortsteil_plz_mapping[normalized_ortsteil]\n",
    "            ortsteil_matches += 1\n",
    "\n",
    "# Strategie 2: Street-basiert (für fehlende PLZ)\n",
    "street_matches = 0\n",
    "for idx, row in temp_df.iterrows():\n",
    "    if pd.isna(row['plz']) and row['street'] in street_plz_mapping:\n",
    "        temp_df.loc[idx, 'plz'] = street_plz_mapping[row['street']]\n",
    "        street_matches += 1\n",
    "\n",
    "# Übertrage PLZ zurück zu df_normalized\n",
    "df_normalized['plz'] = temp_df['plz']\n",
    "\n",
    "# Ergebnisse\n",
    "total_with_plz = df_normalized['plz'].notna().sum()\n",
    "print(f\"PLZ-Extraktion Ergebnisse:\")\n",
    "print(f\"  Ortsteil-basiert: {ortsteil_matches:,} Matches\")\n",
    "print(f\"  Street-basiert: {street_matches:,} Matches\")\n",
    "print(f\"  Total mit PLZ: {total_with_plz:,}/{len(df_normalized):,} ({total_with_plz/len(df_normalized)*100:.1f}%)\")\n",
    "\n",
    "# Zeige PLZ-Verteilung\n",
    "if total_with_plz > 0:\n",
    "    print(f\"\\nTop 10 PLZ im 2018-2019 Dataset:\")\n",
    "    plz_counts = df_normalized['plz'].value_counts().head(10)\n",
    "    for plz, count in plz_counts.items():\n",
    "        print(f\"  {plz}: {count:,} Einträge\")\n",
    "\n",
    "print(f\"\\n✅ PLZ-Extraktion für 2018-2019 Dataset abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f89a0211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 ERWEITERTE PLZ-EXTRAKTION FÜR 2018-2019\n",
      "============================================================\n",
      "🧪 TESTE ERWEITERTE PLZ-EXTRAKTION\n",
      "==================================================\n",
      "Test der erweiterten PLZ-Extraktion:\n",
      "   'Unter den Linden' in 'Mitte' → PLZ: 10117\n",
      "   'Alexanderplatz' in 'Mitte' → PLZ: 10178\n",
      "   'Boxhagener Straße' in 'Friedrichshain' → PLZ: 10245\n",
      "   'Kurfürstendamm' in 'Charlottenburg' → PLZ: 10719\n",
      "   'Sonnenallee' in 'Neukölln' → PLZ: 12437\n",
      "   'Hauptstraße' in 'Steglitz' → PLZ: 13158\n",
      "   'Müllerstraße' in 'Wedding' → PLZ: 13353\n",
      "\n",
      "🔄 WENDE ERWEITERTE PLZ-EXTRAKTION AN\n",
      "==================================================\n",
      "Verfügbare Spalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'street', 'floor', 'typeOfFlat', 'yearConstructed', 'totalRent', 'plz']\n",
      "✅ Erweiterte PLZ-Extraktion Ergebnisse:\n",
      "   📊 Nur erweiterte Methode: 9,505 von 10,387 (91.5%)\n",
      "   📊 Kombiniert (alt + neu): 9,505 von 10,387 (91.5%)\n",
      "   📈 Verbesserung: +7,745 PLZ (440.1% mehr)\n",
      "   🎉 ZIEL ERREICHT: >80% PLZ-Abdeckung!\n",
      "\n",
      "📋 Verbesserte PLZ-Verteilung (Top 10):\n",
      "   10179: 762 Einträge\n",
      "   10557: 721 Einträge\n",
      "   14055: 567 Einträge\n",
      "   12555: 498 Einträge\n",
      "   10247: 424 Einträge\n",
      "   13347: 394 Einträge\n",
      "   13591: 374 Einträge\n",
      "   14197: 354 Einträge\n",
      "   10965: 313 Einträge\n",
      "   10997: 299 Einträge\n",
      "\n",
      "✅ Erweiterte PLZ-Extraktion für 2018-2019 Dataset abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# ERWEITERTE PLZ-EXTRAKTION FÜR 2018-2019 DATASET\n",
    "# ===================================================================\n",
    "print(\"\\n🔍 ERWEITERTE PLZ-EXTRAKTION FÜR 2018-2019\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def extract_plz_advanced_2018_2019(street, district, ortsteil_mapping=None, street_mapping=None):\n",
    "    \"\"\"\n",
    "    Erweiterte PLZ-Extraktion für 2018-2019 Dataset mit mehreren Fallback-Strategien\n",
    "    \n",
    "    Args:\n",
    "        street: Straßenname\n",
    "        district: Bezirk\n",
    "        ortsteil_mapping: Ortsteil-zu-PLZ-Mapping\n",
    "        street_mapping: Street-zu-PLZ-Mapping\n",
    "    \n",
    "    Returns:\n",
    "        str: PLZ oder None\n",
    "    \"\"\"\n",
    "    # Strategie 1: Street-basierte PLZ-Extraktion (falls vorhanden)\n",
    "    if pd.notna(street) and street_mapping and street in street_mapping:\n",
    "        return str(street_mapping[street])\n",
    "    \n",
    "    # Strategie 2: Bezirk-zu-PLZ-Mapping (erweitert)\n",
    "    if pd.notna(district) and ortsteil_mapping:\n",
    "        district_str = str(district).strip()\n",
    "        \n",
    "        # Direkte Bezirks-Zuordnung\n",
    "        if district_str in ortsteil_mapping:\n",
    "            return str(ortsteil_mapping[district_str])\n",
    "        \n",
    "        # Erweiterte Bezirks-Aliases\n",
    "        district_aliases = {\n",
    "            # Hauptbezirke\n",
    "            'Mitte': 'Mitte',\n",
    "            'Friedrichshain-Kreuzberg': 'Friedrichshain',\n",
    "            'Pankow': 'Pankow',\n",
    "            'Charlottenburg-Wilmersdorf': 'Charlottenburg',\n",
    "            'Spandau': 'Spandau',\n",
    "            'Steglitz-Zehlendorf': 'Steglitz',\n",
    "            'Tempelhof-Schöneberg': 'Tempelhof',\n",
    "            'Neukölln': 'Neukölln',\n",
    "            'Treptow-Köpenick': 'Treptow',\n",
    "            'Marzahn-Hellersdorf': 'Marzahn',\n",
    "            'Lichtenberg': 'Lichtenberg',\n",
    "            'Reinickendorf': 'Reinickendorf',\n",
    "            # Einzelteile zusammengesetzter Bezirke\n",
    "            'Friedrichshain': 'Friedrichshain',\n",
    "            'Kreuzberg': 'Kreuzberg',\n",
    "            'Charlottenburg': 'Charlottenburg',\n",
    "            'Wilmersdorf': 'Wilmersdorf',\n",
    "            'Steglitz': 'Steglitz',\n",
    "            'Zehlendorf': 'Zehlendorf',\n",
    "            'Tempelhof': 'Tempelhof',\n",
    "            'Schöneberg': 'Schöneberg',\n",
    "            'Treptow': 'Treptow',\n",
    "            'Köpenick': 'Köpenick',\n",
    "            'Marzahn': 'Marzahn',\n",
    "            'Hellersdorf': 'Hellersdorf'\n",
    "        }\n",
    "        \n",
    "        # Prüfe Aliases\n",
    "        for alias, canonical in district_aliases.items():\n",
    "            if alias.lower() in district_str.lower():\n",
    "                if canonical in ortsteil_mapping:\n",
    "                    return str(ortsteil_mapping[canonical])\n",
    "    \n",
    "    # Strategie 3: Häufige Straßenname-Patterns\n",
    "    if pd.notna(street):\n",
    "        street_str = str(street).strip()\n",
    "        \n",
    "        # Bekannte Straßen-zu-PLZ-Mapping (häufigste Berliner Straßen)\n",
    "        common_streets = {\n",
    "            'Unter den Linden': '10117',\n",
    "            'Alexanderplatz': '10178',\n",
    "            'Potsdamer Platz': '10785',\n",
    "            'Kurfürstendamm': '10719',\n",
    "            'Friedrichstraße': '10117',\n",
    "            'Hackescher Markt': '10178',\n",
    "            'Warschauer Straße': '10243',\n",
    "            'Boxhagener Straße': '10245',\n",
    "            'Kastanienallee': '10435',\n",
    "            'Oranienstraße': '10999',\n",
    "            'Bergmannstraße': '10961',\n",
    "            'Savignyplatz': '10623',\n",
    "            'Rosenthaler Straße': '10119',\n",
    "            'Torstraße': '10119',\n",
    "            'Invalidenstraße': '10115',\n",
    "            'Chausseestraße': '10115',\n",
    "            'Brunnenstraße': '10119',\n",
    "            'Bernauer Straße': '10119',\n",
    "            'Prenzlauer Allee': '10405',\n",
    "            'Karl-Marx-Allee': '10243',\n",
    "            'Frankfurter Allee': '10247',\n",
    "            'Sonnenallee': '12047',\n",
    "            'Hermannstraße': '12049',\n",
    "            'Kantstraße': '10623',\n",
    "            'Wilmersdorfer Straße': '10627',\n",
    "            'Uhlandstraße': '10623',\n",
    "            'Ku\\'damm': '10719',\n",
    "            'Tauentzienstraße': '10789',\n",
    "            'Nollendorfplatz': '10777',\n",
    "            'Wittenbergplatz': '10789'\n",
    "        }\n",
    "        \n",
    "        # Prüfe auf bekannte Straßen (auch Teilstrings)\n",
    "        for known_street, plz in common_streets.items():\n",
    "            if known_street.lower() in street_str.lower():\n",
    "                return plz\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Teste die erweiterte PLZ-Extraktion\n",
    "print(\"🧪 TESTE ERWEITERTE PLZ-EXTRAKTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_cases = [\n",
    "    (\"Unter den Linden\", \"Mitte\"),\n",
    "    (\"Alexanderplatz\", \"Mitte\"),\n",
    "    (\"Boxhagener Straße\", \"Friedrichshain\"),\n",
    "    (\"Kurfürstendamm\", \"Charlottenburg\"),\n",
    "    (\"Sonnenallee\", \"Neukölln\"),\n",
    "    (\"Hauptstraße\", \"Steglitz\"),\n",
    "    (\"Müllerstraße\", \"Wedding\")\n",
    "]\n",
    "\n",
    "print(\"Test der erweiterten PLZ-Extraktion:\")\n",
    "for street, district in test_cases:\n",
    "    plz = extract_plz_advanced_2018_2019(street, district, ortsteil_plz_mapping, street_plz_mapping)\n",
    "    print(f\"   '{street}' in '{district}' → PLZ: {plz}\")\n",
    "\n",
    "# Erstelle erweiterte PLZ-Extraktion für alle Daten\n",
    "print(\"\\n🔄 WENDE ERWEITERTE PLZ-EXTRAKTION AN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Verwende die verfügbaren Spalten\n",
    "available_columns = df_normalized.columns.tolist()\n",
    "print(f\"Verfügbare Spalten: {available_columns}\")\n",
    "\n",
    "# Wende erweiterte PLZ-Extraktion an\n",
    "df_normalized['plz_advanced'] = df_normalized.apply(\n",
    "    lambda row: extract_plz_advanced_2018_2019(\n",
    "        row.get('street', None),\n",
    "        row.get('district', None),\n",
    "        ortsteil_plz_mapping,\n",
    "        street_plz_mapping\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Kombiniere alte und neue PLZ-Extraktion\n",
    "df_normalized['plz_combined'] = df_normalized['plz_advanced'].fillna(df_normalized['plz'])\n",
    "\n",
    "# Ergebnisse der erweiterten PLZ-Extraktion\n",
    "plz_advanced_found = df_normalized['plz_advanced'].notna().sum()\n",
    "plz_combined_found = df_normalized['plz_combined'].notna().sum()\n",
    "total_rows = len(df_normalized)\n",
    "\n",
    "print(f\"✅ Erweiterte PLZ-Extraktion Ergebnisse:\")\n",
    "print(f\"   📊 Nur erweiterte Methode: {plz_advanced_found:,} von {total_rows:,} ({plz_advanced_found/total_rows*100:.1f}%)\")\n",
    "print(f\"   📊 Kombiniert (alt + neu): {plz_combined_found:,} von {total_rows:,} ({plz_combined_found/total_rows*100:.1f}%)\")\n",
    "\n",
    "# Verbesserung berechnen\n",
    "old_plz_count = df_normalized['plz'].notna().sum()\n",
    "improvement = plz_combined_found - old_plz_count\n",
    "improvement_pct = (improvement / old_plz_count) * 100 if old_plz_count > 0 else 0\n",
    "\n",
    "print(f\"   📈 Verbesserung: +{improvement:,} PLZ ({improvement_pct:.1f}% mehr)\")\n",
    "\n",
    "if plz_combined_found / total_rows >= 0.8:\n",
    "    print(f\"   🎉 ZIEL ERREICHT: >80% PLZ-Abdeckung!\")\n",
    "elif improvement > 0:\n",
    "    print(f\"   📈 VERBESSERUNG: Erhöhte PLZ-Abdeckung\")\n",
    "else:\n",
    "    print(f\"   ⚠️  KEINE VERBESSERUNG: Alternative Strategie erforderlich\")\n",
    "\n",
    "# Aktualisiere die PLZ-Spalte mit dem kombinierten Ergebnis\n",
    "df_normalized['plz'] = df_normalized['plz_combined']\n",
    "\n",
    "# Zeige verbesserte PLZ-Verteilung\n",
    "if plz_combined_found > 0:\n",
    "    print(f\"\\n📋 Verbesserte PLZ-Verteilung (Top 10):\")\n",
    "    plz_counts_new = df_normalized['plz'].value_counts().head(10)\n",
    "    for plz, count in plz_counts_new.items():\n",
    "        print(f\"   {plz}: {count:,} Einträge\")\n",
    "\n",
    "# Bereinige temporäre Spalten\n",
    "df_normalized = df_normalized.drop(['plz_advanced', 'plz_combined'], axis=1)\n",
    "\n",
    "print(f\"\\n✅ Erweiterte PLZ-Extraktion für 2018-2019 Dataset abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t0",
   "metadata": {},
   "source": [
    "## 8. Export des finalen angereicherten Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "u1v2w3x4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT FINALES ANGEREICHERTES DATASET\n",
      "============================================================\n",
      "✅ Finales angereichertes Dataset exportiert: data/processed/dataset_2018_2019_enriched.csv\n",
      "Dateigröße: 10,387 Zeilen x 15 Spalten\n",
      "✅ Export-Validierung erfolgreich: 10,387 Zeilen geladen\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPORT FINALES ANGEREICHERTES DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Export\n",
    "output_file_enriched = 'data/processed/dataset_2018_2019_enriched.csv'\n",
    "df_enriched.to_csv(output_file_enriched, index=False)\n",
    "\n",
    "print(f\"✅ Finales angereichertes Dataset exportiert: {output_file_enriched}\")\n",
    "print(f\"Dateigröße: {len(df_enriched):,} Zeilen x {len(df_enriched.columns)} Spalten\")\n",
    "\n",
    "# Validierung durch Wiedereinlesen\n",
    "test_df_enriched = pd.read_csv(output_file_enriched)\n",
    "print(f\"✅ Export-Validierung erfolgreich: {len(test_df_enriched):,} Zeilen geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39382f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔧 KRITISCHE PLZ-DATENTYP-REPARATUR VOR EXPORT\n",
      "============================================================\n",
      "PLZ-Datentyp vor Reparatur: object\n",
      "PLZ-Beispiele vor Reparatur: [np.int64(13591), np.int64(12527), np.int64(13053)]\n",
      "PLZ-Datentyp nach Reparatur: object\n",
      "PLZ-Beispiele nach Reparatur: ['13591', '12527', '13053']\n",
      "✅ PLZ-Datentyp-Reparatur abgeschlossen!\n",
      "   ➡️  PLZ wird jetzt als String gespeichert für korrekten Join\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# KRITISCHE PLZ-DATENTYP-REPARATUR VOR EXPORT\n",
    "# ===================================================================\n",
    "print(\"\\n🔧 KRITISCHE PLZ-DATENTYP-REPARATUR VOR EXPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Stelle sicher, dass PLZ als STRING gespeichert wird (nicht als Integer/Float)\n",
    "# Dies ist kritisch für den späteren Join in 04_Combine_Datasets.ipynb\n",
    "print(\"PLZ-Datentyp vor Reparatur:\", df_enriched['plz'].dtype)\n",
    "print(\"PLZ-Beispiele vor Reparatur:\", df_enriched['plz'].dropna().head(3).tolist())\n",
    "\n",
    "# Konvertiere PLZ zu String\n",
    "df_enriched['plz'] = df_enriched['plz'].apply(\n",
    "    lambda x: str(int(x)) if pd.notna(x) else None\n",
    ")\n",
    "\n",
    "print(\"PLZ-Datentyp nach Reparatur:\", df_enriched['plz'].dtype)  \n",
    "print(\"PLZ-Beispiele nach Reparatur:\", df_enriched['plz'].dropna().head(3).tolist())\n",
    "\n",
    "print(\"✅ PLZ-Datentyp-Reparatur abgeschlossen!\")\n",
    "print(\"   ➡️  PLZ wird jetzt als String gespeichert für korrekten Join\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
