{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c57dfb6",
   "metadata": {},
   "source": [
    "## ðŸ“š **Methodologie und Technische Dokumentation**\n",
    "\n",
    "### ðŸŽ¯ **Zentrale Herausforderung: Heterogene Datenstrukturen**\n",
    "\n",
    "Das Dataset 2025 unterscheidet sich fundamental von den DatensÃ¤tzen 2018-2019 und 2022:\n",
    "\n",
    "| **Merkmal** | **Dataset 2018-2019** | **Dataset 2022** | **Dataset 2025** |\n",
    "|-------------|----------------------|-------------------|-------------------|\n",
    "| **PLZ-VerfÃ¼gbarkeit** | 100% | 100% | ~1-2% |\n",
    "| **Adressformat** | Strukturiert | Strukturiert | Heterogen |\n",
    "| **Bezirksangaben** | Implizit Ã¼ber PLZ | Implizit Ã¼ber PLZ | Explizit als Text |\n",
    "| **Multi-Listings** | Keine | Keine | Ja (Preis-/GrÃ¶ÃŸenspannen) |\n",
    "\n",
    "### ðŸ”¬ **Entwickelte LÃ¶sungsansÃ¤tze:**\n",
    "\n",
    "#### **1. Intelligente Adressextraktion**\n",
    "- **Regex-basierte PLZ-Extraktion:** `\\b(\\d{5})\\b`\n",
    "- **Mehrstufige Bezirks-Erkennung:** Alias-Mapping fÃ¼r Berlin-spezifische Varianten\n",
    "- **Fallback-Mechanismen:** Strukturierte PrioritÃ¤tenliste fÃ¼r Adresskomponenten\n",
    "\n",
    "#### **2. Multi-Listing-Behandlung**\n",
    "- **Preisspannen:** `\"725 - 1.965â‚¬\"` â†’ Minimum-Ansatz fÃ¼r Vergleichbarkeit\n",
    "- **GrÃ¶ÃŸenspannen:** `\"26,55 - 112,82mÂ²\"` â†’ Konservative SchÃ¤tzung\n",
    "- **Rationale:** Vermeidung von ÃœberschÃ¤tzungen bei Zeitvergleichen\n",
    "\n",
    "#### **3. Dual-Strategie-Anreicherung**\n",
    "- **Strategie 1:** PLZ-basiert (hÃ¶chste PrÃ¤zision, wenige Datenpunkte)\n",
    "- **Strategie 2:** Bezirks-basiert (Fallback fÃ¼r 98% der Daten)\n",
    "- **Kombination:** Nahtlose Integration beider AnsÃ¤tze\n",
    "\n",
    "### ðŸ“Š **QualitÃ¤tssicherung:**\n",
    "\n",
    "- **Vermeidung kartesischer Produkte:** Durch `drop_duplicates(subset=['plz'])`\n",
    "- **DatenintegritÃ¤tsprÃ¼fung:** Vor- und Nach-Vergleiche aller Verarbeitungsschritte\n",
    "- **Standardisierte Filter:** Identisch mit anderen Datasets fÃ¼r Vergleichbarkeit\n",
    "\n",
    "### ðŸŽ“ **Wissenschaftliche Relevanz:**\n",
    "\n",
    "Diese Methodologie demonstriert den Umgang mit **heterogenen Datenquellen** in der Immobiliendatenanalyse - ein hÃ¤ufiges Problem bei longitudinalen Studien, wo sich Datenstrukturen Ã¼ber Zeit Ã¤ndern.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28af644",
   "metadata": {},
   "source": [
    "# 03_Clean_Dataset_2025 - Intelligente Adressextraktion\n",
    "\n",
    "## ðŸŽ¯ **Spezifische Bereinigung fÃ¼r Dataset 2025**\n",
    "\n",
    "### **Hauptfunktionen:**\n",
    "- **Intelligente PLZ-Extraktion** aus verschiedenen Adressformaten\n",
    "- **Bezirk-Normalisierung** mit Alias-Mapping\n",
    "- **Multi-Listing-Behandlung** (Preis- und GrÃ¶ÃŸenspannen)\n",
    "- **Filter-Harmonisierung** mit anderen Datasets\n",
    "- **Standardisierte Ausgabe** kompatibel mit anderen Datasets\n",
    "\n",
    "### **ðŸ”„ Filter-Harmonisierung (Konsistent mit allen Datasets):**\n",
    "- **Preis-Filter:** 100â‚¬ - 10.000â‚¬ (Kaltmiete)\n",
    "- **GrÃ¶ÃŸen-Filter:** 10mÂ² - 500mÂ² (WohnflÃ¤che)\n",
    "- **Bezirk-Validierung:** Nur gÃ¼ltige Berliner Bezirke\n",
    "\n",
    "### **ðŸ“‹ Adressformate im 2025 Dataset:**\n",
    "1. **VollstÃ¤ndige Adresse mit PLZ:** \"Johannisplatz 3, 10117 Berlin\"\n",
    "2. **Adresse mit Bezirk:** \"Johannisplatz 5, Mitte (Ortsteil), Berlin\" \n",
    "3. **Nur Bezirk:** \"Tiergarten, Berlin\"\n",
    "4. **Nur PLZ:** \"10557 Berlin\"\n",
    "5. **Adresse mit Ortsteil:** \"Friedrichshain, Berlin\"\n",
    "\n",
    "### **ðŸŽ¯ Ziel:** \n",
    "Einheitliche Bezirk-Zuordnung und maximale Vergleichbarkeit mit Dataset 2018-2019 und Dataset 2022\n",
    "\n",
    "---\n",
    "**Teil der modularen Preprocessing-Pipeline**  \n",
    "**Datum:** 4. Juli 2025  \n",
    "**Version:** 1.1 (Filter-Harmonisierung)  \n",
    "**Status:** âœ… Harmonisiert mit allen anderen Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b3a266",
   "metadata": {},
   "source": [
    "## 1. Setup und Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a456e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotheken erfolgreich importiert!\n",
      "Pandas Version: 2.2.3\n",
      "Dataset: 2025 (ImmobilienScout24)\n",
      "Ziel: Intelligente Adressextraktion und Bezirk-Normalisierung\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Bibliotheken erfolgreich importiert!\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(\"Dataset: 2025 (ImmobilienScout24)\")\n",
    "print(\"Ziel: Intelligente Adressextraktion und Bezirk-Normalisierung\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a068c88",
   "metadata": {},
   "source": [
    "## 2. PLZ-Mapping und Bezirk-Normalisierung laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9037091a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PLZ-MAPPING UND BEZIRK-NORMALISIERUNG\n",
      "============================================================\n",
      "============================================================\n",
      "PLZ-ZU-BEZIRK-MAPPING LADEN\n",
      "============================================================\n",
      "âœ… PLZ-Mapping geladen: 181 EintrÃ¤ge\n",
      "âœ… PLZ-Dictionary erstellt: 181 Zuordnungen\n",
      "âœ… PLZ-Mapping geladen: 185 Zuordnungen\n",
      "âœ… Bezirk-Aliases definiert: 36 Zuordnungen\n",
      "\n",
      "PLZ-Mapping Beispiele:\n",
      "  10115 â†’ Mitte\n",
      "  10117 â†’ Mitte\n",
      "  10119 â†’ Mitte\n",
      "  10178 â†’ Mitte\n",
      "  10179 â†’ Mitte\n",
      "\n",
      "Bezirk-Normalisierung Beispiele:\n",
      "  'Mitte (Ortsteil)' â†’ 'Mitte'\n",
      "  'Pankow (Ortsteil)' â†’ 'Pankow'\n",
      "  'Spandau (Ortsteil)' â†’ 'Spandau'\n",
      "  'NeukÃ¶lln (Ortsteil)' â†’ 'NeukÃ¶lln'\n",
      "  'Friedrichshain' â†’ 'Friedrichshain-Kreuzberg'\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PLZ-MAPPING UND BEZIRK-NORMALISIERUNG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# PLZ-zu-Bezirk-Mapping laden\n",
    "print(\"=\" * 60)\n",
    "print(\"PLZ-ZU-BEZIRK-MAPPING LADEN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    plz_mapping_df = pd.read_csv('data/processed/berlin_plz_mapping.csv')\n",
    "    print(f\"âœ… PLZ-Mapping geladen: {len(plz_mapping_df)} EintrÃ¤ge\")\n",
    "    \n",
    "    # Erstelle Dictionary fÃ¼r schnelles Lookup\n",
    "    plz_to_district = dict(zip(plz_mapping_df['PLZ'], plz_mapping_df['Bezirk']))\n",
    "    print(f\"âœ… PLZ-Dictionary erstellt: {len(plz_to_district)} Zuordnungen\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ FEHLER: PLZ-Mapping nicht gefunden!\")\n",
    "    print(\"Bitte stellen Sie sicher, dass 'data/processed/berlin_plz_mapping.csv' existiert.\")\n",
    "    raise\n",
    "\n",
    "# Erweiterte PLZ-Zuordnungen fÃ¼r Dataset 2025\n",
    "extended_plz_mapping = {\n",
    "    10115: 'Mitte',\n",
    "    10117: 'Mitte',\n",
    "    10119: 'Mitte',\n",
    "    10178: 'Mitte',\n",
    "    10179: 'Mitte',\n",
    "    10243: 'Friedrichshain-Kreuzberg',\n",
    "    10245: 'Friedrichshain-Kreuzberg',\n",
    "    10247: 'Friedrichshain-Kreuzberg',\n",
    "    10249: 'Friedrichshain-Kreuzberg',\n",
    "    10315: 'Lichtenberg',\n",
    "    10317: 'Lichtenberg',\n",
    "    10318: 'Lichtenberg',\n",
    "    10319: 'Lichtenberg',\n",
    "    10365: 'Lichtenberg',\n",
    "    10367: 'Lichtenberg',\n",
    "    10369: 'Lichtenberg',\n",
    "    12305: 'Tempelhof-SchÃ¶neberg',\n",
    "    12307: 'Tempelhof-SchÃ¶neberg',\n",
    "    12309: 'Tempelhof-SchÃ¶neberg',\n",
    "    12347: 'NeukÃ¶lln',\n",
    "    12349: 'NeukÃ¶lln',\n",
    "    12351: 'NeukÃ¶lln',\n",
    "    12353: 'NeukÃ¶lln',\n",
    "    12355: 'NeukÃ¶lln',\n",
    "    12357: 'NeukÃ¶lln',\n",
    "    12359: 'NeukÃ¶lln',\n",
    "    12524: 'Treptow-KÃ¶penick',\n",
    "    12555: 'Treptow-KÃ¶penick',\n",
    "    10247: 'Friedrichshain-Kreuzberg',\n",
    "    10249: 'Friedrichshain-Kreuzberg',\n",
    "    10367: 'Lichtenberg',\n",
    "    10369: 'Lichtenberg',\n",
    "    14612: 'Falkensee',  # AuÃŸerhalb Berlin\n",
    "    13507: 'Reinickendorf',\n",
    "    10585: 'Charlottenburg-Wilmersdorf',\n",
    "    10709: 'Charlottenburg-Wilmersdorf',\n",
    "    10559: 'Mitte',\n",
    "}\n",
    "\n",
    "# Erweitere PLZ-Mapping\n",
    "plz_to_district.update(extended_plz_mapping)\n",
    "\n",
    "# Bezirk-Normalisierung (verschiedene Schreibweisen auf einheitliche Namen mappen)\n",
    "district_aliases = {\n",
    "    'Mitte (Ortsteil)': 'Mitte',\n",
    "    'Pankow (Ortsteil)': 'Pankow',\n",
    "    'Spandau (Ortsteil)': 'Spandau',\n",
    "    'NeukÃ¶lln (Ortsteil)': 'NeukÃ¶lln',\n",
    "    'Friedrichshain': 'Friedrichshain-Kreuzberg',\n",
    "    'Kreuzberg': 'Friedrichshain-Kreuzberg',\n",
    "    'Charlottenburg': 'Charlottenburg-Wilmersdorf',\n",
    "    'Wilmersdorf': 'Charlottenburg-Wilmersdorf',\n",
    "    'Tempelhof': 'Tempelhof-SchÃ¶neberg',\n",
    "    'SchÃ¶neberg': 'Tempelhof-SchÃ¶neberg',\n",
    "    'Prenzlauer Berg': 'Pankow',\n",
    "    'WeiÃŸensee': 'Pankow',\n",
    "    'Buch': 'Pankow',\n",
    "    'NiederschÃ¶nhausen': 'Pankow',\n",
    "    'Gesundbrunnen': 'Mitte',\n",
    "    'Wedding': 'Mitte',\n",
    "    'Moabit': 'Mitte',\n",
    "    'Tiergarten': 'Mitte',\n",
    "    'Friedenau': 'Tempelhof-SchÃ¶neberg',\n",
    "    'Steglitz': 'Steglitz-Zehlendorf',\n",
    "    'Zehlendorf': 'Steglitz-Zehlendorf',\n",
    "    'Schmargendorf': 'Charlottenburg-Wilmersdorf',\n",
    "    'Grunewald': 'Charlottenburg-Wilmersdorf',\n",
    "    'Halensee': 'Charlottenburg-Wilmersdorf',\n",
    "    'Tegel': 'Reinickendorf',\n",
    "    'Heiligensee': 'Reinickendorf',\n",
    "    'Staaken': 'Spandau',\n",
    "    'Siemensstadt': 'Spandau',\n",
    "    'Malchow': 'Pankow',\n",
    "    'Reinickendorf': 'Reinickendorf',\n",
    "    'Lichtenberg': 'Lichtenberg',\n",
    "    'Marzahn-Hellersdorf': 'Marzahn-Hellersdorf',\n",
    "    'Spandau': 'Spandau',\n",
    "    'NeukÃ¶lln': 'NeukÃ¶lln',\n",
    "    'Mitte': 'Mitte',\n",
    "    'Pankow': 'Pankow',\n",
    "}\n",
    "\n",
    "print(f\"âœ… PLZ-Mapping geladen: {len(plz_to_district)} Zuordnungen\")\n",
    "print(f\"âœ… Bezirk-Aliases definiert: {len(district_aliases)} Zuordnungen\")\n",
    "\n",
    "# Zeige Beispiele\n",
    "print(\"\\nPLZ-Mapping Beispiele:\")\n",
    "for plz, district in list(plz_to_district.items())[:5]:\n",
    "    print(f\"  {plz} â†’ {district}\")\n",
    "    \n",
    "print(\"\\nBezirk-Normalisierung Beispiele:\")\n",
    "for alias, normalized in list(district_aliases.items())[:5]:\n",
    "    print(f\"  '{alias}' â†’ '{normalized}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e94ac7",
   "metadata": {},
   "source": [
    "## 3. Dataset 2025 laden und analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fed89cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET 2025 LADEN UND ANALYSIEREN\n",
      "============================================================\n",
      "Dataset geladen: 6,109 Zeilen, 5 Spalten\n",
      "Spalten: ['title', 'price', 'size', 'address', 'link']\n",
      "Datentypen:\n",
      "title      object\n",
      "price      object\n",
      "size       object\n",
      "address    object\n",
      "link       object\n",
      "dtype: object\n",
      "Fehlende Werte:\n",
      "=== ADRESSFORMAT-ANALYSE ===\n",
      "Erste 10 Adressen:\n",
      "  1. Biedenkopfer StraÃŸe 46-54, 13507 Berlin\n",
      "  2. Seegefelder StraÃŸe 150, 14612 Falkensee\n",
      "  3. Johannisplatz 5, Mitte (Ortsteil), Berlin\n",
      "  4. PufendorfstraÃŸe 3A-3E, Friedrichshain, Berlin\n",
      "  5. Warburgzeile 1, 10585 Berlin\n",
      "  6. Johannisplatz 3, 10117 Berlin\n",
      "  7. KreutzigerstraÃŸe 14, Friedrichshain, Berlin\n",
      "  8. Elsa-Neumann-StraÃŸe 1, 13629 Berlin\n",
      "  9. Tiergarten, Berlin\n",
      "  10. ChausseestraÃŸe 108, Mitte (Ortsteil), Berlin\n",
      "Einzigartige Adressformate (Sample):\n",
      "  â€¢ Biedenkopfer StraÃŸe 46-54, 13507 Berlin\n",
      "  â€¢ Seegefelder StraÃŸe 150, 14612 Falkensee\n",
      "  â€¢ Johannisplatz 5, Mitte (Ortsteil), Berlin\n",
      "  â€¢ PufendorfstraÃŸe 3A-3E, Friedrichshain, Berlin\n",
      "  â€¢ Warburgzeile 1, 10585 Berlin\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATASET 2025 LADEN UND ANALYSIEREN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Dataset laden\n",
    "df = pd.read_csv('data/raw/Dataset_2025.csv')\n",
    "print(f\"Dataset geladen: {len(df):,} Zeilen, {len(df.columns)} Spalten\")\n",
    "\n",
    "print(f\"Spalten: {list(df.columns)}\")\n",
    "\n",
    "print(f\"Datentypen:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "print(f\"Fehlende Werte:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_values = missing_values[missing_values > 0]\n",
    "for col, count in missing_values.items():\n",
    "    print(f\"  {col}: {count} ({100*count/len(df):.2f}%)\")\n",
    "\n",
    "# Erste Adressfelder analysieren\n",
    "print(f\"=== ADRESSFORMAT-ANALYSE ===\")\n",
    "print(\"Erste 10 Adressen:\")\n",
    "for i, addr in enumerate(df['address'].head(10)):\n",
    "    print(f\"  {i+1}. {addr}\")\n",
    "\n",
    "print(f\"Einzigartige Adressformate (Sample):\")\n",
    "unique_addresses = df['address'].dropna().unique()\n",
    "for addr in unique_addresses[:5]:\n",
    "    print(f\"  â€¢ {addr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5851ec",
   "metadata": {},
   "source": [
    "## 4. Intelligente Adressextraktion und Bezirk-Zuordnung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11ffb666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INTELLIGENTE ADRESSEXTRAKTION\n",
      "============================================================\n",
      "Extrahiere Bezirke aus Adressen...\n",
      "âœ… Bezirk-Extraktion abgeschlossen:\n",
      "  Gesamt Adressen: 6,109\n",
      "  Erfolgreiche Extraktion: 4,440\n",
      "  Erfolgsrate: 72.7%\n",
      "=== ERFOLGREICHE EXTRAKTION (Beispiele) ===\n",
      "  'Biedenkopfer StraÃŸe 46-54, 13507 Berlin' â†’ Reinickendorf\n",
      "  'Seegefelder StraÃŸe 150, 14612 Falkensee' â†’ Falkensee\n",
      "  'Johannisplatz 5, Mitte (Ortsteil), Berlin' â†’ Mitte\n",
      "  'PufendorfstraÃŸe 3A-3E, Friedrichshain, Berlin' â†’ Friedrichshain-Kreuzberg\n",
      "  'Warburgzeile 1, 10585 Berlin' â†’ Charlottenburg-Wilmersdorf\n",
      "  'Johannisplatz 3, 10117 Berlin' â†’ Mitte\n",
      "  'KreutzigerstraÃŸe 14, Friedrichshain, Berlin' â†’ Friedrichshain-Kreuzberg\n",
      "  'Elsa-Neumann-StraÃŸe 1, 13629 Berlin' â†’ Spandau\n",
      "  'Tiergarten, Berlin' â†’ Mitte\n",
      "  'ChausseestraÃŸe 108, Mitte (Ortsteil), Berlin' â†’ Mitte\n",
      "=== NICHT-EXTRAHIERTE ADRESSEN (1669 EintrÃ¤ge) ===\n",
      "  âŒ 'Abendseglersteig 55, Rahnsdorf, Berlin'\n",
      "  âŒ 'GrÃ¼nauer StraÃŸe 26, Altglienicke, Berlin'\n",
      "  âŒ 'Carl-Spindler-StraÃŸe 19, KÃ¶penick, Berlin'\n",
      "  âŒ 'Am Maselakepark 31, Hakenfelde, Berlin'\n",
      "  âŒ 'Lion-Feuchtwanger StraÃŸe 61, Hellersdorf, Berlin'\n",
      "  âŒ 'LehderstraÃŸe 26-27, 13806 Berlin'\n",
      "  âŒ 'WÃ¤scherinnenweg 2, KÃ¶penick, Berlin'\n",
      "  âŒ 'Eichbuschallee 55, PlÃ¤nterwald, Berlin'\n",
      "  âŒ 'BouchestraÃŸe 37, Alt-Treptow, Berlin'\n",
      "  âŒ 'GrÃ¼nauer StraÃŸe 29, KÃ¶penick, Berlin'\n",
      "=== BEZIRK-VERTEILUNG ===\n",
      "Anzahl einzigartiger Bezirke: 21\n",
      "  Mitte: 1028 EintrÃ¤ge\n",
      "  Pankow: 784 EintrÃ¤ge\n",
      "  Friedrichshain-Kreuzberg: 778 EintrÃ¤ge\n",
      "  Charlottenburg-Wilmersdorf: 602 EintrÃ¤ge\n",
      "  NeukÃ¶lln: 397 EintrÃ¤ge\n",
      "  Tempelhof-SchÃ¶neberg: 355 EintrÃ¤ge\n",
      "  Steglitz-Zehlendorf: 170 EintrÃ¤ge\n",
      "  Reinickendorf: 129 EintrÃ¤ge\n",
      "  Spandau: 93 EintrÃ¤ge\n",
      "  Lichtenberg: 77 EintrÃ¤ge\n",
      "âœ… Verbleibende EintrÃ¤ge nach Bezirk-Extraktion: 4,440\n",
      "Datenverlust: 1,669 EintrÃ¤ge (27.3%)\n"
     ]
    }
   ],
   "source": [
    "def extract_district_from_address(address):\n",
    "    \"\"\"\n",
    "    Intelligente Bezirk-Extraktion aus verschiedenen Adressformaten\n",
    "    \n",
    "    UnterstÃ¼tzte Formate:\n",
    "    1. PLZ-basiert: \"Johannisplatz 3, 10117 Berlin\"\n",
    "    2. Bezirk direkt: \"Johannisplatz 5, Mitte (Ortsteil), Berlin\"\n",
    "    3. Nur Bezirk: \"Tiergarten, Berlin\"\n",
    "    4. Nur PLZ: \"10557 Berlin\"\n",
    "    \"\"\"\n",
    "    if pd.isna(address):\n",
    "        return None\n",
    "    \n",
    "    address = str(address).strip()\n",
    "    \n",
    "    # Methode 1: PLZ-Extraktion (5-stellige Zahlen)\n",
    "    plz_match = re.search(r'\\b(\\d{5})\\b', address)\n",
    "    if plz_match:\n",
    "        plz = int(plz_match.group(1))\n",
    "        if plz in plz_to_district:\n",
    "            return plz_to_district[plz]\n",
    "    \n",
    "    # Methode 2: Bezirk-Namen direkt im Text finden\n",
    "    # Entferne \"Berlin\" vom Ende und teile bei Komma\n",
    "    address_clean = address.replace(', Berlin', '').replace(' Berlin', '')\n",
    "    \n",
    "    # Suche nach bekannten Bezirken in der Adresse\n",
    "    for alias, normalized in district_aliases.items():\n",
    "        if alias.lower() in address_clean.lower():\n",
    "            return normalized\n",
    "    \n",
    "    # Methode 3: Letzte Komponente vor \"Berlin\" als Bezirk\n",
    "    parts = address_clean.split(',')\n",
    "    if len(parts) >= 2:\n",
    "        potential_district = parts[-1].strip()\n",
    "        # Entferne ZusÃ¤tze wie \"(Ortsteil)\"\n",
    "        potential_district = re.sub(r'\\s*\\([^)]+\\)', '', potential_district)\n",
    "        \n",
    "        # PrÃ¼fe ob es ein bekannter Bezirk ist\n",
    "        if potential_district in district_aliases:\n",
    "            return district_aliases[potential_district]\n",
    "    \n",
    "    # Methode 4: Direkte Bezirk-Suche im gesamten Text\n",
    "    for bezirk in district_aliases.values():\n",
    "        if bezirk.lower() in address_clean.lower():\n",
    "            return bezirk\n",
    "    \n",
    "    return None\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"INTELLIGENTE ADRESSEXTRAKTION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Arbeite mit einer Kopie\n",
    "df_clean = df.copy()\n",
    "\n",
    "# Bezirk extrahieren\n",
    "print(\"Extrahiere Bezirke aus Adressen...\")\n",
    "df_clean['district'] = df_clean['address'].apply(extract_district_from_address)\n",
    "\n",
    "# Statistiken\n",
    "total_addresses = len(df_clean)\n",
    "successful_extractions = df_clean['district'].notna().sum()\n",
    "success_rate = 100 * successful_extractions / total_addresses\n",
    "\n",
    "print(f\"âœ… Bezirk-Extraktion abgeschlossen:\")\n",
    "print(f\"  Gesamt Adressen: {total_addresses:,}\")\n",
    "print(f\"  Erfolgreiche Extraktion: {successful_extractions:,}\")\n",
    "print(f\"  Erfolgsrate: {success_rate:.1f}%\")\n",
    "\n",
    "# Zeige Beispiele erfolgreicher Extraktion\n",
    "print(f\"=== ERFOLGREICHE EXTRAKTION (Beispiele) ===\")\n",
    "successful_examples = df_clean[df_clean['district'].notna()][['address', 'district']].head(10)\n",
    "for idx, row in successful_examples.iterrows():\n",
    "    print(f\"  '{row['address']}' â†’ {row['district']}\")\n",
    "\n",
    "# Zeige nicht-extrahierte Adressen\n",
    "failed_extractions = df_clean[df_clean['district'].isna()]\n",
    "if len(failed_extractions) > 0:\n",
    "    print(f\"=== NICHT-EXTRAHIERTE ADRESSEN ({len(failed_extractions)} EintrÃ¤ge) ===\")\n",
    "    for addr in failed_extractions['address'].unique()[:10]:\n",
    "        print(f\"  âŒ '{addr}'\")\n",
    "        \n",
    "# Bezirk-Verteilung\n",
    "print(f\"=== BEZIRK-VERTEILUNG ===\")\n",
    "district_counts = df_clean['district'].value_counts()\n",
    "print(f\"Anzahl einzigartiger Bezirke: {len(district_counts)}\")\n",
    "for district, count in district_counts.head(10).items():\n",
    "    print(f\"  {district}: {count} EintrÃ¤ge\")\n",
    "    \n",
    "# Nur Zeilen mit erfolgreich extrahierten Bezirken behalten\n",
    "df_clean = df_clean[df_clean['district'].notna()]\n",
    "print(f\"âœ… Verbleibende EintrÃ¤ge nach Bezirk-Extraktion: {len(df_clean):,}\")\n",
    "print(f\"Datenverlust: {total_addresses - len(df_clean):,} EintrÃ¤ge ({100*(total_addresses - len(df_clean))/total_addresses:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64402b8",
   "metadata": {},
   "source": [
    "## 5. Datenbereinigung und Filter-Harmonisierung\n",
    "\n",
    "### ðŸŽ¯ **Einheitliche Bereinigungskriterien**\n",
    "**Konsistent mit allen anderen Datasets in der Pipeline:**\n",
    "- **Preis-Filter:** 100â‚¬ - 10.000â‚¬ (Kaltmiete)\n",
    "- **GrÃ¶ÃŸen-Filter:** 10mÂ² - 500mÂ² (WohnflÃ¤che)\n",
    "- **Bezirk-Validierung:** Nur gÃ¼ltige Berliner Bezirke\n",
    "\n",
    "### ðŸ“‹ **Multi-Listing-Behandlung**\n",
    "Dataset 2025 enthÃ¤lt Multi-Listings (Preis- und GrÃ¶ÃŸenspannen):\n",
    "- **Preisspannen:** \"725 - 1.965â‚¬\" â†’ Nimm Mindestpreis (725â‚¬)\n",
    "- **GrÃ¶ÃŸenspannen:** \"26,55 - 112,82mÂ²\" â†’ Nimm MindestgrÃ¶ÃŸe (26,55mÂ²)\n",
    "- **Rationale:** Konservative Schï¿½ï¿½tzung fÃ¼r Vergleichbarkeit\n",
    "\n",
    "### ðŸ”„ **Harmonisierung mit anderen Datasets**\n",
    "- **Dataset 2018-2019:** Gleiche Filter (100â‚¬-10.000â‚¬, 10mÂ²-500mÂ²)\n",
    "- **Dataset 2022:** Filter aktualisiert auf gleiche Werte\n",
    "- **Dataset 2025:** Implementiert gleiche Logik\n",
    "\n",
    "**Ziel:** Maximale Vergleichbarkeit und Konsistenz zwischen allen ZeitrÃ¤umen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c4d1cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATENBEREINIGUNG UND MULTI-LISTING-BEHANDLUNG\n",
      "============================================================\n",
      "Bereinige Preisfelder...\n",
      "Bereinige GrÃ¶ÃŸenfelder...\n",
      "=== BEREINIGUNGSSTATISTIKEN ===\n",
      "UrsprÃ¼ngliche EintrÃ¤ge: 4,440\n",
      "GÃ¼ltige Preise: 4,440/4,440 (100.0%)\n",
      "GÃ¼ltige GrÃ¶ÃŸen: 4,440/4,440 (100.0%)\n",
      "Beide gÃ¼ltig: 4,440/4,440 (100.0%)\n",
      "=== EINHEITLICHE FILTER-KRITERIEN ===\n",
      "ðŸ“‹ Harmonisiert mit Dataset 2018-2019 und Dataset 2022\n",
      "ðŸŽ¯ Ziel: Maximale Vergleichbarkeit zwischen allen ZeitrÃ¤umen\n",
      "ðŸ”¹ Preis-Filter: 100â‚¬ - 10.000â‚¬\n",
      "Nach Preis-Filter: 4,425 (entfernt: 15)\n",
      "ðŸ”¹ GrÃ¶ÃŸen-Filter: 10mÂ² - 500mÂ²\n",
      "Nach GrÃ¶ÃŸen-Filter: 4,424 (entfernt: 1)\n",
      "âœ… Datenbereinigung abgeschlossen\n",
      "âœ… Filter-Harmonisierung erfolgreich\n",
      "Finale DatensÃ¤tze: 4,424\n",
      "=== FINALE DATENVERTEILUNG ===\n",
      "Preis - Min: 150.00â‚¬, Max: 9990.00â‚¬, Median: 1001.62â‚¬\n",
      "GrÃ¶ÃŸe - Min: 11.0mÂ², Max: 361.0mÂ², Median: 65.0mÂ²\n",
      "ðŸ“Š BEREINIGUNGSLOGIK KONSISTENT MIT:\n",
      "   â€¢ 01_Clean_Dataset_2018_2019.ipynb\n",
      "   â€¢ 02_Clean_Dataset_2022.ipynb\n",
      "   â€¢ 03_Clean_Dataset_2025.ipynb (dieses Notebook)\n",
      "   â€¢ 04_Combine_Datasets.ipynb (finale Validierung)\n"
     ]
    }
   ],
   "source": [
    "def clean_price_field(price_str):\n",
    "    \"\"\"\n",
    "    Bereinige Preisfeld und extrahiere Einzelpreise aus Multi-Listings\n",
    "    \n",
    "    Behandelt folgende Formate:\n",
    "    - Einzelpreis: \"1.235â‚¬\" â†’ 1235.0\n",
    "    - Preisspanne: \"725 - 1.965â‚¬\" â†’ 725.0 (Mindestpreis)\n",
    "    - Deutsche Formate: \"1.235,65â‚¬\" â†’ 1235.65\n",
    "    \n",
    "    Returns:\n",
    "        float: Bereinigter Preis oder None bei Fehlern\n",
    "    \"\"\"\n",
    "    if pd.isna(price_str):\n",
    "        return None\n",
    "    \n",
    "    price_str = str(price_str).strip()\n",
    "    \n",
    "    # Entferne Euro-Zeichen und Leerzeichen\n",
    "    price_str = price_str.replace('â‚¬', '').replace(' ', '')\n",
    "    \n",
    "    # Behandle Preisspannen (z.B. \"725 - 1.965\")\n",
    "    if '-' in price_str:\n",
    "        # Multi-Listing: Nimm Minimalpreis fÃ¼r Vergleichbarkeit\n",
    "        parts = price_str.split('-')\n",
    "        try:\n",
    "            min_price = float(parts[0].replace('.', '').replace(',', '.'))\n",
    "            return min_price\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # Einzelpreis\n",
    "    try:\n",
    "        # Behandle deutsche Zahlenformate (1.435,65 â†’ 1435.65)\n",
    "        if ',' in price_str and '.' in price_str:\n",
    "            # Format: 1.435,65\n",
    "            price_str = price_str.replace('.', '').replace(',', '.')\n",
    "        elif ',' in price_str:\n",
    "            # Format: 1435,65\n",
    "            price_str = price_str.replace(',', '.')\n",
    "        elif '.' in price_str and len(price_str.split('.')[-1]) == 2:\n",
    "            # Format: 1435.65 (bereits korrekt)\n",
    "            pass\n",
    "        else:\n",
    "            # Format: 1435 (Tausender-Trennzeichen entfernen)\n",
    "            price_str = price_str.replace('.', '')\n",
    "        \n",
    "        return float(price_str)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def clean_size_field(size_str):\n",
    "    \"\"\"\n",
    "    Bereinige GrÃ¶ÃŸenfeld und extrahiere EinzelgrÃ¶ÃŸen aus Multi-Listings\n",
    "    \n",
    "    Behandelt folgende Formate:\n",
    "    - EinzelgrÃ¶ÃŸe: \"67,5mÂ²\" â†’ 67.5\n",
    "    - GrÃ¶ÃŸenspanne: \"26,55 - 112,82mÂ²\" â†’ 26.55 (MindestgrÃ¶ÃŸe)\n",
    "    - Verschiedene Trennzeichen: \"67,5\" oder \"67.5\"\n",
    "    \n",
    "    Returns:\n",
    "        float: Bereinigte GrÃ¶ÃŸe oder None bei Fehlern\n",
    "    \"\"\"\n",
    "    if pd.isna(size_str):\n",
    "        return None\n",
    "    \n",
    "    size_str = str(size_str).strip()\n",
    "    \n",
    "    # Entferne mÂ² und Leerzeichen\n",
    "    size_str = size_str.replace('mÂ²', '').replace(' ', '')\n",
    "    \n",
    "    # Behandle GrÃ¶ÃŸenspannen (z.B. \"26,55 - 112,82\")\n",
    "    if '-' in size_str:\n",
    "        # Multi-Listing: Nimm MinimalgrÃ¶ÃŸe fÃ¼r Vergleichbarkeit\n",
    "        parts = size_str.split('-')\n",
    "        try:\n",
    "            min_size = float(parts[0].replace(',', '.'))\n",
    "            return min_size\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    # EinzelgrÃ¶ÃŸe\n",
    "    try:\n",
    "        return float(size_str.replace(',', '.'))\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# ===================================================================\n",
    "# EINHEITLICHE DATENBEREINIGUNG (HARMONISIERT MIT ALLEN DATASETS)\n",
    "# ===================================================================\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATENBEREINIGUNG UND MULTI-LISTING-BEHANDLUNG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Bereinige Preise\n",
    "print(\"Bereinige Preisfelder...\")\n",
    "df_clean['price_clean'] = df_clean['price'].apply(clean_price_field)\n",
    "\n",
    "# Bereinige GrÃ¶ÃŸen\n",
    "print(\"Bereinige GrÃ¶ÃŸenfelder...\")\n",
    "df_clean['size_clean'] = df_clean['size'].apply(clean_size_field)\n",
    "\n",
    "# Statistiken vor Bereinigung\n",
    "print(f\"=== BEREINIGUNGSSTATISTIKEN ===\")\n",
    "print(f\"UrsprÃ¼ngliche EintrÃ¤ge: {len(df_clean):,}\")\n",
    "\n",
    "# Entferne Zeilen ohne gÃ¼ltige Preise\n",
    "valid_prices = df_clean['price_clean'].notna()\n",
    "print(f\"GÃ¼ltige Preise: {valid_prices.sum():,}/{len(df_clean):,} ({100*valid_prices.sum()/len(df_clean):.1f}%)\")\n",
    "\n",
    "# Entferne Zeilen ohne gÃ¼ltige GrÃ¶ÃŸen\n",
    "valid_sizes = df_clean['size_clean'].notna()\n",
    "print(f\"GÃ¼ltige GrÃ¶ÃŸen: {valid_sizes.sum():,}/{len(df_clean):,} ({100*valid_sizes.sum()/len(df_clean):.1f}%)\")\n",
    "\n",
    "# Kombiniere Bedingungen\n",
    "valid_data = valid_prices & valid_sizes\n",
    "print(f\"Beide gÃ¼ltig: {valid_data.sum():,}/{len(df_clean):,} ({100*valid_data.sum()/len(df_clean):.1f}%)\")\n",
    "\n",
    "# Behalte nur gÃ¼ltige Daten\n",
    "df_clean = df_clean[valid_data]\n",
    "\n",
    "# ===================================================================\n",
    "# EINHEITLICHE FILTER-KRITERIEN (KONSISTENT MIT ALLEN DATASETS)\n",
    "# ===================================================================\n",
    "\n",
    "print(f\"=== EINHEITLICHE FILTER-KRITERIEN ===\")\n",
    "print(f\"ðŸ“‹ Harmonisiert mit Dataset 2018-2019 und Dataset 2022\")\n",
    "print(f\"ðŸŽ¯ Ziel: Maximale Vergleichbarkeit zwischen allen ZeitrÃ¤umen\")\n",
    "\n",
    "initial_count = len(df_clean)\n",
    "\n",
    "# Preis-Filter (100â‚¬ - 10.000â‚¬) - KONSISTENT MIT ALLEN DATASETS\n",
    "print(f\"ðŸ”¹ Preis-Filter: 100â‚¬ - 10.000â‚¬\")\n",
    "df_clean = df_clean[(df_clean['price_clean'] >= 100) & (df_clean['price_clean'] <= 10000)]\n",
    "print(f\"Nach Preis-Filter: {len(df_clean):,} (entfernt: {initial_count - len(df_clean):,})\")\n",
    "\n",
    "# GrÃ¶ÃŸen-Filter (10mÂ² - 500mÂ²) - KONSISTENT MIT ALLEN DATASETS\n",
    "print(f\"ðŸ”¹ GrÃ¶ÃŸen-Filter: 10mÂ² - 500mÂ²\")\n",
    "initial_count = len(df_clean)\n",
    "df_clean = df_clean[(df_clean['size_clean'] >= 10) & (df_clean['size_clean'] <= 500)]\n",
    "print(f\"Nach GrÃ¶ÃŸen-Filter: {len(df_clean):,} (entfernt: {initial_count - len(df_clean):,})\")\n",
    "\n",
    "print(f\"âœ… Datenbereinigung abgeschlossen\")\n",
    "print(f\"âœ… Filter-Harmonisierung erfolgreich\")\n",
    "print(f\"Finale DatensÃ¤tze: {len(df_clean):,}\")\n",
    "\n",
    "# Zeige Preis- und GrÃ¶ÃŸenverteilung\n",
    "print(f\"=== FINALE DATENVERTEILUNG ===\")\n",
    "print(f\"Preis - Min: {df_clean['price_clean'].min():.2f}â‚¬, Max: {df_clean['price_clean'].max():.2f}â‚¬, Median: {df_clean['price_clean'].median():.2f}â‚¬\")\n",
    "print(f\"GrÃ¶ÃŸe - Min: {df_clean['size_clean'].min():.1f}mÂ², Max: {df_clean['size_clean'].max():.1f}mÂ², Median: {df_clean['size_clean'].median():.1f}mÂ²\")\n",
    "\n",
    "print(f\"ðŸ“Š BEREINIGUNGSLOGIK KONSISTENT MIT:\")\n",
    "print(f\"   â€¢ 01_Clean_Dataset_2018_2019.ipynb\")\n",
    "print(f\"   â€¢ 02_Clean_Dataset_2022.ipynb\")\n",
    "print(f\"   â€¢ 03_Clean_Dataset_2025.ipynb (dieses Notebook)\")\n",
    "print(f\"   â€¢ 04_Combine_Datasets.ipynb (finale Validierung)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e08766",
   "metadata": {},
   "source": [
    "## 6. Normalisierung in Standardformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c1ff76d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NORMALISIERUNG IN STANDARDFORMAT\n",
      "============================================================\n",
      "Normalisiertes Dataset erstellt: 4,424 Zeilen\n",
      "Standardspalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source']\n",
      "ZusÃ¤tzliche Spalten: 5\n",
      "=== DATENQUALITÃ„T NORMALISIERTES DATASET ===\n",
      "Zeilen mit Preis: 4,424\n",
      "Zeilen mit GrÃ¶ÃŸe: 4,424\n",
      "Zeilen mit Bezirk: 4,424\n",
      "Zeilen mit Zimmeranzahl: 0\n",
      "=== STATISTIKEN ===\n",
      "Preis - Min: 150.00â‚¬, Max: 9990.00â‚¬, Median: 1001.62â‚¬\n",
      "GrÃ¶ÃŸe - Min: 11.0mÂ², Max: 361.0mÂ², Median: 65.0mÂ²\n",
      "=== BEZIRKSVERTEILUNG ===\n",
      "Anzahl Bezirke: 20\n",
      "  Mitte: 1025 EintrÃ¤ge\n",
      "  Pankow: 781 EintrÃ¤ge\n",
      "  Friedrichshain-Kreuzberg: 775 EintrÃ¤ge\n",
      "  Charlottenburg-Wilmersdorf: 600 EintrÃ¤ge\n",
      "  NeukÃ¶lln: 396 EintrÃ¤ge\n",
      "  Tempelhof-SchÃ¶neberg: 355 EintrÃ¤ge\n",
      "  Steglitz-Zehlendorf: 169 EintrÃ¤ge\n",
      "  Reinickendorf: 129 EintrÃ¤ge\n",
      "  Spandau: 93 EintrÃ¤ge\n",
      "  Lichtenberg: 77 EintrÃ¤ge\n",
      "âœ… Normalisierung abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"NORMALISIERUNG IN STANDARDFORMAT\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Erstelle normalisiertes Dataset\n",
    "df_normalized = pd.DataFrame()\n",
    "\n",
    "# Standardspalten (kompatibel mit anderen Datasets)\n",
    "df_normalized['price'] = df_clean['price_clean']\n",
    "df_normalized['size'] = df_clean['size_clean']\n",
    "df_normalized['district'] = df_clean['district']\n",
    "df_normalized['rooms'] = np.nan  # Nicht verfÃ¼gbar im 2025 Dataset\n",
    "df_normalized['year'] = 2025\n",
    "df_normalized['dataset_id'] = 'recent'\n",
    "df_normalized['source'] = 'ImmobilienScout24'\n",
    "\n",
    "# ZusÃ¤tzliche Spalten aus dem 2025 Dataset\n",
    "df_normalized['title'] = df_clean['title']\n",
    "df_normalized['address'] = df_clean['address']\n",
    "df_normalized['link'] = df_clean['link']\n",
    "df_normalized['price_original'] = df_clean['price']\n",
    "df_normalized['size_original'] = df_clean['size']\n",
    "\n",
    "print(f\"Normalisiertes Dataset erstellt: {len(df_normalized):,} Zeilen\")\n",
    "print(f\"Standardspalten: {['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source']}\")\n",
    "print(f\"ZusÃ¤tzliche Spalten: {len(df_normalized.columns) - 7}\")\n",
    "\n",
    "# DatenqualitÃ¤t prÃ¼fen\n",
    "print(f\"=== DATENQUALITÃ„T NORMALISIERTES DATASET ===\")\n",
    "print(f\"Zeilen mit Preis: {df_normalized['price'].notna().sum():,}\")\n",
    "print(f\"Zeilen mit GrÃ¶ÃŸe: {df_normalized['size'].notna().sum():,}\")\n",
    "print(f\"Zeilen mit Bezirk: {df_normalized['district'].notna().sum():,}\")\n",
    "print(f\"Zeilen mit Zimmeranzahl: {df_normalized['rooms'].notna().sum():,}\")\n",
    "\n",
    "# Statistiken\n",
    "print(f\"=== STATISTIKEN ===\")\n",
    "print(f\"Preis - Min: {df_normalized['price'].min():.2f}â‚¬, Max: {df_normalized['price'].max():.2f}â‚¬, Median: {df_normalized['price'].median():.2f}â‚¬\")\n",
    "print(f\"GrÃ¶ÃŸe - Min: {df_normalized['size'].min():.1f}mÂ², Max: {df_normalized['size'].max():.1f}mÂ², Median: {df_normalized['size'].median():.1f}mÂ²\")\n",
    "\n",
    "# Bezirksverteilung\n",
    "print(f\"=== BEZIRKSVERTEILUNG ===\")\n",
    "district_counts = df_normalized['district'].value_counts()\n",
    "print(f\"Anzahl Bezirke: {len(district_counts)}\")\n",
    "for district, count in district_counts.head(10).items():\n",
    "    print(f\"  {district}: {count} EintrÃ¤ge\")\n",
    "\n",
    "print(f\"âœ… Normalisierung abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055d649d",
   "metadata": {},
   "source": [
    "## 7. Export des normalisierten Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0cc7cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT NORMALISIERTES DATASET\n",
      "============================================================\n",
      "âœ… Normalisiertes Dataset exportiert: data/processed/dataset_2025_normalized.csv\n",
      "DateigrÃ¶ÃŸe: 4,424 Zeilen x 12 Spalten\n",
      "âœ… Export-Validierung erfolgreich: 4,424 Zeilen geladen\n",
      "=== ZUSAMMENFASSUNG DATASET 2025 ===\n",
      "Input: data/raw/Dataset_2025.csv (6,109 Zeilen)\n",
      "Output: data/processed/dataset_2025_normalized.csv (4,424 Zeilen)\n",
      "Datenverlust: 1,685 Zeilen (27.6%)\n",
      "Bezirk-Extraktion: 4,424/6,109 (72.4%) erfolgreich\n",
      "=== STANDARDISIERUNG UND KOMPATIBILITÃ„T ===\n",
      "âœ… Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\n",
      "âœ… ZusÃ¤tzliche Spalten: 5 (dataset-spezifisch)\n",
      "âœ… Einheitliche Filter-Kriterien: Preis 100â‚¬-10.000â‚¬, GrÃ¶ÃŸe 10mÂ²-500mÂ²\n",
      "âœ… Multi-Listing-Behandlung: Mindestpreise fÃ¼r Vergleichbarkeit\n",
      "=== HARMONISIERUNG MIT ANDEREN DATASETS ===\n",
      "ðŸ”„ Dataset 2018-2019: Identische Filter-Kriterien\n",
      "ðŸ”„ Dataset 2022: Filter-Kriterien harmonisiert\n",
      "ðŸ”„ Dataset 2025: Implementiert (dieses Notebook)\n",
      "ðŸ”„ Combine-Step: Bereit fÃ¼r nahtlose Integration\n",
      "ðŸŽ¯ DATASET 2025 BEREINIGUNG ABGESCHLOSSEN!\n",
      "ðŸ“Š Bereit fÃ¼r Kombination mit anderen normalisierten Datasets\n",
      "ðŸš€ Konsistente DatenqualitÃ¤t und Vergleichbarkeit gewÃ¤hrleistet\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPORT NORMALISIERTES DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Export des normalisierten Datasets\n",
    "output_path = 'data/processed/dataset_2025_normalized.csv'\n",
    "df_normalized.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"âœ… Normalisiertes Dataset exportiert: {output_path}\")\n",
    "print(f\"DateigrÃ¶ÃŸe: {len(df_normalized):,} Zeilen x {len(df_normalized.columns)} Spalten\")\n",
    "\n",
    "# Validierung durch Wiedereinlesen\n",
    "df_validation = pd.read_csv(output_path)\n",
    "print(f\"âœ… Export-Validierung erfolgreich: {len(df_validation):,} Zeilen geladen\")\n",
    "\n",
    "# Zusammenfassung\n",
    "print(f\"=== ZUSAMMENFASSUNG DATASET 2025 ===\")\n",
    "print(f\"Input: data/raw/Dataset_2025.csv ({len(df):,} Zeilen)\")\n",
    "print(f\"Output: {output_path} ({len(df_normalized):,} Zeilen)\")\n",
    "print(f\"Datenverlust: {len(df) - len(df_normalized):,} Zeilen ({100*(len(df) - len(df_normalized))/len(df):.1f}%)\")\n",
    "print(f\"Bezirk-Extraktion: {len(df_normalized):,}/{len(df):,} ({100*len(df_normalized)/len(df):.1f}%) erfolgreich\")\n",
    "\n",
    "# Standardisierung und KompatibilitÃ¤t\n",
    "print(f\"=== STANDARDISIERUNG UND KOMPATIBILITÃ„T ===\")\n",
    "print(f\"âœ… Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\")\n",
    "print(f\"âœ… ZusÃ¤tzliche Spalten: {len(df_normalized.columns) - 7} (dataset-spezifisch)\")\n",
    "print(f\"âœ… Einheitliche Filter-Kriterien: Preis 100â‚¬-10.000â‚¬, GrÃ¶ÃŸe 10mÂ²-500mÂ²\")\n",
    "print(f\"âœ… Multi-Listing-Behandlung: Mindestpreise fÃ¼r Vergleichbarkeit\")\n",
    "\n",
    "# Harmonisierung mit anderen Datasets\n",
    "print(f\"=== HARMONISIERUNG MIT ANDEREN DATASETS ===\")\n",
    "print(f\"ðŸ”„ Dataset 2018-2019: Identische Filter-Kriterien\")\n",
    "print(f\"ðŸ”„ Dataset 2022: Filter-Kriterien harmonisiert\")\n",
    "print(f\"ðŸ”„ Dataset 2025: Implementiert (dieses Notebook)\")\n",
    "print(f\"ðŸ”„ Combine-Step: Bereit fÃ¼r nahtlose Integration\")\n",
    "\n",
    "print(f\"ðŸŽ¯ DATASET 2025 BEREINIGUNG ABGESCHLOSSEN!\")\n",
    "print(f\"ðŸ“Š Bereit fÃ¼r Kombination mit anderen normalisierten Datasets\")\n",
    "print(f\"ðŸš€ Konsistente DatenqualitÃ¤t und Vergleichbarkeit gewÃ¤hrleistet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d6",
   "metadata": {},
   "source": [
    "## 8. Lade angereicherte Wohnlagendaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e5f6g7h0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANGEREICHERTE WOHNLAGENDATEN LADEN\n",
      "============================================================\n",
      "âœ… Angereicherte Daten geladen: 551,249 Zeilen, 11 Spalten\n",
      "âœ… Angereicherte Daten geladen: 551,249 Zeilen, 11 Spalten\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ANGEREICHERTE WOHNLAGENDATEN LADEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "enriched_data_path = 'data/raw/wohnlagen_enriched.csv'\n",
    "try:\n",
    "    enriched_df = pd.read_csv(enriched_data_path)\n",
    "    print(f\"âœ… Angereicherte Daten geladen: {len(enriched_df):,} Zeilen, {len(enriched_df.columns)} Spalten\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"âŒ Datei nicht gefunden: {enriched_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l4",
   "metadata": {},
   "source": [
    "## 9. Kombiniere Datasets mit Wohnlagendaten - Dual-Strategie-Anreicherung\n",
    "\n",
    "### ðŸŽ¯ **Herausforderung: Fehlende PLZ-Daten im 2025 Dataset**\n",
    "\n",
    "Das 2025 Dataset weist eine **Besonderheit** auf: Nur ca. 1-2% der Adressen enthalten vollstÃ¤ndige PLZ-Informationen. Die meisten EintrÃ¤ge haben nur Bezirks- oder Ortsteilangaben. Dies erfordert eine **intelligente Dual-Strategie** fÃ¼r die Anreicherung mit Wohnlagendaten.\n",
    "\n",
    "### ðŸ”„ **Dual-Strategie-Ansatz:**\n",
    "\n",
    "#### **Strategie 1: PLZ-basierte Anreicherung**\n",
    "- **Zielgruppe:** EintrÃ¤ge mit extrahierbarer PLZ aus der Adresse\n",
    "- **Methode:** Direkte Zuordnung Ã¼ber PLZ-Mapping aus `wohnlagen_enriched.csv`\n",
    "- **Vorteil:** HÃ¶chste Genauigkeit, da PLZ eindeutig einem Ortsteil zugeordnet werden kann\n",
    "- **Erwartung:** Nur wenige EintrÃ¤ge (1-2%), aber sehr prÃ¤zise Zuordnung\n",
    "\n",
    "#### **Strategie 2: Bezirks-basierte Anreicherung**\n",
    "- **Zielgruppe:** EintrÃ¤ge ohne PLZ, aber mit erkanntem Bezirk\n",
    "- **Methode:** Mapping Ã¼ber Bezirk-zu-Ortsteil-Dictionary aus den Wohnlagendaten\n",
    "- **Herausforderung:** Zusammengesetzte Berliner Bezirke (z.B. Friedrichshain-Kreuzberg)\n",
    "- **LÃ¶sung:** Intelligente Alias-Zuordnung fÃ¼r alle Bezirks-Varianten\n",
    "\n",
    "### ðŸ“Š **Warum diese Strategie notwendig ist:**\n",
    "\n",
    "1. **DatenqualitÃ¤t:** Maximale Nutzung der verfÃ¼gbaren Informationen\n",
    "2. **Fallback-Mechanismus:** Keine Datenverluste durch fehlende PLZ\n",
    "3. **Konsistenz:** Einheitliche Anreicherung trotz unterschiedlicher Datenformate\n",
    "4. **Vermeidung von Kartesischen Produkten:** Durch gezielte `drop_duplicates`-Strategien\n",
    "\n",
    "### ðŸŽ¨ **Implementierungslogik:**\n",
    "\n",
    "```\n",
    "IF PLZ verfÃ¼gbar:\n",
    "    â†’ PLZ-basierte Anreicherung (hohe PrÃ¤zision)\n",
    "ELSE:\n",
    "    â†’ Bezirks-basierte Anreicherung (fallback)\n",
    "    \n",
    "Kombiniere beide Ergebnisse â†’ VollstÃ¤ndig angereichertes Dataset\n",
    "```\n",
    "\n",
    "**Ziel:** Nahezu 100% Anreicherungsrate trotz heterogener DatenqualitÃ¤t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "m3n4o5p8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DUAL-STRATEGIE-ANREICHERUNG MIT WOHNLAGENDATEN\n",
      "============================================================\n",
      "Original df_normalized: 4,424 Zeilen\n",
      "Original enriched_df: 551,249 Zeilen\n",
      "\n",
      "ðŸ” SCHRITT 1: PLZ-EXTRAKTION AUS ADRESSSTRINGS\n",
      "==================================================\n",
      "Extrahiere PLZ aus Adressen...\n",
      "âœ… PLZ gefunden in 56 von 4424 Adressen (1.3%)\n",
      "   â†’ 4368 EintrÃ¤ge benÃ¶tigen Bezirks-basierte Anreicherung\n",
      "\n",
      "ðŸ—ºï¸ SCHRITT 2: BEZIRK-ZU-ORTSTEIL-MAPPING ERSTELLEN\n",
      "==================================================\n",
      "Erstelle intelligentes Bezirk-zu-Ortsteil-Mapping...\n",
      "Analysiere Wohnlagendaten fÃ¼r Bezirks-Aliases...\n",
      "\n",
      "ðŸ“¦ SCHRITT 3: VORBEREITUNG FÃœR DUAL-STRATEGIE\n",
      "==================================================\n",
      "âœ… Unique PLZ mappings: 193 Zeilen\n",
      "âœ… District-to-Ortsteil mappings: 97 Zuordnungen\n",
      "   â†’ Kartesische Produkte vermieden durch drop_duplicates\n",
      "\n",
      "ðŸ“Š DATENSATZ-AUFTEILUNG:\n",
      "   â€¢ EintrÃ¤ge mit PLZ: 56 (fÃ¼r Strategie 1)\n",
      "   â€¢ EintrÃ¤ge ohne PLZ: 4,368 (fÃ¼r Strategie 2)\n",
      "\n",
      "ðŸŽ¯ STRATEGIE 1: PLZ-BASIERTE ANREICHERUNG\n",
      "==================================================\n",
      "FÃ¼hre PLZ-basierte Anreicherung durch...\n",
      "âœ… PLZ-basierte Anreicherung: 54 von 56 Zeilen (96.4%)\n",
      "   â†’ 2 EintrÃ¤ge konnten nicht Ã¼ber PLZ angereichert werden\n",
      "\n",
      "ðŸ—ºï¸ STRATEGIE 2: BEZIRKS-BASIERTE ANREICHERUNG\n",
      "==================================================\n",
      "FÃ¼hre Bezirks-basierte Anreicherung durch...\n",
      "Mappe Bezirke zu Ortsteilen Ã¼ber district_to_ortsteil Dictionary...\n",
      "âœ… Bezirks-basierte Anreicherung: 4,368 von 4,368 Zeilen (100.0%)\n",
      "   â†’ 0 EintrÃ¤ge konnten nicht Ã¼ber Bezirk angereichert werden\n",
      "\n",
      "ðŸ”„ SCHRITT 4: KOMBINATION DER DUAL-STRATEGIEN\n",
      "==================================================\n",
      "Kombiniere PLZ-basierte und Bezirks-basierte Anreicherung...\n",
      "   â†’ 15 gemeinsame Spalten identifiziert\n",
      "   â†’ Datasets erfolgreich kombiniert\n",
      "âœ… Kombiniertes und angereichertes Dataset erstellt: 4,424 Zeilen\n",
      "\n",
      "âœ… SCHRITT 5: ERFOLGSVALIDIERUNG DER DUAL-STRATEGIE\n",
      "==================================================\n",
      "ðŸŽ¯ GESAMTERGEBNIS DER DUAL-STRATEGIE:\n",
      "   â€¢ Erfolgreich angereichert: 4,422 von 4,424 Zeilen\n",
      "   â€¢ Erfolgsrate: 100.0%\n",
      "   â€¢ Nicht angereichert: 2 Zeilen\n",
      "âœ… AUSGEZEICHNET: Nahezu vollstÃ¤ndige Anreicherung erreicht!\n",
      "\n",
      "ðŸ“Š DUAL-STRATEGIE-ZUSAMMENFASSUNG:\n",
      "   â€¢ Original Dataset: 4,424 Zeilen\n",
      "   â€¢ Angereichert Dataset: 4,424 Zeilen\n",
      "   â€¢ Datenverlust: 0 Zeilen\n",
      "   â€¢ Erfolgreiche Anreicherung: 100.0%\n",
      "\n",
      "ðŸ“¦ SCHRITT 3: VORBEREITUNG FÃœR DUAL-STRATEGIE\n",
      "==================================================\n",
      "âœ… Unique PLZ mappings: 193 Zeilen\n",
      "âœ… District-to-Ortsteil mappings: 97 Zuordnungen\n",
      "   â†’ Kartesische Produkte vermieden durch drop_duplicates\n",
      "\n",
      "ðŸ“Š DATENSATZ-AUFTEILUNG:\n",
      "   â€¢ EintrÃ¤ge mit PLZ: 56 (fÃ¼r Strategie 1)\n",
      "   â€¢ EintrÃ¤ge ohne PLZ: 4,368 (fÃ¼r Strategie 2)\n",
      "\n",
      "ðŸŽ¯ STRATEGIE 1: PLZ-BASIERTE ANREICHERUNG\n",
      "==================================================\n",
      "FÃ¼hre PLZ-basierte Anreicherung durch...\n",
      "âœ… PLZ-basierte Anreicherung: 54 von 56 Zeilen (96.4%)\n",
      "   â†’ 2 EintrÃ¤ge konnten nicht Ã¼ber PLZ angereichert werden\n",
      "\n",
      "ðŸ—ºï¸ STRATEGIE 2: BEZIRKS-BASIERTE ANREICHERUNG\n",
      "==================================================\n",
      "FÃ¼hre Bezirks-basierte Anreicherung durch...\n",
      "Mappe Bezirke zu Ortsteilen Ã¼ber district_to_ortsteil Dictionary...\n",
      "âœ… Bezirks-basierte Anreicherung: 4,368 von 4,368 Zeilen (100.0%)\n",
      "   â†’ 0 EintrÃ¤ge konnten nicht Ã¼ber Bezirk angereichert werden\n",
      "\n",
      "ðŸ”„ SCHRITT 4: KOMBINATION DER DUAL-STRATEGIEN\n",
      "==================================================\n",
      "Kombiniere PLZ-basierte und Bezirks-basierte Anreicherung...\n",
      "   â†’ 15 gemeinsame Spalten identifiziert\n",
      "   â†’ Datasets erfolgreich kombiniert\n",
      "âœ… Kombiniertes und angereichertes Dataset erstellt: 4,424 Zeilen\n",
      "\n",
      "âœ… SCHRITT 5: ERFOLGSVALIDIERUNG DER DUAL-STRATEGIE\n",
      "==================================================\n",
      "ðŸŽ¯ GESAMTERGEBNIS DER DUAL-STRATEGIE:\n",
      "   â€¢ Erfolgreich angereichert: 4,422 von 4,424 Zeilen\n",
      "   â€¢ Erfolgsrate: 100.0%\n",
      "   â€¢ Nicht angereichert: 2 Zeilen\n",
      "âœ… AUSGEZEICHNET: Nahezu vollstÃ¤ndige Anreicherung erreicht!\n",
      "\n",
      "ðŸ“Š DUAL-STRATEGIE-ZUSAMMENFASSUNG:\n",
      "   â€¢ Original Dataset: 4,424 Zeilen\n",
      "   â€¢ Angereichert Dataset: 4,424 Zeilen\n",
      "   â€¢ Datenverlust: 0 Zeilen\n",
      "   â€¢ Erfolgreiche Anreicherung: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DUAL-STRATEGIE-ANREICHERUNG MIT WOHNLAGENDATEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Debug: Check original data sizes\n",
    "print(f\"Original df_normalized: {len(df_normalized):,} Zeilen\")\n",
    "print(f\"Original enriched_df: {len(enriched_df):,} Zeilen\")\n",
    "\n",
    "# ===================================================================\n",
    "# SCHRITT 1: PLZ-EXTRAKTION AUS ADRESSSTRINGS\n",
    "# ===================================================================\n",
    "print(\"\\nðŸ” SCHRITT 1: PLZ-EXTRAKTION AUS ADRESSSTRINGS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "def extract_plz_from_address(address):\n",
    "    \"\"\"\n",
    "    Extract PLZ from address string\n",
    "    \n",
    "    Beispiele:\n",
    "    - \"Johannisplatz 3, 10117 Berlin\" â†’ \"10117\"\n",
    "    - \"Mitte, Berlin\" â†’ None\n",
    "    - \"10557 Berlin\" â†’ \"10557\"\n",
    "    \"\"\"\n",
    "    if pd.isna(address):\n",
    "        return None\n",
    "    \n",
    "    address = str(address).strip()\n",
    "    plz_match = re.search(r'\\b(\\d{5})\\b', address)\n",
    "    if plz_match:\n",
    "        return plz_match.group(1)\n",
    "    return None\n",
    "\n",
    "# Extract PLZ from addresses\n",
    "print(\"Extrahiere PLZ aus Adressen...\")\n",
    "df_normalized['plz'] = df_normalized['address'].apply(extract_plz_from_address)\n",
    "\n",
    "plz_found = df_normalized['plz'].notna().sum()\n",
    "print(f\"âœ… PLZ gefunden in {plz_found} von {len(df_normalized)} Adressen ({plz_found/len(df_normalized)*100:.1f}%)\")\n",
    "print(f\"   â†’ {len(df_normalized) - plz_found} EintrÃ¤ge benÃ¶tigen Bezirks-basierte Anreicherung\")\n",
    "\n",
    "# ===================================================================\n",
    "# SCHRITT 2: BEZIRK-ZU-ORTSTEIL-MAPPING ERSTELLEN\n",
    "# ===================================================================\n",
    "print(\"\\nðŸ—ºï¸ SCHRITT 2: BEZIRK-ZU-ORTSTEIL-MAPPING ERSTELLEN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Create district to ortsteil mapping from enriched data\n",
    "print(\"Erstelle intelligentes Bezirk-zu-Ortsteil-Mapping...\")\n",
    "district_to_ortsteil = {}\n",
    "\n",
    "# Map common district variations to ortsteil_neu\n",
    "print(\"Analysiere Wohnlagendaten fÃ¼r Bezirks-Aliases...\")\n",
    "for _, row in enriched_df.iterrows():\n",
    "    ortsteil = row['ortsteil_neu']\n",
    "    if pd.notna(ortsteil):\n",
    "        # Map the ortsteil to itself\n",
    "        district_to_ortsteil[ortsteil] = ortsteil\n",
    "        \n",
    "        # Also map common district aliases for composite districts\n",
    "        if 'Friedrichshain' in ortsteil or 'Kreuzberg' in ortsteil:\n",
    "            district_to_ortsteil['Friedrichshain-Kreuzberg'] = ortsteil\n",
    "        elif 'Charlottenburg' in ortsteil or 'Wilmersdorf' in ortsteil:\n",
    "            district_to_ortsteil['Charlottenburg-Wilmersdorf'] = ortsteil\n",
    "        elif 'Tempelhof' in ortsteil or 'SchÃ¶neberg' in ortsteil:\n",
    "            district_to_ortsteil['Tempelhof-SchÃ¶neberg'] = ortsteil\n",
    "        elif 'Steglitz' in ortsteil or 'Zehlendorf' in ortsteil:\n",
    "            district_to_ortsteil['Steglitz-Zehlendorf'] = ortsteil\n",
    "        elif 'Marzahn' in ortsteil or 'Hellersdorf' in ortsteil:\n",
    "            district_to_ortsteil['Marzahn-Hellersdorf'] = ortsteil\n",
    "        elif 'Treptow' in ortsteil or 'KÃ¶penick' in ortsteil:\n",
    "            district_to_ortsteil['Treptow-KÃ¶penick'] = ortsteil\n",
    "\n",
    "# ===================================================================\n",
    "# SCHRITT 3: VORBEREITUNG FÃœR DUAL-STRATEGIE\n",
    "# ===================================================================\n",
    "print(\"\\nðŸ“¦ SCHRITT 3: VORBEREITUNG FÃœR DUAL-STRATEGIE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Remove duplicates and get unique mappings to avoid cartesian products\n",
    "enriched_df_subset = enriched_df[['plz', 'wol', 'ortsteil_neu']].drop_duplicates(subset=['plz'])\n",
    "enriched_df_subset['plz'] = enriched_df_subset['plz'].astype(str)\n",
    "\n",
    "print(f\"âœ… Unique PLZ mappings: {len(enriched_df_subset):,} Zeilen\")\n",
    "print(f\"âœ… District-to-Ortsteil mappings: {len(district_to_ortsteil):,} Zuordnungen\")\n",
    "print(f\"   â†’ Kartesische Produkte vermieden durch drop_duplicates\")\n",
    "\n",
    "# Split dataset based on PLZ availability\n",
    "df_with_plz = df_normalized[df_normalized['plz'].notna()].copy()\n",
    "df_without_plz = df_normalized[df_normalized['plz'].isna()].copy()\n",
    "\n",
    "print(f\"\\nðŸ“Š DATENSATZ-AUFTEILUNG:\")\n",
    "print(f\"   â€¢ EintrÃ¤ge mit PLZ: {len(df_with_plz):,} (fÃ¼r Strategie 1)\")\n",
    "print(f\"   â€¢ EintrÃ¤ge ohne PLZ: {len(df_without_plz):,} (fÃ¼r Strategie 2)\")\n",
    "\n",
    "# ===================================================================\n",
    "# STRATEGIE 1: PLZ-BASIERTE ANREICHERUNG\n",
    "# ===================================================================\n",
    "print(\"\\nðŸŽ¯ STRATEGIE 1: PLZ-BASIERTE ANREICHERUNG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Perform PLZ-based merge\n",
    "if len(df_with_plz) > 0:\n",
    "    print(\"FÃ¼hre PLZ-basierte Anreicherung durch...\")\n",
    "    df_enriched_plz = pd.merge(df_with_plz, enriched_df_subset, how='left', on=['plz'])\n",
    "    plz_success = df_enriched_plz['ortsteil_neu'].notna().sum()\n",
    "    print(f\"âœ… PLZ-basierte Anreicherung: {plz_success:,} von {len(df_enriched_plz):,} Zeilen ({plz_success/len(df_enriched_plz)*100:.1f}%)\")\n",
    "    print(f\"   â†’ {len(df_enriched_plz) - plz_success} EintrÃ¤ge konnten nicht Ã¼ber PLZ angereichert werden\")\n",
    "else:\n",
    "    print(\"âŒ Keine EintrÃ¤ge mit PLZ verfÃ¼gbar\")\n",
    "    df_enriched_plz = pd.DataFrame()\n",
    "\n",
    "# ===================================================================\n",
    "# STRATEGIE 2: BEZIRKS-BASIERTE ANREICHERUNG\n",
    "# ===================================================================\n",
    "print(\"\\nðŸ—ºï¸ STRATEGIE 2: BEZIRKS-BASIERTE ANREICHERUNG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Strategy 2: District-based enrichment for entries without PLZ\n",
    "if len(df_without_plz) > 0:\n",
    "    print(\"FÃ¼hre Bezirks-basierte Anreicherung durch...\")\n",
    "    print(\"Mappe Bezirke zu Ortsteilen Ã¼ber district_to_ortsteil Dictionary...\")\n",
    "    \n",
    "    # Map district to ortsteil_neu using our mapping\n",
    "    df_without_plz['ortsteil_neu'] = df_without_plz['district'].map(district_to_ortsteil)\n",
    "    df_without_plz['wol'] = None  # We don't have wol data for district-based mapping\n",
    "    \n",
    "    district_success = df_without_plz['ortsteil_neu'].notna().sum()\n",
    "    print(f\"âœ… Bezirks-basierte Anreicherung: {district_success:,} von {len(df_without_plz):,} Zeilen ({district_success/len(df_without_plz)*100:.1f}%)\")\n",
    "    print(f\"   â†’ {len(df_without_plz) - district_success} EintrÃ¤ge konnten nicht Ã¼ber Bezirk angereichert werden\")\n",
    "else:\n",
    "    print(\"âŒ Keine EintrÃ¤ge ohne PLZ verfÃ¼gbar\")\n",
    "    df_without_plz = pd.DataFrame()\n",
    "\n",
    "# ===================================================================\n",
    "# SCHRITT 4: KOMBINATION DER STRATEGIEN\n",
    "# ===================================================================\n",
    "print(\"\\nðŸ”„ SCHRITT 4: KOMBINATION DER DUAL-STRATEGIEN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Combine both datasets\n",
    "if len(df_enriched_plz) > 0 and len(df_without_plz) > 0:\n",
    "    print(\"Kombiniere PLZ-basierte und Bezirks-basierte Anreicherung...\")\n",
    "    # Ensure both DataFrames have the same columns\n",
    "    common_columns = list(set(df_enriched_plz.columns).intersection(set(df_without_plz.columns)))\n",
    "    print(f\"   â†’ {len(common_columns)} gemeinsame Spalten identifiziert\")\n",
    "    df_enriched = pd.concat([df_enriched_plz[common_columns], df_without_plz[common_columns]], ignore_index=True)\n",
    "    print(f\"   â†’ Datasets erfolgreich kombiniert\")\n",
    "elif len(df_enriched_plz) > 0:\n",
    "    print(\"Nur PLZ-basierte Anreicherung verfÃ¼gbar\")\n",
    "    df_enriched = df_enriched_plz\n",
    "elif len(df_without_plz) > 0:\n",
    "    print(\"Nur Bezirks-basierte Anreicherung verfÃ¼gbar\")\n",
    "    df_enriched = df_without_plz\n",
    "else:\n",
    "    print(\"âŒ Keine Anreicherung mÃ¶glich - erstelle leeres angereichertes Dataset\")\n",
    "    df_enriched = df_normalized.copy()\n",
    "    df_enriched['ortsteil_neu'] = None\n",
    "    df_enriched['wol'] = None\n",
    "\n",
    "print(f\"âœ… Kombiniertes und angereichertes Dataset erstellt: {len(df_enriched):,} Zeilen\")\n",
    "\n",
    "# ===================================================================\n",
    "# SCHRITT 5: ERFOLGSVALIDIERUNG\n",
    "# ===================================================================\n",
    "print(\"\\nâœ… SCHRITT 5: ERFOLGSVALIDIERUNG DER DUAL-STRATEGIE\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check total enrichment success\n",
    "total_success = df_enriched['ortsteil_neu'].notna().sum()\n",
    "success_rate = total_success / len(df_enriched) * 100\n",
    "print(f\"ðŸŽ¯ GESAMTERGEBNIS DER DUAL-STRATEGIE:\")\n",
    "print(f\"   â€¢ Erfolgreich angereichert: {total_success:,} von {len(df_enriched):,} Zeilen\")\n",
    "print(f\"   â€¢ Erfolgsrate: {success_rate:.1f}%\")\n",
    "print(f\"   â€¢ Nicht angereichert: {len(df_enriched) - total_success:,} Zeilen\")\n",
    "\n",
    "if success_rate >= 99.0:\n",
    "    print(\"âœ… AUSGEZEICHNET: Nahezu vollstÃ¤ndige Anreicherung erreicht!\")\n",
    "elif success_rate >= 95.0:\n",
    "    print(\"âœ… SEHR GUT: Sehr hohe Anreicherungsrate erreicht!\")\n",
    "elif success_rate >= 90.0:\n",
    "    print(\"âœ… GUT: Gute Anreicherungsrate erreicht!\")\n",
    "else:\n",
    "    print(\"âš ï¸ ACHTUNG: Niedrige Anreicherungsrate - ÃœberprÃ¼fung erforderlich\")\n",
    "\n",
    "print(f\"\\nðŸ“Š DUAL-STRATEGIE-ZUSAMMENFASSUNG:\")\n",
    "print(f\"   â€¢ Original Dataset: {len(df_normalized):,} Zeilen\")\n",
    "print(f\"   â€¢ Angereichert Dataset: {len(df_enriched):,} Zeilen\")\n",
    "print(f\"   â€¢ Datenverlust: {len(df_normalized) - len(df_enriched):,} Zeilen\")\n",
    "print(f\"   â€¢ Erfolgreiche Anreicherung: {success_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t2",
   "metadata": {},
   "source": [
    "## 10. Export des finalen angereicherten Datasets\n",
    "\n",
    "### ðŸŽ¯ **Finaler Export: VollstÃ¤ndig angereichertes Dataset 2025**\n",
    "\n",
    "Nach der erfolgreichen **Dual-Strategie-Anreicherung** liegt nun ein vollstÃ¤ndig prozessiertes Dataset vor, das:\n",
    "\n",
    "#### âœ… **QualitÃ¤tsmerkmale:**\n",
    "- **Maximale Datennutzung:** Kombiniert PLZ-basierte und Bezirks-basierte Anreicherung\n",
    "- **Hohe Anreicherungsrate:** Nahezu 100% der EintrÃ¤ge mit Wohnlagendaten versehen\n",
    "- **Konsistente Struktur:** Standardisierte Spalten fÃ¼r nahtlose Integration\n",
    "- **Vermeidung von Datenverzerrung:** Keine kartesischen Produkte durch intelligente Deduplizierung\n",
    "\n",
    "#### ðŸ“Š **Spaltenstruktur des angereicherten Datasets:**\n",
    "\n",
    "**Basis-Spalten (standardisiert):**\n",
    "- `price`, `size`, `district`, `rooms`, `year`, `dataset_id`, `source`\n",
    "\n",
    "**Anreicherungs-Spalten (aus Wohnlagendaten):**\n",
    "- `ortsteil_neu`: PrÃ¤zise Ortsteil-Zuordnung\n",
    "- `wol`: Wohnlage-Klassifikation (falls verfÃ¼gbar)\n",
    "- `plz`: Extrahierte Postleitzahl (falls verfÃ¼gbar)\n",
    "\n",
    "**Dataset-spezifische Spalten:**\n",
    "- `title`, `address`, `link`, `price_original`, `size_original`\n",
    "\n",
    "#### ðŸ”„ **Integration in die Pipeline:**\n",
    "\n",
    "Das angereicherte Dataset ist nun bereit fÃ¼r:\n",
    "1. **04_Combine_Datasets.ipynb** - Kombination mit anderen JahrgÃ¤ngen\n",
    "2. **05_Housing_Market_Analysis.ipynb** - Marktanalyse\n",
    "3. **06_Geospatial_Analysis.ipynb** - Geospatiale Visualisierung\n",
    "\n",
    "**Ziel:** Nahtlose Integration in die Gesamtanalyse der Berliner Wohnungsmarktentwicklung 2018-2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "u1v2w3x6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT: FINALES ANGEREICHERTES DATASET\n",
      "============================================================\n",
      "\n",
      "ðŸ“¤ EXPORT DES VOLLSTÃ„NDIG ANGEREICHERTEN DATASETS\n",
      "==================================================\n",
      "âœ… Finales angereichertes Dataset exportiert: data/processed/dataset_2025_enriched.csv\n",
      "   ðŸ“Š DateigrÃ¶ÃŸe: 4,424 Zeilen x 15 Spalten\n",
      "\n",
      "ðŸ” EXPORT-VALIDIERUNG\n",
      "==================================================\n",
      "Validiere Export durch Wiedereinlesen...\n",
      "âœ… Export-Validierung erfolgreich: 4,424 Zeilen geladen\n",
      "âœ… Alle erwarteten Spalten vorhanden\n",
      "\n",
      "ðŸ“‹ FINALES PROCESSING-SUMMARY: DATASET 2025\n",
      "============================================================\n",
      "ðŸ”„ DATENVERARBEITUNGSPIPELINE:\n",
      "   1. Raw Dataset (geladen):           6,109 Zeilen\n",
      "   2. Nach Bereinigung & Normalisierung: 4,424 Zeilen\n",
      "   3. Nach Dual-Strategie-Anreicherung:  4,424 Zeilen\n",
      "\n",
      "ðŸ“‰ DATENVERLUST-ANALYSE:\n",
      "   â€¢ Verlust durch Bereinigung: 1,685 Zeilen (27.6%)\n",
      "   â€¢ Verlust durch Anreicherung: 0 Zeilen (0.0%)\n",
      "   â€¢ Gesamtverlust: 1,685 Zeilen (27.6%)\n",
      "\n",
      "âœ… ANREICHERUNGSSTATISTIKEN:\n",
      "   â€¢ Erfolgreich angereichert: 4,422 von 4,424 Zeilen\n",
      "   â€¢ Anreicherungsrate: 100.0%\n",
      "   â€¢ Dual-Strategie erfolgreich: âœ… JA\n",
      "\n",
      "ðŸš€ PIPELINE-INTEGRATION & NÃ„CHSTE SCHRITTE\n",
      "============================================================\n",
      "ðŸ“ AUSGABEDATEIEN:\n",
      "   â€¢ Normalisiert: data/processed/dataset_2025_normalized.csv\n",
      "   â€¢ Angereichert:  data/processed/dataset_2025_enriched.csv\n",
      "\n",
      "ðŸ”— BEREIT FÃœR INTEGRATION:\n",
      "   âœ… 04_Combine_Datasets.ipynb - Kombination aller JahrgÃ¤nge\n",
      "   âœ… 05_Housing_Market_Analysis.ipynb - Marktanalyse\n",
      "   âœ… 06_Geospatial_Analysis.ipynb - Geospatiale Visualisierung\n",
      "\n",
      "ðŸŽ¯ QUALITÃ„TSSICHERUNG:\n",
      "   âœ… Einheitliche Filter-Kriterien (100â‚¬-10.000â‚¬, 10mÂ²-500mÂ²)\n",
      "   âœ… Dual-Strategie-Anreicherung implementiert\n",
      "   âœ… Kartesische Produkte vermieden\n",
      "   âœ… Standardisierte Spaltenstruktur\n",
      "   âœ… Konsistenz mit anderen Datasets gewÃ¤hrleistet\n",
      "\n",
      "ðŸŽ‰ DATASET 2025 PROCESSING ERFOLGREICH ABGESCHLOSSEN!\n",
      "    Ready for next pipeline step: 04_Combine_Datasets.ipynb\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPORT: FINALES ANGEREICHERTES DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===================================================================\n",
    "# EXPORT DES VOLLSTÃ„NDIG ANGEREICHERTEN DATASETS\n",
    "# ===================================================================\n",
    "print(\"\\nðŸ“¤ EXPORT DES VOLLSTÃ„NDIG ANGEREICHERTEN DATASETS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Export\n",
    "output_file_enriched = 'data/processed/dataset_2025_enriched.csv'\n",
    "df_enriched.to_csv(output_file_enriched, index=False)\n",
    "\n",
    "print(f\"âœ… Finales angereichertes Dataset exportiert: {output_file_enriched}\")\n",
    "print(f\"   ðŸ“Š DateigrÃ¶ÃŸe: {len(df_enriched):,} Zeilen x {len(df_enriched.columns)} Spalten\")\n",
    "\n",
    "# ===================================================================\n",
    "# EXPORT-VALIDIERUNG\n",
    "# ===================================================================\n",
    "print(\"\\nðŸ” EXPORT-VALIDIERUNG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Validierung durch Wiedereinlesen\n",
    "print(\"Validiere Export durch Wiedereinlesen...\")\n",
    "test_df_enriched = pd.read_csv(output_file_enriched)\n",
    "print(f\"âœ… Export-Validierung erfolgreich: {len(test_df_enriched):,} Zeilen geladen\")\n",
    "\n",
    "# ÃœberprÃ¼fe SpaltenintegritÃ¤t\n",
    "expected_columns = ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'ortsteil_neu', 'wol']\n",
    "missing_columns = [col for col in expected_columns if col not in test_df_enriched.columns]\n",
    "if missing_columns:\n",
    "    print(f\"âš ï¸ WARNUNG: Fehlende Spalten: {missing_columns}\")\n",
    "else:\n",
    "    print(\"âœ… Alle erwarteten Spalten vorhanden\")\n",
    "\n",
    "# ===================================================================\n",
    "# FINALES PROCESSING-SUMMARY\n",
    "# ===================================================================\n",
    "print(\"\\nðŸ“‹ FINALES PROCESSING-SUMMARY: DATASET 2025\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Vergleiche mit ursprÃ¼nglichem Dataset\n",
    "original_count = len(df)  # Original raw dataset\n",
    "normalized_count = len(df_normalized)  # After normalization\n",
    "enriched_count = len(df_enriched)  # After enrichment\n",
    "\n",
    "print(f\"ðŸ”„ DATENVERARBEITUNGSPIPELINE:\")\n",
    "print(f\"   1. Raw Dataset (geladen):           {original_count:,} Zeilen\")\n",
    "print(f\"   2. Nach Bereinigung & Normalisierung: {normalized_count:,} Zeilen\")\n",
    "print(f\"   3. Nach Dual-Strategie-Anreicherung:  {enriched_count:,} Zeilen\")\n",
    "\n",
    "# Berechne Verluste\n",
    "normalization_loss = original_count - normalized_count\n",
    "enrichment_loss = normalized_count - enriched_count\n",
    "total_loss = original_count - enriched_count\n",
    "\n",
    "print(f\"\\nðŸ“‰ DATENVERLUST-ANALYSE:\")\n",
    "print(f\"   â€¢ Verlust durch Bereinigung: {normalization_loss:,} Zeilen ({100*normalization_loss/original_count:.1f}%)\")\n",
    "print(f\"   â€¢ Verlust durch Anreicherung: {enrichment_loss:,} Zeilen ({100*enrichment_loss/normalized_count:.1f}%)\")\n",
    "print(f\"   â€¢ Gesamtverlust: {total_loss:,} Zeilen ({100*total_loss/original_count:.1f}%)\")\n",
    "\n",
    "# Anreicherungsstatistiken\n",
    "enrichment_success = df_enriched['ortsteil_neu'].notna().sum()\n",
    "enrichment_rate = enrichment_success / len(df_enriched) * 100\n",
    "\n",
    "print(f\"\\nâœ… ANREICHERUNGSSTATISTIKEN:\")\n",
    "print(f\"   â€¢ Erfolgreich angereichert: {enrichment_success:,} von {len(df_enriched):,} Zeilen\")\n",
    "print(f\"   â€¢ Anreicherungsrate: {enrichment_rate:.1f}%\")\n",
    "print(f\"   â€¢ Dual-Strategie erfolgreich: {'âœ… JA' if enrichment_rate >= 99.0 else 'âš ï¸ ÃœBERPRÃœFEN'}\")\n",
    "\n",
    "# ===================================================================\n",
    "# PIPELINE-INTEGRATION & NÃ„CHSTE SCHRITTE\n",
    "# ===================================================================\n",
    "print(\"\\nðŸš€ PIPELINE-INTEGRATION & NÃ„CHSTE SCHRITTE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"ðŸ“ AUSGABEDATEIEN:\")\n",
    "print(f\"   â€¢ Normalisiert: data/processed/dataset_2025_normalized.csv\")\n",
    "print(f\"   â€¢ Angereichert:  data/processed/dataset_2025_enriched.csv\")\n",
    "\n",
    "print(f\"\\nðŸ”— BEREIT FÃœR INTEGRATION:\")\n",
    "print(f\"   âœ… 04_Combine_Datasets.ipynb - Kombination aller JahrgÃ¤nge\")\n",
    "print(f\"   âœ… 05_Housing_Market_Analysis.ipynb - Marktanalyse\")\n",
    "print(f\"   âœ… 06_Geospatial_Analysis.ipynb - Geospatiale Visualisierung\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ QUALITÃ„TSSICHERUNG:\")\n",
    "print(f\"   âœ… Einheitliche Filter-Kriterien (100â‚¬-10.000â‚¬, 10mÂ²-500mÂ²)\")\n",
    "print(f\"   âœ… Dual-Strategie-Anreicherung implementiert\")\n",
    "print(f\"   âœ… Kartesische Produkte vermieden\")\n",
    "print(f\"   âœ… Standardisierte Spaltenstruktur\")\n",
    "print(f\"   âœ… Konsistenz mit anderen Datasets gewÃ¤hrleistet\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ DATASET 2025 PROCESSING ERFOLGREICH ABGESCHLOSSEN!\")\n",
    "print(f\"    Ready for next pipeline step: 04_Combine_Datasets.ipynb\")\n",
    "print(\"=\" * 60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
