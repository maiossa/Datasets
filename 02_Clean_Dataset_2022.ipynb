{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfab51b2",
   "metadata": {},
   "source": [
    "# Dataset 2022 Bereinigung und Normalisierung\n",
    "## Spezialisiertes Modul f√ºr Springer/Immowelt/Immonet Dataset\n",
    "\n",
    "### Ziel\n",
    "Bereinigung und Normalisierung des aktuellen Datasets (2022) in ein standardisiertes Format f√ºr die gemeinsame Analyse.\n",
    "\n",
    "### Input\n",
    "- `data/raw/Dataset_2022.csv`\n",
    "- `data/processed/berlin_plz_mapping.csv` (PLZ-zu-Bezirk-Mapping)\n",
    "\n",
    "### Output\n",
    "- `data/processed/dataset_2022_normalized.csv`\n",
    "\n",
    "### Besonderheiten\n",
    "- **PLZ-zu-Bezirk-Mapping erforderlich** (Dataset enth√§lt nur PLZ, keine Bezirksnamen)\n",
    "- Umfangreiche Spaltenstruktur mit vielen Features\n",
    "- Deutsche Zahlenformate\n",
    "\n",
    "### Standardisierte Ausgabespalten\n",
    "- `price`: Normalisierter Preis (KALTMIETE in ‚Ç¨)\n",
    "- `size`: Normalisierte Gr√∂√üe (WOHNFLAECHE in m¬≤)\n",
    "- `district`: Berliner Bezirk (via PLZ-Mapping)\n",
    "- `rooms`: Anzahl Zimmer (ZIMMER)\n",
    "- `year`: Jahr des Datasets (2022)\n",
    "- `dataset_id`: Eindeutige Dataset-Kennzeichnung (current)\n",
    "- `source`: Datenquelle\n",
    "\n",
    "---\n",
    "**Teil der modularen Preprocessing-Pipeline**  \n",
    "**Datum:** 4. Juli 2025  \n",
    "**Version:** 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754def0",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "102355a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotheken erfolgreich importiert!\n",
      "Pandas Version: 2.3.0\n",
      "Dataset: 2022 (Springer/Immowelt/Immonet)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "print(\"Bibliotheken erfolgreich importiert!\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"Dataset: 2022 (Springer/Immowelt/Immonet)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8f7bfbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PLZ conversion and debug functions loaded!\n",
      "   ‚Ä¢ convert_plz_to_string(): Robust PLZ format conversion\n",
      "   ‚Ä¢ debug_plz_coverage(): Track PLZ coverage at each stage\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# ROBUST PLZ-TO-STRING CONVERSION FUNCTION\n",
    "# ===================================================================\n",
    "def convert_plz_to_string(plz_value):\n",
    "    \"\"\"\n",
    "    Robust PLZ-to-string conversion function.\n",
    "    Handles floats, ints, and strings properly.\n",
    "    Returns clean string PLZ without .0 suffix, or None if invalid.\n",
    "    \n",
    "    Examples:\n",
    "    - 10117.0 -> \"10117\"\n",
    "    - 10117 -> \"10117\"\n",
    "    - \"10117\" -> \"10117\"\n",
    "    - nan -> None\n",
    "    - \"invalid\" -> None\n",
    "    \"\"\"\n",
    "    if pd.isna(plz_value):\n",
    "        return None\n",
    "    \n",
    "    # Handle string inputs\n",
    "    if isinstance(plz_value, str):\n",
    "        plz_value = plz_value.strip()\n",
    "        if plz_value.lower() in ['nan', 'none', '']:\n",
    "            return None\n",
    "        # Remove .0 suffix if present\n",
    "        if plz_value.endswith('.0'):\n",
    "            plz_value = plz_value[:-2]\n",
    "        # Validate PLZ format (5 digits)\n",
    "        if plz_value.isdigit() and len(plz_value) == 5:\n",
    "            return plz_value\n",
    "        return None\n",
    "    \n",
    "    # Handle numeric inputs (int or float)\n",
    "    try:\n",
    "        plz_int = int(plz_value)\n",
    "        if 10000 <= plz_int <= 99999:  # Valid German PLZ range\n",
    "            return str(plz_int)\n",
    "        return None\n",
    "    except (ValueError, TypeError):\n",
    "        return None\n",
    "\n",
    "# ===================================================================\n",
    "# DEBUG LOGGING FUNCTION\n",
    "# ===================================================================\n",
    "def debug_plz_coverage(df, stage_name):\n",
    "    \"\"\"Debug function to track PLZ coverage at each stage\"\"\"\n",
    "    total_rows = len(df)\n",
    "    plz_coverage = df['plz'].notna().sum()\n",
    "    plz_percentage = (plz_coverage / total_rows * 100) if total_rows > 0 else 0\n",
    "    \n",
    "    print(f\"üîç DEBUG - {stage_name}:\")\n",
    "    print(f\"   Total rows: {total_rows:,}\")\n",
    "    print(f\"   PLZ coverage: {plz_coverage:,} ({plz_percentage:.1f}%)\")\n",
    "    print(f\"   PLZ missing: {total_rows - plz_coverage:,}\")\n",
    "    \n",
    "    # Show PLZ data types\n",
    "    if plz_coverage > 0:\n",
    "        plz_types = df['plz'].dropna().apply(type).value_counts()\n",
    "        type_dict = {}\n",
    "        for type_key, count in plz_types.items():\n",
    "            type_dict[str(type_key)] = count\n",
    "        print(f\"   PLZ data types: {type_dict}\")\n",
    "        # Show sample PLZ values\n",
    "        sample_plz = df['plz'].dropna().head(3).tolist()\n",
    "        print(f\"   Sample PLZ: {sample_plz}\")\n",
    "    print()\n",
    "\n",
    "print(\"‚úÖ PLZ conversion and debug functions loaded!\")\n",
    "print(\"   ‚Ä¢ convert_plz_to_string(): Robust PLZ format conversion\")\n",
    "print(\"   ‚Ä¢ debug_plz_coverage(): Track PLZ coverage at each stage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1604fc",
   "metadata": {},
   "source": [
    "## 2. PLZ-zu-Bezirk-Mapping laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91d98e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PLZ-ZU-BEZIRK-MAPPING LADEN\n",
      "============================================================\n",
      "‚úÖ PLZ-Mapping geladen: 181 Eintr√§ge\n",
      "‚úÖ PLZ-Dictionary erstellt: 181 Zuordnungen\n",
      "\n",
      "Beispiele:\n",
      "  10115 ‚Üí Mitte\n",
      "  10117 ‚Üí Mitte\n",
      "  10119 ‚Üí Prenzlauer Berg\n",
      "  10178 ‚Üí Mitte\n",
      "  10179 ‚Üí Mitte\n",
      "\n",
      "Abgedeckte Bezirke: 19\n",
      "Bezirke: ['Charlottenburg', 'Friedrichshain', 'Kreuzberg', 'Lichtenberg', 'Marzahn-Hellersdorf', 'Mitte', 'Neuk√∂lln', 'Pankow', 'Prenzlauer Berg', 'Reinickendorf', 'Sch√∂neberg', 'Spandau', 'Steglitz', 'Tempelhof', 'Tiergarten', 'Treptow-K√∂penick', 'Wedding', 'Wilmersdorf', 'Zehlendorf']\n"
     ]
    }
   ],
   "source": [
    "# PLZ-zu-Bezirk-Mapping laden\n",
    "print(\"=\" * 60)\n",
    "print(\"PLZ-ZU-BEZIRK-MAPPING LADEN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    plz_mapping_df = pd.read_csv('data/processed/berlin_plz_mapping.csv')\n",
    "    print(f\"‚úÖ PLZ-Mapping geladen: {len(plz_mapping_df)} Eintr√§ge\")\n",
    "    \n",
    "    # Erstelle Dictionary f√ºr schnelles Lookup\n",
    "    plz_to_district = dict(zip(plz_mapping_df['PLZ'], plz_mapping_df['Bezirk']))\n",
    "    print(f\"‚úÖ PLZ-Dictionary erstellt: {len(plz_to_district)} Zuordnungen\")\n",
    "    \n",
    "    # Zeige einige Beispiele\n",
    "    print(f\"\\nBeispiele:\")\n",
    "    for plz, bezirk in list(plz_to_district.items())[:5]:\n",
    "        print(f\"  {plz} ‚Üí {bezirk}\")\n",
    "        \n",
    "    print(f\"\\nAbgedeckte Bezirke: {len(set(plz_to_district.values()))}\")\n",
    "    print(f\"Bezirke: {sorted(set(plz_to_district.values()))}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"‚ùå FEHLER: PLZ-Mapping nicht gefunden!\")\n",
    "    print(\"Bitte stellen Sie sicher, dass 'data/processed/berlin_plz_mapping.csv' existiert.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3f0cfe",
   "metadata": {},
   "source": [
    "## 3. Daten laden und erste Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c255b68d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET 2022 LADEN UND ANALYSIEREN\n",
      "============================================================\n",
      "Dataset geladen: 2,950 Zeilen, 75 Spalten\n",
      "\n",
      "Erste 10 Spalten: ['ID', 'SORTE', 'PLZ', 'KALTMIETE', 'WARMMIETE', 'NEBENKOSTEN', 'KAUTION', 'HEIZUNGSKOSTEN', 'ZIMMER', 'PARKPLAETZE']\n",
      "Letzte 5 Spalten: ['Stein', 'Marmor', 'Doppelboden', 'Terracotta', 'Sonstiges']\n",
      "\n",
      "=== KERNFELDER ANALYSE ===\n",
      "PLZ: 2950/2950 (100.0%) nicht-null\n",
      "KALTMIETE: 2772/2950 (94.0%) nicht-null\n",
      "WOHNFLAECHE: 2950/2950 (100.0%) nicht-null\n",
      "ZIMMER: 2942/2950 (99.7%) nicht-null\n",
      "\n",
      "=== PLZ-ANALYSE ===\n",
      "Einzigartige PLZ: 183\n",
      "PLZ-Beispiele: [np.int64(10115), np.int64(10117), np.int64(10119), np.int64(10178), np.int64(10179), np.int64(10243), np.int64(10245), np.int64(10247), np.int64(10249), np.int64(10315)]\n",
      "\n",
      "Erste 5 Zeilen (Kernfelder):\n",
      "     PLZ  KALTMIETE  WOHNFLAECHE  ZIMMER\n",
      "0  13125     860.00        73.00     3.0\n",
      "1  13125     450.28        48.84     2.0\n",
      "2  13125     739.00        54.79     2.0\n",
      "3  13125     899.00        74.49     3.0\n",
      "4  13125     899.00        74.49     3.0\n"
     ]
    }
   ],
   "source": [
    "# Lade Dataset 2022\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET 2022 LADEN UND ANALYSIEREN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Lade Rohdaten\n",
    "df_raw = pd.read_csv('data/raw/Dataset_2022.csv')\n",
    "print(f\"Dataset geladen: {df_raw.shape[0]:,} Zeilen, {df_raw.shape[1]} Spalten\")\n",
    "\n",
    "# Grundlegende Informationen\n",
    "print(f\"\\nErste 10 Spalten: {list(df_raw.columns[:10])}\")\n",
    "print(f\"Letzte 5 Spalten: {list(df_raw.columns[-5:])}\")\n",
    "\n",
    "# Kernfelder analysieren\n",
    "core_fields = ['PLZ', 'KALTMIETE', 'WOHNFLAECHE', 'ZIMMER']\n",
    "print(f\"\\n=== KERNFELDER ANALYSE ===\")\n",
    "for field in core_fields:\n",
    "    if field in df_raw.columns:\n",
    "        non_null = df_raw[field].notna().sum()\n",
    "        print(f\"{field}: {non_null}/{len(df_raw)} ({non_null/len(df_raw)*100:.1f}%) nicht-null\")\n",
    "    else:\n",
    "        print(f\"‚ùå {field}: Spalte nicht gefunden!\")\n",
    "\n",
    "# PLZ-Analyse\n",
    "print(f\"\\n=== PLZ-ANALYSE ===\")\n",
    "unique_plz = df_raw['PLZ'].dropna().unique()\n",
    "print(f\"Einzigartige PLZ: {len(unique_plz)}\")\n",
    "print(f\"PLZ-Beispiele: {sorted(unique_plz)[:10]}\")\n",
    "\n",
    "# Erste 5 Zeilen der Kernfelder\n",
    "print(f\"\\nErste 5 Zeilen (Kernfelder):\")\n",
    "print(df_raw[core_fields].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee2f21d",
   "metadata": {},
   "source": [
    "## 4. Spezifische Bereinigung Dataset 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f3a6a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPEZIFISCHE BEREINIGUNG DATASET 2022\n",
      "============================================================\n",
      "Arbeitskopie erstellt: 2950 Zeilen\n",
      "\n",
      "=== PREIS-BEREINIGUNG (KALTMIETE) ===\n",
      "KALTMIETE - Typ: float64\n",
      "Nicht-null Werte: 2772\n",
      "Nach Entfernung NaN-Preise: 2772 Zeilen\n",
      "G√ºltige Preise nach Bereinigung: 2772/2772 (100.0%)\n",
      "Preisspanne: 163.31‚Ç¨ - 3000.00‚Ç¨\n",
      "Entfernte unrealistische Preise: 0\n",
      "\n",
      "=== GR√ñSSEN-BEREINIGUNG (WOHNFLAECHE) ===\n",
      "WOHNFLAECHE - Typ: float64\n",
      "Nicht-null Werte: 2772\n",
      "G√ºltige Gr√∂√üen nach Bereinigung: 2772/2772 (100.0%)\n",
      "Gr√∂√üenspanne: 13.0m¬≤ - 230.0m¬≤\n",
      "Entfernte unrealistische Gr√∂√üen: 0\n",
      "\n",
      "=== PLZ-ZU-BEZIRK-ZUORDNUNG ===\n",
      "PLZ - Typ: int64\n",
      "Nicht-null PLZ: 2772\n",
      "\n",
      "=== ERWEITERUNG DES PLZ-MAPPINGS ===\n",
      "PLZ-Mapping erweitert: 188 Zuordnungen\n",
      "Erfolgreiche PLZ-zu-Bezirk-Zuordnungen: 2681/2772 (96.7%)\n",
      "\n",
      "Verbleibende nicht zugeordnete PLZ (10 einzigartige):\n",
      "  12247: 16 Eintr√§ge\n",
      "  12207: 16 Eintr√§ge\n",
      "  13627: 12 Eintr√§ge\n",
      "  12203: 11 Eintr√§ge\n",
      "  12209: 9 Eintr√§ge\n",
      "  12279: 7 Eintr√§ge\n",
      "  12277: 7 Eintr√§ge\n",
      "  12249: 6 Eintr√§ge\n",
      "  10551: 4 Eintr√§ge\n",
      "  12205: 3 Eintr√§ge\n",
      "Entfernte Eintr√§ge ohne Bezirk: 91\n",
      "\n",
      "=== ZIMMER-BEREINIGUNG (ZIMMER) ===\n",
      "ZIMMER - Typ: float64\n",
      "Nicht-null Werte: 2676\n",
      "G√ºltige Zimmerzahlen nach Bereinigung: 2676/2681 (99.8%)\n",
      "Zimmerspanne: 1.0 - 5.0\n",
      "Entfernte unrealistische Zimmerzahlen: 5\n",
      "\n",
      "‚úÖ Spezifische Bereinigung abgeschlossen\n",
      "Verbleibende Datens√§tze: 2676 (Verlust: 274)\n"
     ]
    }
   ],
   "source": [
    "# Spezifische Bereinigung f√ºr Dataset 2022\n",
    "print(\"=\" * 60)\n",
    "print(\"SPEZIFISCHE BEREINIGUNG DATASET 2022\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Erstelle Arbeitskopie\n",
    "df_clean = df_raw.copy()\n",
    "print(f\"Arbeitskopie erstellt: {len(df_clean)} Zeilen\")\n",
    "\n",
    "# === PREIS-BEREINIGUNG (KALTMIETE) ===\n",
    "print(\"\\n=== PREIS-BEREINIGUNG (KALTMIETE) ===\")\n",
    "print(f\"KALTMIETE - Typ: {df_clean['KALTMIETE'].dtype}\")\n",
    "print(f\"Nicht-null Werte: {df_clean['KALTMIETE'].notna().sum()}\")\n",
    "\n",
    "# Nur Zeilen mit g√ºltigen Preisen behalten\n",
    "df_clean = df_clean[df_clean['KALTMIETE'].notna()]\n",
    "print(f\"Nach Entfernung NaN-Preise: {len(df_clean)} Zeilen\")\n",
    "\n",
    "# Unrealistische Preise entfernen (< 100‚Ç¨ oder > 10000‚Ç¨) - Konsistent mit anderen Datasets\n",
    "initial_count = len(df_clean)\n",
    "df_clean = df_clean[(df_clean['KALTMIETE'] >= 100) & (df_clean['KALTMIETE'] <= 10000)]\n",
    "removed_prices = initial_count - len(df_clean)\n",
    "print(f\"G√ºltige Preise nach Bereinigung: {len(df_clean)}/{initial_count} ({100*len(df_clean)/initial_count:.1f}%)\")\n",
    "print(f\"Preisspanne: {df_clean['KALTMIETE'].min():.2f}‚Ç¨ - {df_clean['KALTMIETE'].max():.2f}‚Ç¨\")\n",
    "print(f\"Entfernte unrealistische Preise: {removed_prices}\")\n",
    "\n",
    "# === GR√ñSSEN-BEREINIGUNG (WOHNFLAECHE) ===\n",
    "print(\"\\n=== GR√ñSSEN-BEREINIGUNG (WOHNFLAECHE) ===\")\n",
    "print(f\"WOHNFLAECHE - Typ: {df_clean['WOHNFLAECHE'].dtype}\")\n",
    "print(f\"Nicht-null Werte: {df_clean['WOHNFLAECHE'].notna().sum()}\")\n",
    "\n",
    "# Unrealistische Gr√∂√üen entfernen (< 10m¬≤ oder > 500m¬≤) - Konsistent mit anderen Datasets\n",
    "initial_count = len(df_clean)\n",
    "df_clean = df_clean[(df_clean['WOHNFLAECHE'] >= 10) & (df_clean['WOHNFLAECHE'] <= 500)]\n",
    "removed_sizes = initial_count - len(df_clean)\n",
    "print(f\"G√ºltige Gr√∂√üen nach Bereinigung: {len(df_clean)}/{initial_count} ({100*len(df_clean)/initial_count:.1f}%)\")\n",
    "print(f\"Gr√∂√üenspanne: {df_clean['WOHNFLAECHE'].min():.1f}m¬≤ - {df_clean['WOHNFLAECHE'].max():.1f}m¬≤\")\n",
    "print(f\"Entfernte unrealistische Gr√∂√üen: {removed_sizes}\")\n",
    "\n",
    "# === PLZ-ZU-BEZIRK-ZUORDNUNG ===\n",
    "print(\"\\n=== PLZ-ZU-BEZIRK-ZUORDNUNG ===\")\n",
    "print(f\"PLZ - Typ: {df_clean['PLZ'].dtype}\")\n",
    "print(f\"Nicht-null PLZ: {df_clean['PLZ'].notna().sum()}\")\n",
    "\n",
    "# WICHTIG: Erweitere das PLZ-Mapping BEVOR die Zuordnung gemacht wird\n",
    "print(\"\\n=== ERWEITERUNG DES PLZ-MAPPINGS ===\")\n",
    "# Erweitere das PLZ-Dictionary um fehlende PLZ-Codes\n",
    "additional_plz_mapping = {\n",
    "    12627: 'Marzahn-Hellersdorf',\n",
    "    12629: 'Marzahn-Hellersdorf',\n",
    "    13593: 'Spandau',\n",
    "    13595: 'Spandau',\n",
    "    13597: 'Spandau',\n",
    "    13599: 'Spandau',\n",
    "    14052: 'Charlottenburg-Wilmersdorf',\n",
    "    14055: 'Charlottenburg-Wilmersdorf',\n",
    "    14057: 'Charlottenburg-Wilmersdorf',\n",
    "    14059: 'Charlottenburg-Wilmersdorf',\n",
    "    10315: 'Lichtenberg',\n",
    "    10317: 'Lichtenberg',\n",
    "    10318: 'Lichtenberg',\n",
    "    10319: 'Lichtenberg',\n",
    "    10365: 'Lichtenberg',\n",
    "    10367: 'Lichtenberg',\n",
    "    10369: 'Lichtenberg',\n",
    "    13125: 'Pankow',\n",
    "    13127: 'Pankow',\n",
    "    13129: 'Pankow',\n",
    "    13156: 'Pankow',\n",
    "    13158: 'Pankow',\n",
    "    13159: 'Pankow',\n",
    "    13187: 'Pankow',\n",
    "    13189: 'Pankow',\n",
    "    12305: 'Tempelhof-Sch√∂neberg',\n",
    "    12307: 'Tempelhof-Sch√∂neberg',\n",
    "    12309: 'Tempelhof-Sch√∂neberg',\n",
    "    12347: 'Neuk√∂lln',\n",
    "    12349: 'Neuk√∂lln',\n",
    "    12351: 'Neuk√∂lln',\n",
    "    12353: 'Neuk√∂lln',\n",
    "    12355: 'Neuk√∂lln',\n",
    "    12357: 'Neuk√∂lln',\n",
    "    12359: 'Neuk√∂lln',\n",
    "    12681: 'Marzahn-Hellersdorf',\n",
    "    12683: 'Marzahn-Hellersdorf',\n",
    "    12685: 'Marzahn-Hellersdorf',\n",
    "    12687: 'Marzahn-Hellersdorf',\n",
    "    12689: 'Marzahn-Hellersdorf',\n",
    "    12679: 'Marzahn-Hellersdorf',\n",
    "    13051: 'Pankow',\n",
    "    13053: 'Pankow',\n",
    "    13055: 'Pankow',\n",
    "    13057: 'Pankow',\n",
    "    13059: 'Pankow',\n",
    "    13086: 'Pankow',\n",
    "    13088: 'Pankow',\n",
    "    13089: 'Pankow',\n",
    "    13403: 'Reinickendorf',\n",
    "    13405: 'Reinickendorf',\n",
    "    13407: 'Reinickendorf',\n",
    "    13409: 'Reinickendorf',\n",
    "    13435: 'Reinickendorf',\n",
    "    13437: 'Reinickendorf',\n",
    "    13439: 'Reinickendorf',\n",
    "    13465: 'Reinickendorf',\n",
    "    13467: 'Reinickendorf',\n",
    "    13469: 'Reinickendorf',\n",
    "    13503: 'Reinickendorf',\n",
    "    13505: 'Reinickendorf',\n",
    "    13507: 'Reinickendorf',\n",
    "    13509: 'Reinickendorf',\n",
    "    13581: 'Spandau',\n",
    "    13583: 'Spandau',\n",
    "    13585: 'Spandau',\n",
    "    13587: 'Spandau',\n",
    "    13589: 'Spandau',\n",
    "    13591: 'Spandau',\n",
    "    14195: 'Steglitz-Zehlendorf',\n",
    "    14197: 'Steglitz-Zehlendorf',\n",
    "    14199: 'Steglitz-Zehlendorf',\n",
    "    14163: 'Steglitz-Zehlendorf',\n",
    "    14165: 'Steglitz-Zehlendorf',\n",
    "    14167: 'Steglitz-Zehlendorf',\n",
    "    14169: 'Steglitz-Zehlendorf',\n",
    "    14129: 'Steglitz-Zehlendorf',\n",
    "    14109: 'Steglitz-Zehlendorf',\n",
    "}\n",
    "\n",
    "# Erweitere das urspr√ºngliche PLZ-Dictionary\n",
    "plz_to_district.update(additional_plz_mapping)\n",
    "print(f\"PLZ-Mapping erweitert: {len(plz_to_district)} Zuordnungen\")\n",
    "\n",
    "# Jetzt PLZ zu Bezirk zuordnen\n",
    "df_clean['district'] = df_clean['PLZ'].map(plz_to_district)\n",
    "successful_mappings = df_clean['district'].notna().sum()\n",
    "print(f\"Erfolgreiche PLZ-zu-Bezirk-Zuordnungen: {successful_mappings}/{len(df_clean)} ({100*successful_mappings/len(df_clean):.1f}%)\")\n",
    "\n",
    "# Zeige nicht zugeordnete PLZ\n",
    "unmapped_plz = df_clean[df_clean['district'].isna()]['PLZ'].value_counts().head(10)\n",
    "if len(unmapped_plz) > 0:\n",
    "    unique_unmapped = df_clean[df_clean['district'].isna()]['PLZ'].nunique()\n",
    "    print(f\"\\nVerbleibende nicht zugeordnete PLZ ({unique_unmapped} einzigartige):\")\n",
    "    for plz, count in unmapped_plz.items():\n",
    "        print(f\"  {plz}: {count} Eintr√§ge\")\n",
    "\n",
    "# Nur Zeilen mit g√ºltigen Bezirken behalten\n",
    "initial_count = len(df_clean)\n",
    "df_clean = df_clean[df_clean['district'].notna()]\n",
    "removed_no_district = initial_count - len(df_clean)\n",
    "print(f\"Entfernte Eintr√§ge ohne Bezirk: {removed_no_district}\")\n",
    "\n",
    "# === ZIMMER-BEREINIGUNG (ZIMMER) ===\n",
    "print(\"\\n=== ZIMMER-BEREINIGUNG (ZIMMER) ===\")\n",
    "print(f\"ZIMMER - Typ: {df_clean['ZIMMER'].dtype}\")\n",
    "print(f\"Nicht-null Werte: {df_clean['ZIMMER'].notna().sum()}\")\n",
    "\n",
    "# Unrealistische Zimmerzahlen entfernen\n",
    "if len(df_clean) > 0:\n",
    "    initial_count = len(df_clean)\n",
    "    df_clean = df_clean[(df_clean['ZIMMER'] >= 1) & (df_clean['ZIMMER'] <= 10)]\n",
    "    removed_rooms = initial_count - len(df_clean)\n",
    "    print(f\"G√ºltige Zimmerzahlen nach Bereinigung: {len(df_clean)}/{initial_count} ({100*len(df_clean)/initial_count:.1f}%)\")\n",
    "    if len(df_clean) > 0:\n",
    "        print(f\"Zimmerspanne: {df_clean['ZIMMER'].min():.1f} - {df_clean['ZIMMER'].max():.1f}\")\n",
    "    print(f\"Entfernte unrealistische Zimmerzahlen: {removed_rooms}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Spezifische Bereinigung abgeschlossen\")\n",
    "print(f\"Verbleibende Datens√§tze: {len(df_clean)} (Verlust: {len(df_raw) - len(df_clean)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7bbc0a",
   "metadata": {},
   "source": [
    "## 5. Normalisierung in Standardformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d678813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NORMALISIERUNG IN STANDARDFORMAT\n",
      "============================================================\n",
      "Normalisiertes Dataset erstellt: 2676 Zeilen\n",
      "Standardspalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'plz']\n",
      "Zus√§tzliche Spalten: 15\n",
      "\n",
      "=== DATENQUALIT√ÑT NORMALISIERTES DATASET ===\n",
      "Zeilen mit Preis: 2676\n",
      "Zeilen mit Gr√∂√üe: 2676\n",
      "Zeilen mit Bezirk: 2676\n",
      "Zeilen mit Zimmeranzahl: 2676\n",
      "\n",
      "=== STATISTIKEN ===\n",
      "Preis - Min: 180.00‚Ç¨, Max: 3000.00‚Ç¨, Median: 790.60‚Ç¨\n",
      "Gr√∂√üe - Min: 13.0m¬≤, Max: 230.0m¬≤, Median: 65.1m¬≤\n",
      "Zimmer - Min: 1.0, Max: 5.0, Median: 2.0\n",
      "\n",
      "=== BEZIRKSVERTEILUNG ===\n",
      "Anzahl Bezirke: 21\n",
      "  Spandau: 329 Eintr√§ge\n",
      "  Pankow: 256 Eintr√§ge\n",
      "  Reinickendorf: 247 Eintr√§ge\n",
      "  Treptow-K√∂penick: 243 Eintr√§ge\n",
      "  Marzahn-Hellersdorf: 237 Eintr√§ge\n",
      "  Lichtenberg: 151 Eintr√§ge\n",
      "  Neuk√∂lln: 147 Eintr√§ge\n",
      "  Friedrichshain: 113 Eintr√§ge\n",
      "  Sch√∂neberg: 107 Eintr√§ge\n",
      "  Mitte: 99 Eintr√§ge\n",
      "\n",
      "‚úÖ Normalisierung abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "# Normalisierung in Standardformat\n",
    "print(\"=\" * 60)\n",
    "print(\"NORMALISIERUNG IN STANDARDFORMAT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Erstelle normalisiertes Dataset mit Standardspalten\n",
    "df_normalized = pd.DataFrame()\n",
    "\n",
    "if len(df_clean) > 0:\n",
    "    # Standardspalten zuweisen\n",
    "    df_normalized['price'] = df_clean['KALTMIETE'].astype('float64')\n",
    "    df_normalized['size'] = df_clean['WOHNFLAECHE'].astype('float64')\n",
    "    df_normalized['district'] = df_clean['district'].astype('string')\n",
    "    df_normalized['rooms'] = df_clean['ZIMMER'].astype('float64')\n",
    "    df_normalized['year'] = 2022\n",
    "    df_normalized['dataset_id'] = 'current'\n",
    "    df_normalized['source'] = 'Springer/Immowelt/Immonet'\n",
    "    df_normalized['plz'] = df_clean['PLZ'].astype('string')\n",
    "\n",
    "    # Zus√§tzliche wichtige Spalten aus Original-Dataset beibehalten\n",
    "    if 'WARMMIETE' in df_clean.columns:\n",
    "        df_normalized['warmmiete'] = df_clean['WARMMIETE']\n",
    "    if 'NEBENKOSTEN' in df_clean.columns:\n",
    "        df_normalized['nebenkosten'] = df_clean['NEBENKOSTEN']\n",
    "    if 'KAUTION' in df_clean.columns:\n",
    "        df_normalized['kaution'] = df_clean['KAUTION']\n",
    "    if 'BAUJAHR' in df_clean.columns:\n",
    "        df_normalized['baujahr'] = df_clean['BAUJAHR']\n",
    "    if 'ZUSTAND' in df_clean.columns:\n",
    "        df_normalized['zustand'] = df_clean['ZUSTAND']\n",
    "    if 'ENERGIEEFFIZIENSKLASSE' in df_clean.columns:\n",
    "        df_normalized['energieeffiziensklasse'] = df_clean['ENERGIEEFFIZIENSKLASSE']\n",
    "\n",
    "    # Ausstattungsmerkmale (boolean)\n",
    "    ausstattung_cols = ['m√∂bliert', 'Balkon', 'Terrasse', 'Garten', 'Einbauk√ºche', \n",
    "                       'Garage', 'Stellplatz', 'Personenaufzug', 'Keller']\n",
    "    for col in ausstattung_cols:\n",
    "        if col in df_clean.columns:\n",
    "            df_normalized[f'ausstattung_{col.lower()}'] = df_clean[col]\n",
    "\n",
    "print(f\"Normalisiertes Dataset erstellt: {len(df_normalized)} Zeilen\")\n",
    "print(f\"Standardspalten: {['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'plz']}\")\n",
    "print(f\"Zus√§tzliche Spalten: {len(df_normalized.columns) - 8}\")\n",
    "\n",
    "# Datenqualit√§t pr√ºfen\n",
    "print(f\"\\n=== DATENQUALIT√ÑT NORMALISIERTES DATASET ===\")\n",
    "print(f\"Zeilen mit Preis: {df_normalized['price'].notna().sum()}\")\n",
    "print(f\"Zeilen mit Gr√∂√üe: {df_normalized['size'].notna().sum()}\")\n",
    "print(f\"Zeilen mit Bezirk: {df_normalized['district'].notna().sum()}\")\n",
    "print(f\"Zeilen mit Zimmeranzahl: {df_normalized['rooms'].notna().sum()}\")\n",
    "\n",
    "# Statistiken\n",
    "if len(df_normalized) > 0:\n",
    "    print(f\"\\n=== STATISTIKEN ===\")\n",
    "    print(f\"Preis - Min: {df_normalized['price'].min():.2f}‚Ç¨, Max: {df_normalized['price'].max():.2f}‚Ç¨, Median: {df_normalized['price'].median():.2f}‚Ç¨\")\n",
    "    print(f\"Gr√∂√üe - Min: {df_normalized['size'].min():.1f}m¬≤, Max: {df_normalized['size'].max():.1f}m¬≤, Median: {df_normalized['size'].median():.1f}m¬≤\")\n",
    "    print(f\"Zimmer - Min: {df_normalized['rooms'].min():.1f}, Max: {df_normalized['rooms'].max():.1f}, Median: {df_normalized['rooms'].median():.1f}\")\n",
    "\n",
    "    # Bezirksverteilung\n",
    "    print(f\"\\n=== BEZIRKSVERTEILUNG ===\")\n",
    "    district_counts = df_normalized['district'].value_counts()\n",
    "    print(f\"Anzahl Bezirke: {len(district_counts)}\")\n",
    "    for district, count in district_counts.head(10).items():\n",
    "        print(f\"  {district}: {count} Eintr√§ge\")\n",
    "\n",
    "print(f\"\\n‚úÖ Normalisierung abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ddabaf",
   "metadata": {},
   "source": [
    "## 6. Export des normalisierten Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be53cc50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT NORMALISIERTES DATASET\n",
      "============================================================\n",
      "‚úÖ Normalisiertes Dataset exportiert: data/processed/dataset_2022_normalized.csv\n",
      "Dateigr√∂√üe: 2676 Zeilen x 23 Spalten\n",
      "‚úÖ Export-Validierung erfolgreich: 2676 Zeilen geladen\n",
      "\n",
      "=== ZUSAMMENFASSUNG DATASET 2022 ===\n",
      "Input: data/raw/Dataset_2022.csv (2950 Zeilen)\n",
      "Output: data/processed/dataset_2022_normalized.csv (2676 Zeilen)\n",
      "Datenverlust: 274 Zeilen (9.3%)\n",
      "PLZ-zu-Bezirk-Mapping: 2676/2676 (100.0%) erfolgreich\n",
      "Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\n",
      "Zus√§tzliche Spalten: 16\n",
      "\n",
      "üéØ DATASET 2022 BEREINIGUNG ABGESCHLOSSEN!\n",
      "Bereit f√ºr Kombination mit anderen normalisierten Datasets.\n"
     ]
    }
   ],
   "source": [
    "# Export des normalisierten Datasets\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPORT NORMALISIERTES DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ausgabedatei\n",
    "output_file = 'data/processed/dataset_2022_normalized.csv'\n",
    "\n",
    "# Export\n",
    "df_normalized.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Normalisiertes Dataset exportiert: {output_file}\")\n",
    "print(f\"Dateigr√∂√üe: {len(df_normalized)} Zeilen x {len(df_normalized.columns)} Spalten\")\n",
    "\n",
    "# Validierung des Exports\n",
    "test_load = pd.read_csv(output_file)\n",
    "print(f\"‚úÖ Export-Validierung erfolgreich: {len(test_load)} Zeilen geladen\")\n",
    "\n",
    "# Zusammenfassung\n",
    "print(f\"\\n=== ZUSAMMENFASSUNG DATASET 2022 ===\")\n",
    "print(f\"Input: data/raw/Dataset_2022.csv ({len(df_raw)} Zeilen)\")\n",
    "print(f\"Output: {output_file} ({len(df_normalized)} Zeilen)\")\n",
    "print(f\"Datenverlust: {len(df_raw) - len(df_normalized)} Zeilen ({((len(df_raw) - len(df_normalized))/len(df_raw)*100):.1f}%)\")\n",
    "print(f\"PLZ-zu-Bezirk-Mapping: {df_normalized['district'].notna().sum()}/{len(df_normalized)} ({df_normalized['district'].notna().sum()/len(df_normalized)*100:.1f}%) erfolgreich\")\n",
    "print(f\"Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\")\n",
    "print(f\"Zus√§tzliche Spalten: {len(df_normalized.columns) - 7}\")\n",
    "\n",
    "print(f\"\\nüéØ DATASET 2022 BEREINIGUNG ABGESCHLOSSEN!\")\n",
    "print(f\"Bereit f√ºr Kombination mit anderen normalisierten Datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d5",
   "metadata": {},
   "source": [
    "## 7. Lade angereicherte Wohnlagendaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5f6g7h9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANGEREICHERTE WOHNLAGENDATEN LADEN\n",
      "============================================================\n",
      "‚úÖ Angereicherte Daten geladen: 551,249 Zeilen, 11 Spalten\n",
      "‚úÖ Angereicherte Daten geladen: 551,249 Zeilen, 11 Spalten\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ANGEREICHERTE WOHNLAGENDATEN LADEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "enriched_data_path = 'data/raw/wohnlagen_enriched.csv'\n",
    "try:\n",
    "    enriched_df = pd.read_csv(enriched_data_path)\n",
    "    print(f\"‚úÖ Angereicherte Daten geladen: {len(enriched_df):,} Zeilen, {len(enriched_df.columns)} Spalten\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Datei nicht gefunden: {enriched_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l3",
   "metadata": {},
   "source": [
    "## 8. Kombiniere Datasets mit Wohnlagendaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "m3n4o5p7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KOMBINIERE MIT WOHNLAGENDATEN - ENHANCED WITH PLZ FIXES\n",
      "============================================================\n",
      "Original df_normalized: 2,676 Zeilen\n",
      "Original enriched_df: 2,676 Zeilen\n",
      "\n",
      "üîß ROBUST PLZ CONVERSION\n",
      "==================================================\n",
      "üîç DEBUG - Vor PLZ-Konvertierung:\n",
      "   Total rows: 2,676\n",
      "   PLZ coverage: 2,676 (100.0%)\n",
      "   PLZ missing: 0\n",
      "   PLZ data types: {\"<class 'str'>\": 2676}\n",
      "   Sample PLZ: ['13125', '13125', '13125']\n",
      "\n",
      "Konvertiere PLZ zu String-Format...\n",
      "üîç DEBUG - Nach PLZ-Konvertierung:\n",
      "   Total rows: 2,676\n",
      "   PLZ coverage: 2,676 (100.0%)\n",
      "   PLZ missing: 0\n",
      "   PLZ data types: {\"<class 'str'>\": 2676}\n",
      "   Sample PLZ: ['13125', '13125', '13125']\n",
      "\n",
      "\n",
      "üì¶ VORBEREITUNG ANREICHERUNGSDATEN\n",
      "==================================================\n",
      "Konvertiere enriched_df PLZ zu String...\n",
      "Unique PLZ mappings: 173 Zeilen\n",
      "PLZ overlap: 173 von 173 PLZ im Dataset\n",
      "‚úÖ Gute PLZ-√úbereinstimmung f√ºr Anreicherung\n",
      "\n",
      "üîÑ ANREICHERUNG MIT DATENERHALTUNG\n",
      "==================================================\n",
      "PLZ vor Anreicherung: 2,676\n",
      "üîç DEBUG - Nach Anreicherung:\n",
      "   Total rows: 2,676\n",
      "   PLZ coverage: 2,676 (100.0%)\n",
      "   PLZ missing: 0\n",
      "   PLZ data types: {\"<class 'str'>\": 2676}\n",
      "   Sample PLZ: ['13125', '13125', '13125']\n",
      "\n",
      "PLZ nach Anreicherung: 2,676\n",
      "‚úÖ PLZ vollst√§ndig erhalten!\n",
      "‚úÖ Kombiniertes und angereichertes Dataset erstellt: 2,676 Zeilen\n",
      "\n",
      "‚úÖ ANREICHERUNGS-ERFOLG VALIDIERUNG\n",
      "==================================================\n",
      "Erfolgreiche Anreicherung: 2,676 von 2,676 Zeilen (100.0%)\n",
      "WOL-Abdeckung: 2,676 von 2,676 Zeilen (100.0%)\n",
      "\n",
      "üìä ANREICHERUNGS-ZUSAMMENFASSUNG:\n",
      "   ‚Ä¢ Original Dataset: 2,676 Zeilen\n",
      "   ‚Ä¢ Angereichert Dataset: 2,676 Zeilen\n",
      "   ‚Ä¢ PLZ-Erhaltung: 2,676/2,676 (‚úÖ Vollst√§ndig)\n",
      "   ‚Ä¢ Ortsteil-Anreicherung: 100.0%\n",
      "   ‚Ä¢ WOL-Anreicherung: 100.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"KOMBINIERE MIT WOHNLAGENDATEN - ENHANCED WITH PLZ FIXES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Debug: Check original data sizes\n",
    "print(f\"Original df_normalized: {len(df_normalized):,} Zeilen\")\n",
    "print(f\"Original enriched_df: {len(enriched_df):,} Zeilen\")\n",
    "\n",
    "# ===================================================================\n",
    "# ROBUST PLZ CONVERSION BEFORE ENRICHMENT\n",
    "# ===================================================================\n",
    "print(\"\\nüîß ROBUST PLZ CONVERSION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Debug PLZ status before conversion\n",
    "debug_plz_coverage(df_normalized, \"Vor PLZ-Konvertierung\")\n",
    "\n",
    "# Convert PLZ to string using robust function\n",
    "print(\"Konvertiere PLZ zu String-Format...\")\n",
    "df_normalized['plz'] = df_normalized['plz'].apply(convert_plz_to_string)\n",
    "\n",
    "# Debug PLZ status after conversion\n",
    "debug_plz_coverage(df_normalized, \"Nach PLZ-Konvertierung\")\n",
    "\n",
    "# ===================================================================\n",
    "# PREPARE ENRICHMENT DATA WITH PLZ CONVERSION\n",
    "# ===================================================================\n",
    "print(\"\\nüì¶ VORBEREITUNG ANREICHERUNGSDATEN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Convert enriched_df PLZ to string as well\n",
    "print(\"Konvertiere enriched_df PLZ zu String...\")\n",
    "enriched_df['plz'] = enriched_df['plz'].apply(convert_plz_to_string)\n",
    "\n",
    "# Create a unique mapping of PLZ to avoid cartesian product\n",
    "enriched_df_subset = enriched_df[['plz', 'wol', 'ortsteil_neu']].drop_duplicates(subset=['plz'])\n",
    "print(f\"Unique PLZ mappings: {len(enriched_df_subset):,} Zeilen\")\n",
    "\n",
    "# Debug: Check PLZ overlap\n",
    "df_plz_unique = set(df_normalized['plz'].dropna().unique())\n",
    "enriched_plz_unique = set(enriched_df_subset['plz'].dropna().unique())\n",
    "overlap = df_plz_unique.intersection(enriched_plz_unique)\n",
    "print(f\"PLZ overlap: {len(overlap)} von {len(df_plz_unique)} PLZ im Dataset\")\n",
    "\n",
    "if len(overlap) > 0:\n",
    "    print(f\"‚úÖ Gute PLZ-√úbereinstimmung f√ºr Anreicherung\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Keine PLZ-√úbereinstimmung - pr√ºfe PLZ-Formate\")\n",
    "\n",
    "# ===================================================================\n",
    "# PERFORM ENRICHMENT WITH DATA PRESERVATION\n",
    "# ===================================================================\n",
    "print(\"\\nüîÑ ANREICHERUNG MIT DATENERHALTUNG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Store original PLZ count for comparison\n",
    "original_plz_count = df_normalized['plz'].notna().sum()\n",
    "print(f\"PLZ vor Anreicherung: {original_plz_count:,}\")\n",
    "\n",
    "# Perform the merge (LEFT JOIN to preserve all original data)\n",
    "df_enriched = pd.merge(df_normalized, enriched_df_subset, how='left', on=['plz'])\n",
    "\n",
    "# Debug PLZ status after enrichment\n",
    "debug_plz_coverage(df_enriched, \"Nach Anreicherung\")\n",
    "\n",
    "# Verify PLZ preservation\n",
    "plz_after_enrichment = df_enriched['plz'].notna().sum()\n",
    "print(f\"PLZ nach Anreicherung: {plz_after_enrichment:,}\")\n",
    "\n",
    "if plz_after_enrichment != original_plz_count:\n",
    "    print(f\"‚ö†Ô∏è PLZ-Verlust erkannt: {original_plz_count - plz_after_enrichment} PLZ verloren!\")\n",
    "else:\n",
    "    print(f\"‚úÖ PLZ vollst√§ndig erhalten!\")\n",
    "\n",
    "print(f\"‚úÖ Kombiniertes und angereichertes Dataset erstellt: {len(df_enriched):,} Zeilen\")\n",
    "\n",
    "# ===================================================================\n",
    "# ENRICHMENT SUCCESS VALIDATION\n",
    "# ===================================================================\n",
    "print(\"\\n‚úÖ ANREICHERUNGS-ERFOLG VALIDIERUNG\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check merge success\n",
    "successful_enrichment = df_enriched['ortsteil_neu'].notna().sum()\n",
    "enrichment_rate = (successful_enrichment / len(df_enriched)) * 100\n",
    "print(f\"Erfolgreiche Anreicherung: {successful_enrichment:,} von {len(df_enriched):,} Zeilen ({enrichment_rate:.1f}%)\")\n",
    "\n",
    "# Check WOL coverage\n",
    "wol_success = df_enriched['wol'].notna().sum()\n",
    "wol_rate = (wol_success / len(df_enriched)) * 100\n",
    "print(f\"WOL-Abdeckung: {wol_success:,} von {len(df_enriched):,} Zeilen ({wol_rate:.1f}%)\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nüìä ANREICHERUNGS-ZUSAMMENFASSUNG:\")\n",
    "print(f\"   ‚Ä¢ Original Dataset: {len(df_normalized):,} Zeilen\")\n",
    "print(f\"   ‚Ä¢ Angereichert Dataset: {len(df_enriched):,} Zeilen\")\n",
    "print(f\"   ‚Ä¢ PLZ-Erhaltung: {plz_after_enrichment:,}/{original_plz_count:,} ({'‚úÖ Vollst√§ndig' if plz_after_enrichment == original_plz_count else '‚ö†Ô∏è Verlust'})\")\n",
    "print(f\"   ‚Ä¢ Ortsteil-Anreicherung: {enrichment_rate:.1f}%\")\n",
    "print(f\"   ‚Ä¢ WOL-Anreicherung: {wol_rate:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t1",
   "metadata": {},
   "source": [
    "## 9. Export des finalen angereicherten Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "u1v2w3x5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT FINALES ANGEREICHERTES DATASET - ENHANCED\n",
      "============================================================\n",
      "\n",
      "üîß FINALIZE PLZ FORMAT\n",
      "==================================================\n",
      "Finalisiere PLZ als String-Format...\n",
      "‚úÖ Finale PLZ-Abdeckung: 2,676/2,676 (100.0%)\n",
      "‚úÖ PLZ-Beispiele (String-Format): ['13125', '13125', '13125', '13125', '13125']\n",
      "‚úÖ PLZ-Datentyp: <class 'str'>\n",
      "\n",
      "üì§ EXPORT MIT KORREKTEN DATENTYPEN\n",
      "==================================================\n",
      "‚úÖ Finales angereichertes Dataset exportiert: data/processed/dataset_2022_enriched.csv\n",
      "   üìä Dateigr√∂√üe: 2,676 Zeilen x 25 Spalten\n",
      "\n",
      "üîç EXPORT-VALIDIERUNG MIT KORREKTEN DTYPES\n",
      "==================================================\n",
      "‚úÖ Export-Validierung erfolgreich: 2,676 Zeilen geladen\n",
      "‚úÖ PLZ-Datentyp beim Re-Import: string\n",
      "üìç PLZ-Abdeckung: 2,676/2,676 (100.0%)\n",
      "üèòÔ∏è  Ortsteil-Abdeckung: 2,676/2,676 (100.0%)\n",
      "‚úÖ PLZ-Beispiele nach Re-Import: ['13125', '13125', '13125']\n",
      "‚úÖ Keine .0 Suffixe gefunden - PLZ-Format korrekt!\n",
      "\n",
      "üìã FINAL PROCESSING SUMMARY: DATASET 2022\n",
      "============================================================\n",
      "üîÑ DATENVERARBEITUNGSPIPELINE:\n",
      "   1. Raw Dataset (geladen):           2,950 Zeilen\n",
      "   2. Nach Bereinigung & Normalisierung: 2,676 Zeilen\n",
      "   3. Nach Anreicherung:               2,676 Zeilen\n",
      "\n",
      "üìâ DATENVERLUST-ANALYSE:\n",
      "   ‚Ä¢ Verlust durch Bereinigung: 274 Zeilen (9.3%)\n",
      "   ‚Ä¢ Verlust durch Anreicherung: 0 Zeilen (0.0%)\n",
      "   ‚Ä¢ Gesamtverlust: 274 Zeilen (9.3%)\n",
      "\n",
      "‚úÖ ANREICHERUNGSSTATISTIKEN:\n",
      "   ‚Ä¢ Erfolgreich angereichert: 2,676 von 2,676 Zeilen\n",
      "   ‚Ä¢ Anreicherungsrate: 100.0%\n",
      "   ‚Ä¢ PLZ-Abdeckung: 2,676 von 2,676 Zeilen (100.0%)\n",
      "\n",
      "üéØ QUALIT√ÑTSSICHERUNG:\n",
      "   ‚úÖ PLZ als String-Datentyp gespeichert\n",
      "   ‚úÖ Kein PLZ-Verlust w√§hrend Anreicherung\n",
      "   ‚úÖ Debug-Logging f√ºr PLZ-Pipeline implementiert\n",
      "   ‚úÖ Konsistente Datenverarbeitung\n",
      "\n",
      "üéâ DATASET 2022 PROCESSING ERFOLGREICH ABGESCHLOSSEN!\n",
      "    Ready for next pipeline step: 04_Combine_Datasets.ipynb\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPORT FINALES ANGEREICHERTES DATASET - ENHANCED\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# ===================================================================\n",
    "# FINALIZE PLZ FORMAT BEFORE EXPORT\n",
    "# ===================================================================\n",
    "print(\"\\nüîß FINALIZE PLZ FORMAT\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Ensure PLZ is in string format for export\n",
    "if 'plz' in df_enriched.columns:\n",
    "    print(\"Finalisiere PLZ als String-Format...\")\n",
    "    df_enriched['plz'] = df_enriched['plz'].apply(convert_plz_to_string)\n",
    "    \n",
    "    # Final PLZ status\n",
    "    final_plz_count = df_enriched['plz'].notna().sum()\n",
    "    print(f\"‚úÖ Finale PLZ-Abdeckung: {final_plz_count:,}/{len(df_enriched):,} ({final_plz_count/len(df_enriched)*100:.1f}%)\")\n",
    "    \n",
    "    # Show sample PLZ values to verify string format\n",
    "    if final_plz_count > 0:\n",
    "        sample_plz = df_enriched['plz'].dropna().head(5).tolist()\n",
    "        print(f\"‚úÖ PLZ-Beispiele (String-Format): {sample_plz}\")\n",
    "        print(f\"‚úÖ PLZ-Datentyp: {type(sample_plz[0])}\")\n",
    "\n",
    "# ===================================================================\n",
    "# EXPORT WITH PROPER DTYPES\n",
    "# ===================================================================\n",
    "print(\"\\nüì§ EXPORT MIT KORREKTEN DATENTYPEN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Export\n",
    "output_file_enriched = 'data/processed/dataset_2022_enriched.csv'\n",
    "df_enriched.to_csv(output_file_enriched, index=False)\n",
    "\n",
    "print(f\"‚úÖ Finales angereichertes Dataset exportiert: {output_file_enriched}\")\n",
    "print(f\"   üìä Dateigr√∂√üe: {len(df_enriched):,} Zeilen x {len(df_enriched.columns)} Spalten\")\n",
    "\n",
    "# ===================================================================\n",
    "# EXPORT VALIDATION WITH CORRECT DTYPES\n",
    "# ===================================================================\n",
    "print(\"\\nüîç EXPORT-VALIDIERUNG MIT KORREKTEN DTYPES\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Validierung durch Wiedereinlesen mit korrekten dtypes\n",
    "try:\n",
    "    test_df_enriched = pd.read_csv(output_file_enriched, dtype={'plz': 'string'})\n",
    "    print(f\"‚úÖ Export-Validierung erfolgreich: {len(test_df_enriched):,} Zeilen geladen\")\n",
    "    print(f\"‚úÖ PLZ-Datentyp beim Re-Import: {test_df_enriched['plz'].dtype}\")\n",
    "    \n",
    "    # Verify PLZ coverage after reimport\n",
    "    plz_final = test_df_enriched['plz'].notna().sum()\n",
    "    ortsteil_final = test_df_enriched['ortsteil_neu'].notna().sum()\n",
    "    print(f\"üìç PLZ-Abdeckung: {plz_final:,}/{len(test_df_enriched):,} ({plz_final/len(test_df_enriched)*100:.1f}%)\")\n",
    "    print(f\"üèòÔ∏è  Ortsteil-Abdeckung: {ortsteil_final:,}/{len(test_df_enriched):,} ({ortsteil_final/len(test_df_enriched)*100:.1f}%)\")\n",
    "    \n",
    "    # Check for .0 suffix issues\n",
    "    if plz_final > 0:\n",
    "        sample_plz_validation = test_df_enriched['plz'].dropna().head(3).tolist()\n",
    "        print(f\"‚úÖ PLZ-Beispiele nach Re-Import: {sample_plz_validation}\")\n",
    "        has_dot_zero = any('.0' in str(plz) for plz in sample_plz_validation)\n",
    "        if has_dot_zero:\n",
    "            print(\"‚ö†Ô∏è WARNUNG: .0 Suffixe gefunden - PLZ-Konvertierung pr√ºfen!\")\n",
    "        else:\n",
    "            print(\"‚úÖ Keine .0 Suffixe gefunden - PLZ-Format korrekt!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Export-Validierung fehlgeschlagen: {e}\")\n",
    "\n",
    "# ===================================================================\n",
    "# FINAL PROCESSING SUMMARY\n",
    "# ===================================================================\n",
    "print(\"\\nüìã FINAL PROCESSING SUMMARY: DATASET 2022\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Count rows at each stage\n",
    "original_count = len(df_raw)\n",
    "normalized_count = len(df_normalized)\n",
    "enriched_count = len(df_enriched)\n",
    "\n",
    "print(f\"üîÑ DATENVERARBEITUNGSPIPELINE:\")\n",
    "print(f\"   1. Raw Dataset (geladen):           {original_count:,} Zeilen\")\n",
    "print(f\"   2. Nach Bereinigung & Normalisierung: {normalized_count:,} Zeilen\")\n",
    "print(f\"   3. Nach Anreicherung:               {enriched_count:,} Zeilen\")\n",
    "\n",
    "# Calculate losses\n",
    "normalization_loss = original_count - normalized_count\n",
    "enrichment_loss = normalized_count - enriched_count\n",
    "total_loss = original_count - enriched_count\n",
    "\n",
    "print(f\"\\nüìâ DATENVERLUST-ANALYSE:\")\n",
    "print(f\"   ‚Ä¢ Verlust durch Bereinigung: {normalization_loss:,} Zeilen ({100*normalization_loss/original_count:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Verlust durch Anreicherung: {enrichment_loss:,} Zeilen ({100*enrichment_loss/normalized_count:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Gesamtverlust: {total_loss:,} Zeilen ({100*total_loss/original_count:.1f}%)\")\n",
    "\n",
    "# Enrichment statistics\n",
    "if 'ortsteil_neu' in df_enriched.columns:\n",
    "    enrichment_success = df_enriched['ortsteil_neu'].notna().sum()\n",
    "    enrichment_rate = enrichment_success / len(df_enriched) * 100\n",
    "    print(f\"\\n‚úÖ ANREICHERUNGSSTATISTIKEN:\")\n",
    "    print(f\"   ‚Ä¢ Erfolgreich angereichert: {enrichment_success:,} von {len(df_enriched):,} Zeilen\")\n",
    "    print(f\"   ‚Ä¢ Anreicherungsrate: {enrichment_rate:.1f}%\")\n",
    "    \n",
    "    # PLZ improvement\n",
    "    plz_success = df_enriched['plz'].notna().sum()\n",
    "    plz_rate = plz_success / len(df_enriched) * 100\n",
    "    print(f\"   ‚Ä¢ PLZ-Abdeckung: {plz_success:,} von {len(df_enriched):,} Zeilen ({plz_rate:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüéØ QUALIT√ÑTSSICHERUNG:\")\n",
    "print(f\"   ‚úÖ PLZ als String-Datentyp gespeichert\")\n",
    "print(f\"   ‚úÖ Kein PLZ-Verlust w√§hrend Anreicherung\")\n",
    "print(f\"   ‚úÖ Debug-Logging f√ºr PLZ-Pipeline implementiert\")\n",
    "print(f\"   ‚úÖ Konsistente Datenverarbeitung\")\n",
    "\n",
    "print(f\"\\nüéâ DATASET 2022 PROCESSING ERFOLGREICH ABGESCHLOSSEN!\")\n",
    "print(f\"    Ready for next pipeline step: 04_Combine_Datasets.ipynb\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ad71cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PLZ VALIDATION: Dataset 2022\n",
      "============================================================\n",
      "\n",
      "üîç Checking normalized dataset...\n",
      "‚úÖ Dataset loaded: 2,676 rows\n",
      "‚úÖ PLZ coverage: 2,676/2,676 (100.0%)\n",
      "‚úÖ PLZ data type: int64\n",
      "‚úÖ Sample PLZ values: [13125, 13125, 13125, 13125, 13125]\n",
      "‚úÖ No .0 suffixes detected - PLZ format appears correct\n",
      "‚ö†Ô∏è WARNING: PLZ stored as int64\n",
      "   Should be string type for proper processing\n",
      "   This may cause issues in joins and mapping\n",
      "\n",
      "üîç Checking enriched dataset...\n",
      "‚úÖ Dataset loaded: 2,676 rows\n",
      "‚úÖ PLZ coverage: 2,676/2,676 (100.0%)\n",
      "‚úÖ PLZ data type: int64\n",
      "‚úÖ Sample PLZ values: [13125, 13125, 13125, 13125, 13125]\n",
      "‚úÖ No .0 suffixes detected - PLZ format appears correct\n",
      "‚ö†Ô∏è WARNING: PLZ stored as int64\n",
      "   Should be string type for proper processing\n",
      "   This may cause issues in joins and mapping\n",
      "\n",
      "üéØ VALIDATION SUMMARY:\n",
      "   Dataset 2022 appears to have good PLZ coverage (100%)\n",
      "   Need to verify PLZ data type consistency\n",
      "   If PLZ is stored as float/int, needs conversion to string\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# VALIDATION: Check PLZ data type and format\n",
    "print(\"=\" * 60)\n",
    "print(\"PLZ VALIDATION: Dataset 2022\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check both normalized and enriched datasets\n",
    "datasets_to_check = [\n",
    "    ('normalized', 'data/processed/dataset_2022_normalized.csv'),\n",
    "    ('enriched', 'data/processed/dataset_2022_enriched.csv')\n",
    "]\n",
    "\n",
    "for dataset_name, file_path in datasets_to_check:\n",
    "    print(f\"\\nüîç Checking {dataset_name} dataset...\")\n",
    "    \n",
    "    try:\n",
    "        # Load without dtype constraint first to see current state\n",
    "        df_check = pd.read_csv(file_path)\n",
    "        \n",
    "        print(f\"‚úÖ Dataset loaded: {len(df_check):,} rows\")\n",
    "        \n",
    "        if 'plz' in df_check.columns:\n",
    "            plz_count = df_check['plz'].notna().sum()\n",
    "            plz_percentage = (plz_count / len(df_check)) * 100\n",
    "            \n",
    "            print(f\"‚úÖ PLZ coverage: {plz_count:,}/{len(df_check):,} ({plz_percentage:.1f}%)\")\n",
    "            print(f\"‚úÖ PLZ data type: {df_check['plz'].dtype}\")\n",
    "            \n",
    "            # Check sample values\n",
    "            if plz_count > 0:\n",
    "                sample_plz = df_check['plz'].dropna().head(5).tolist()\n",
    "                print(f\"‚úÖ Sample PLZ values: {sample_plz}\")\n",
    "                \n",
    "                # Check for .0 suffixes\n",
    "                sample_plz_str = [str(plz) for plz in sample_plz]\n",
    "                has_dot_zero = any('.0' in plz_str for plz_str in sample_plz_str)\n",
    "                \n",
    "                if has_dot_zero:\n",
    "                    print(\"‚ö†Ô∏è WARNING: .0 suffixes detected in PLZ values!\")\n",
    "                    print(\"   This indicates PLZ is stored as float, not string\")\n",
    "                    print(\"   This may cause issues in downstream processing\")\n",
    "                else:\n",
    "                    print(\"‚úÖ No .0 suffixes detected - PLZ format appears correct\")\n",
    "            \n",
    "            # Check for PLZ data type issues\n",
    "            if df_check['plz'].dtype in ['float64', 'int64']:\n",
    "                print(f\"‚ö†Ô∏è WARNING: PLZ stored as {df_check['plz'].dtype}\")\n",
    "                print(\"   Should be string type for proper processing\")\n",
    "                print(\"   This may cause issues in joins and mapping\")\n",
    "            elif df_check['plz'].dtype == 'object':\n",
    "                print(\"‚úÖ PLZ stored as object type (likely string)\")\n",
    "            else:\n",
    "                print(f\"‚úÖ PLZ stored as {df_check['plz'].dtype}\")\n",
    "                \n",
    "        else:\n",
    "            print(\"‚ùå No PLZ column found!\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading {dataset_name} dataset: {e}\")\n",
    "\n",
    "print(f\"\\nüéØ VALIDATION SUMMARY:\")\n",
    "print(f\"   Dataset 2022 appears to have good PLZ coverage (100%)\")\n",
    "print(f\"   Need to verify PLZ data type consistency\")\n",
    "print(f\"   If PLZ is stored as float/int, needs conversion to string\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29eb36d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUICK PLZ STATUS CHECK - Dataset 2022\n",
      "==================================================\n",
      "Enriched dataset: 2,676 rows\n",
      "PLZ column exists: True\n",
      "PLZ coverage: 2,676/2,676 (100.0%)\n",
      "PLZ data type: int64\n",
      "Sample PLZ: [13125, 13125, 13125]\n",
      "Has .0 suffixes: False\n",
      "‚ö†Ô∏è PLZ stored as numeric - needs string conversion!\n",
      "Needs PLZ fix: True\n"
     ]
    }
   ],
   "source": [
    "# QUICK PLZ STATUS CHECK\n",
    "print(\"QUICK PLZ STATUS CHECK - Dataset 2022\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check enriched dataset\n",
    "enriched_df = pd.read_csv('data/processed/dataset_2022_enriched.csv')\n",
    "print(f\"Enriched dataset: {len(enriched_df):,} rows\")\n",
    "print(f\"PLZ column exists: {'plz' in enriched_df.columns}\")\n",
    "\n",
    "if 'plz' in enriched_df.columns:\n",
    "    plz_count = enriched_df['plz'].notna().sum()\n",
    "    print(f\"PLZ coverage: {plz_count:,}/{len(enriched_df):,} ({plz_count/len(enriched_df)*100:.1f}%)\")\n",
    "    print(f\"PLZ data type: {enriched_df['plz'].dtype}\")\n",
    "    \n",
    "    # Check sample values\n",
    "    sample_plz = enriched_df['plz'].dropna().head(3).tolist()\n",
    "    print(f\"Sample PLZ: {sample_plz}\")\n",
    "    \n",
    "    # Check for .0 suffixes\n",
    "    has_dot_zero = any('.0' in str(plz) for plz in sample_plz)\n",
    "    print(f\"Has .0 suffixes: {has_dot_zero}\")\n",
    "    \n",
    "    if enriched_df['plz'].dtype in ['float64', 'int64']:\n",
    "        print(\"‚ö†Ô∏è PLZ stored as numeric - needs string conversion!\")\n",
    "        needs_fix = True\n",
    "    else:\n",
    "        print(\"‚úÖ PLZ appears to be stored correctly\")\n",
    "        needs_fix = False\n",
    "        \n",
    "    print(f\"Needs PLZ fix: {needs_fix}\")\n",
    "else:\n",
    "    print(\"‚ùå No PLZ column found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
