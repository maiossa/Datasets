{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eacc2b7",
   "metadata": {},
   "source": [
    "# 04_Combine_Datasets - Finale Dataset-Kombination\n",
    "\n",
    "**Kombination aller normalisierten und angereicherten Datasets**\n",
    "- L√§dt die drei normalisierten und angereicherten Datasets (2018-2019, 2022, 2025)\n",
    "- Pr√ºft Schema-Kompatibilit√§t\n",
    "- F√ºhrt Qualit√§tspr√ºfungen durch\n",
    "- Erstellt finales kombiniertes und angereichertes Dataset\n",
    "\n",
    "**Input:**\n",
    "- `data/processed/dataset_2018_2019_enriched.csv`\n",
    "- `data/processed/dataset_2022_enriched.csv` \n",
    "- `data/processed/dataset_2025_enriched.csv`\n",
    "\n",
    "**Output:**\n",
    "- `data/processed/berlin_housing_combined_enriched_final.csv`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "478a928a",
   "metadata": {},
   "source": [
    "## 1. Setup und Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90c54540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotheken erfolgreich importiert!\n",
      "Pandas Version: 2.3.0\n",
      "Ziel: Kombination aller normalisierten Datasets\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Bibliotheken erfolgreich importiert!\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(\"Ziel: Kombination aller normalisierten Datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc575363",
   "metadata": {},
   "source": [
    "## 2. Angereicherte Datasets laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f81cd754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANGEREICHERTE DATASETS LADEN\n",
      "============================================================\n",
      "‚úÖ 2018_2019: 10,387 Zeilen, 15 Spalten\n",
      "‚úÖ 2022: 2,676 Zeilen, 25 Spalten\n",
      "‚úÖ 2025: 4,424 Zeilen, 14 Spalten\n",
      "\n",
      "Geladen: 3 von 3 Datasets\n",
      "üîÑ 2018_2019 neu geladen: 10,387 Zeilen\n",
      "üîÑ 2022 neu geladen: 2,676 Zeilen\n",
      "üîÑ 2025 neu geladen: 4,424 Zeilen\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"ANGEREICHERTE DATASETS LADEN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Dateipfade zu den angereicherten Datasets\n",
    "file_paths = {\n",
    "    '2018_2019': 'data/processed/dataset_2018_2019_enriched.csv',\n",
    "    '2022': 'data/processed/dataset_2022_enriched.csv',\n",
    "    '2025': 'data/processed/dataset_2025_enriched.csv'\n",
    "}\n",
    "\n",
    "# Laden der Datasets\n",
    "datasets = {}\n",
    "for dataset_name, file_path in file_paths.items():\n",
    "    df = pd.read_csv(file_path)\n",
    "    datasets[dataset_name] = df\n",
    "    print(f\"‚úÖ {dataset_name}: {len(df):,} Zeilen, {len(df.columns)} Spalten\")\n",
    "\n",
    "print(f\"\\nGeladen: {len(datasets)} von {len(file_paths)} Datasets\")\n",
    "\n",
    "# Forciere Neuladung der Datasets\n",
    "for name, df in datasets.items():\n",
    "    # Neu laden um sicherzustellen, dass wir die neuesten Daten haben\n",
    "    df = pd.read_csv(file_paths[name])\n",
    "    datasets[name] = df\n",
    "    print(f\"üîÑ {name} neu geladen: {len(df):,} Zeilen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4e8d61",
   "metadata": {},
   "source": [
    "## 3. Schema-Kompatibilit√§t pr√ºfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2fb70ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SCHEMA-KOMPATIBILIT√ÑT PR√úFEN\n",
      "============================================================\n",
      "Erwartete Basis-Spalten (vor PLZ-Enhancement):\n",
      "  - price\n",
      "  - size\n",
      "  - district\n",
      "  - rooms\n",
      "  - year\n",
      "  - dataset_id\n",
      "  - source\n",
      "  - wol\n",
      "  - plz\n",
      "\n",
      "Spalten die durch PLZ-Enhancement hinzugef√ºgt werden:\n",
      "  - ortsteil\n",
      "  - bezirk\n",
      "  - lat\n",
      "  - lon\n",
      "\n",
      "=== DATASET 2018_2019 ===\n",
      "Spalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'street', 'floor', 'typeOfFlat', 'yearConstructed', 'totalRent', 'plz', 'wol', 'ortsteil_neu']\n",
      "‚úÖ Alle Basis-Spalten vorhanden\n",
      "‚ÑπÔ∏è  Zus√§tzliche Spalten: 6 (['street', 'floor', 'typeOfFlat']...)\n",
      "üìç PLZ verf√ºgbar: 1,760/10,387 (16.9%)\n",
      "\n",
      "=== DATASET 2022 ===\n",
      "Spalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'plz', 'warmmiete', 'nebenkosten', 'kaution', 'baujahr', 'zustand', 'energieeffiziensklasse', 'ausstattung_m√∂bliert', 'ausstattung_balkon', 'ausstattung_terrasse', 'ausstattung_garten', 'ausstattung_einbauk√ºche', 'ausstattung_garage', 'ausstattung_stellplatz', 'ausstattung_personenaufzug', 'ausstattung_keller', 'wol', 'ortsteil_neu']\n",
      "‚úÖ Alle Basis-Spalten vorhanden\n",
      "‚ÑπÔ∏è  Zus√§tzliche Spalten: 16 (['warmmiete', 'nebenkosten', 'kaution']...)\n",
      "üìç PLZ verf√ºgbar: 2,676/2,676 (100.0%)\n",
      "\n",
      "=== DATASET 2025 ===\n",
      "Spalten: ['plz', 'wol', 'link', 'size_original', 'size', 'district', 'price_original', 'address', 'dataset_id', 'source', 'year', 'price', 'ortsteil_neu', 'title', 'rooms']\n",
      "‚úÖ Alle Basis-Spalten vorhanden\n",
      "‚ÑπÔ∏è  Zus√§tzliche Spalten: 6 (['link', 'size_original', 'price_original']...)\n",
      "üìç PLZ verf√ºgbar: 56/4,424 (1.3%)\n",
      "\n",
      "=== SCHEMA-KOMPATIBILIT√ÑT ===\n",
      "Valide Datasets: 3/3\n",
      "‚úÖ Alle Datasets sind schema-kompatibel!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"SCHEMA-KOMPATIBILIT√ÑT PR√úFEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Definiere erwartete Standardspalten (vor PLZ-Enhancement)\n",
    "base_columns = ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'wol', 'plz']\n",
    "\n",
    "# Spalten die nach PLZ-Enhancement hinzugef√ºgt werden\n",
    "enhanced_columns = ['ortsteil', 'bezirk', 'lat', 'lon']\n",
    "\n",
    "print(\"Erwartete Basis-Spalten (vor PLZ-Enhancement):\")\n",
    "for col in base_columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(\"\\nSpalten die durch PLZ-Enhancement hinzugef√ºgt werden:\")\n",
    "for col in enhanced_columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "# Pr√ºfe jedes Dataset (nur gegen Basis-Spalten)\n",
    "schema_check = {}\n",
    "for dataset_name, df in datasets.items():\n",
    "    missing_cols = [col for col in base_columns if col not in df.columns]\n",
    "    extra_cols = [col for col in df.columns if col not in base_columns]\n",
    "    \n",
    "    schema_check[dataset_name] = {\n",
    "        'missing': missing_cols,\n",
    "        'extra': extra_cols,\n",
    "        'valid': len(missing_cols) == 0\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n=== DATASET {dataset_name.upper()} ===\")\n",
    "    print(f\"Spalten: {list(df.columns)}\")\n",
    "    if missing_cols:\n",
    "        print(f\"‚ùå Fehlende Basis-Spalten: {missing_cols}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Alle Basis-Spalten vorhanden\")\n",
    "    \n",
    "    if extra_cols:\n",
    "        print(f\"‚ÑπÔ∏è  Zus√§tzliche Spalten: {len(extra_cols)} ({extra_cols[:3]}...)\")\n",
    "    \n",
    "    # Pr√ºfe PLZ-Verf√ºgbarkeit speziell\n",
    "    if 'plz' in df.columns:\n",
    "        plz_available = df['plz'].notna().sum()\n",
    "        print(f\"üìç PLZ verf√ºgbar: {plz_available:,}/{len(df):,} ({plz_available/len(df)*100:.1f}%)\")\n",
    "\n",
    "# Zusammenfassung\n",
    "valid_datasets = [name for name, check in schema_check.items() if check['valid']]\n",
    "print(f\"\\n=== SCHEMA-KOMPATIBILIT√ÑT ===\")\n",
    "print(f\"Valide Datasets: {len(valid_datasets)}/{len(datasets)}\")\n",
    "if len(valid_datasets) == len(datasets):\n",
    "    print(\"‚úÖ Alle Datasets sind schema-kompatibel!\")\n",
    "else:\n",
    "    print(\"‚ùå Schema-Inkompatibilit√§ten gefunden!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6b09c5",
   "metadata": {},
   "source": [
    "## 4. Datenqualit√§t pr√ºfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "70223a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATENQUALIT√ÑT PR√úFEN\n",
      "============================================================\n",
      "\n",
      "=== DATASET 2018_2019 ===\n",
      "  price: 10,387/10,387 (100.0%) nicht-null\n",
      "  size: 10,387/10,387 (100.0%) nicht-null\n",
      "  district: 10,387/10,387 (100.0%) nicht-null\n",
      "  rooms: 10,387/10,387 (100.0%) nicht-null\n",
      "  year: 10,387/10,387 (100.0%) nicht-null\n",
      "  dataset_id: 10,387/10,387 (100.0%) nicht-null\n",
      "  source: 10,387/10,387 (100.0%) nicht-null\n",
      "  wol: 1,760/10,387 (16.9%) nicht-null\n",
      "  plz: 1,760/10,387 (16.9%) nicht-null\n",
      "  Preis (50-20000‚Ç¨): 10,387/10,387 (100.0%) g√ºltig\n",
      "  Gr√∂√üe (5-1000m¬≤): 10,387/10,387 (100.0%) g√ºltig\n",
      "  Einzigartige Bezirke: 79\n",
      "  PLZ verf√ºgbar: 1,760/10,387 (16.9%)\n",
      "\n",
      "=== DATASET 2022 ===\n",
      "  price: 2,676/2,676 (100.0%) nicht-null\n",
      "  size: 2,676/2,676 (100.0%) nicht-null\n",
      "  district: 2,676/2,676 (100.0%) nicht-null\n",
      "  rooms: 2,676/2,676 (100.0%) nicht-null\n",
      "  year: 2,676/2,676 (100.0%) nicht-null\n",
      "  dataset_id: 2,676/2,676 (100.0%) nicht-null\n",
      "  source: 2,676/2,676 (100.0%) nicht-null\n",
      "  wol: 2,676/2,676 (100.0%) nicht-null\n",
      "  plz: 2,676/2,676 (100.0%) nicht-null\n",
      "  Preis (50-20000‚Ç¨): 2,676/2,676 (100.0%) g√ºltig\n",
      "  Gr√∂√üe (5-1000m¬≤): 2,676/2,676 (100.0%) g√ºltig\n",
      "  Einzigartige Bezirke: 21\n",
      "  PLZ verf√ºgbar: 2,676/2,676 (100.0%)\n",
      "\n",
      "=== DATASET 2025 ===\n",
      "  price: 4,424/4,424 (100.0%) nicht-null\n",
      "  size: 4,424/4,424 (100.0%) nicht-null\n",
      "  district: 4,424/4,424 (100.0%) nicht-null\n",
      "  rooms: 0/4,424 (0.0%) nicht-null\n",
      "  year: 4,424/4,424 (100.0%) nicht-null\n",
      "  dataset_id: 4,424/4,424 (100.0%) nicht-null\n",
      "  source: 4,424/4,424 (100.0%) nicht-null\n",
      "  wol: 54/4,424 (1.2%) nicht-null\n",
      "  plz: 56/4,424 (1.3%) nicht-null\n",
      "  Preis (50-20000‚Ç¨): 4,424/4,424 (100.0%) g√ºltig\n",
      "  Gr√∂√üe (5-1000m¬≤): 4,424/4,424 (100.0%) g√ºltig\n",
      "  Einzigartige Bezirke: 20\n",
      "  PLZ verf√ºgbar: 56/4,424 (1.3%)\n",
      "\n",
      "‚úÖ Datenqualit√§tspr√ºfung abgeschlossen\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATENQUALIT√ÑT PR√úFEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "quality_report = {}\n",
    "\n",
    "for dataset_name, df in datasets.items():\n",
    "    print(f\"\\n=== DATASET {dataset_name.upper()} ===\")\n",
    "    \n",
    "    # Grundlegende Statistiken\n",
    "    total_rows = len(df)\n",
    "    \n",
    "    # Vollst√§ndigkeit pr√ºfen (verwende base_columns)\n",
    "    completeness = {}\n",
    "    for col in base_columns:\n",
    "        if col in df.columns:\n",
    "            non_null = df[col].notna().sum()\n",
    "            completeness[col] = (non_null, non_null/total_rows*100)\n",
    "            print(f\"  {col}: {non_null:,}/{total_rows:,} ({non_null/total_rows*100:.1f}%) nicht-null\")\n",
    "    \n",
    "    # Wertebereichs-Pr√ºfungen\n",
    "    if 'price' in df.columns:\n",
    "        price_valid = ((df['price'] >= 50) & (df['price'] <= 20000)).sum()\n",
    "        print(f\"  Preis (50-20000‚Ç¨): {price_valid:,}/{total_rows:,} ({price_valid/total_rows*100:.1f}%) g√ºltig\")\n",
    "    \n",
    "    if 'size' in df.columns:\n",
    "        size_valid = ((df['size'] >= 5) & (df['size'] <= 1000)).sum()\n",
    "        print(f\"  Gr√∂√üe (5-1000m¬≤): {size_valid:,}/{total_rows:,} ({size_valid/total_rows*100:.1f}%) g√ºltig\")\n",
    "    \n",
    "    if 'district' in df.columns:\n",
    "        unique_districts = df['district'].nunique()\n",
    "        print(f\"  Einzigartige Bezirke: {unique_districts}\")\n",
    "    \n",
    "    # PLZ-Qualit√§t pr√ºfen\n",
    "    if 'plz' in df.columns:\n",
    "        plz_available = df['plz'].notna().sum()\n",
    "        print(f\"  PLZ verf√ºgbar: {plz_available:,}/{total_rows:,} ({plz_available/total_rows*100:.1f}%)\")\n",
    "    \n",
    "    quality_report[dataset_name] = {\n",
    "        'total_rows': total_rows,\n",
    "        'completeness': completeness,\n",
    "        'unique_districts': df['district'].nunique() if 'district' in df.columns else 0,\n",
    "        'plz_available': df['plz'].notna().sum() if 'plz' in df.columns else 0\n",
    "    }\n",
    "\n",
    "print(f\"\\n‚úÖ Datenqualit√§tspr√ºfung abgeschlossen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de277ee0",
   "metadata": {},
   "source": [
    "## 5. Datasets kombinieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74881885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASETS KOMBINIEREN\n",
      "============================================================\n",
      "2018_2019: 10,387 Zeilen mit 9 Basis-Spalten\n",
      "2022: 2,676 Zeilen mit 9 Basis-Spalten\n",
      "2025: 4,424 Zeilen mit 9 Basis-Spalten\n",
      "\n",
      "Kombiniere Datasets...\n",
      "‚úÖ Kombiniertes Dataset erstellt: 17,487 Zeilen\n",
      "\n",
      "=== KOMBINATIONS-ZUSAMMENFASSUNG ===\n",
      "Input-Zeilen gesamt: 17,487\n",
      "Output-Zeilen: 17,487\n",
      "Datenverlust: 0 (0.0%)\n",
      "\n",
      "=== JAHRESVERTEILUNG ===\n",
      "  2019: 10,387 Eintr√§ge (59.4%)\n",
      "  2022: 2,676 Eintr√§ge (15.3%)\n",
      "  2025: 4,424 Eintr√§ge (25.3%)\n",
      "\n",
      "=== DATASET-VERTEILUNG ===\n",
      "  historical: 10,387 Eintr√§ge (59.4%)\n",
      "  recent: 4,424 Eintr√§ge (25.3%)\n",
      "  current: 2,676 Eintr√§ge (15.3%)\n",
      "\n",
      "=== PLZ-VERF√úGBARKEIT ===\n",
      "PLZ verf√ºgbar: 4,492/17,487 (25.7%)\n",
      "‚úÖ Bereit f√ºr PLZ-Enhancement mit Ortsteil und Koordinaten\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"DATASETS KOMBINIEREN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Nur Basis-Spalten f√ºr Kombination verwenden (PLZ-Enhancement kommt sp√§ter)\n",
    "datasets_standard = {}\n",
    "for dataset_name, df in datasets.items():\n",
    "    # W√§hle nur Basis-Spalten aus\n",
    "    available_base_cols = [col for col in base_columns if col in df.columns]\n",
    "    df_std = df[available_base_cols].copy()\n",
    "    datasets_standard[dataset_name] = df_std\n",
    "    print(f\"{dataset_name}: {len(df_std):,} Zeilen mit {len(available_base_cols)} Basis-Spalten\")\n",
    "\n",
    "# Kombiniere alle Datasets\n",
    "print(f\"\\nKombiniere Datasets...\")\n",
    "combined_df = pd.concat(datasets_standard.values(), ignore_index=True, sort=False)\n",
    "\n",
    "print(f\"‚úÖ Kombiniertes Dataset erstellt: {len(combined_df):,} Zeilen\")\n",
    "\n",
    "# Zusammenfassung\n",
    "print(f\"\\n=== KOMBINATIONS-ZUSAMMENFASSUNG ===\")\n",
    "total_input_rows = sum(len(df) for df in datasets.values())\n",
    "print(f\"Input-Zeilen gesamt: {total_input_rows:,}\")\n",
    "print(f\"Output-Zeilen: {len(combined_df):,}\")\n",
    "print(f\"Datenverlust: {total_input_rows - len(combined_df):,} ({(total_input_rows - len(combined_df))/total_input_rows*100:.1f}%)\")\n",
    "\n",
    "# Verteilung nach Jahren\n",
    "print(f\"\\n=== JAHRESVERTEILUNG ===\")\n",
    "year_counts = combined_df['year'].value_counts().sort_index()\n",
    "for year, count in year_counts.items():\n",
    "    print(f\"  {year}: {count:,} Eintr√§ge ({count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "# Verteilung nach Dataset-ID\n",
    "print(f\"\\n=== DATASET-VERTEILUNG ===\")\n",
    "dataset_counts = combined_df['dataset_id'].value_counts()\n",
    "for dataset_id, count in dataset_counts.items():\n",
    "    print(f\"  {dataset_id}: {count:,} Eintr√§ge ({count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "# PLZ-Verf√ºgbarkeit pr√ºfen\n",
    "if 'plz' in combined_df.columns:\n",
    "    plz_available = combined_df['plz'].notna().sum()\n",
    "    print(f\"\\n=== PLZ-VERF√úGBARKEIT ===\")\n",
    "    print(f\"PLZ verf√ºgbar: {plz_available:,}/{len(combined_df):,} ({plz_available/len(combined_df)*100:.1f}%)\")\n",
    "    print(\"‚úÖ Bereit f√ºr PLZ-Enhancement mit Ortsteil und Koordinaten\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad070f",
   "metadata": {},
   "source": [
    "## 5.5. Erweiterte PLZ-Geolocation hinzuf√ºgen\n",
    "\n",
    "**üéØ Integration der verbesserten PLZ-Mapping-Datei**\n",
    "\n",
    "Anstatt nur auf Bezirksebene zu arbeiten, verwenden wir jetzt die neue `berlin_plz_mapping_enhanced.csv`, die:\n",
    "- **Ortsteil-Level-Genauigkeit** bietet (z.B. PLZ 12355 ‚Üí Rudow statt nur Neuk√∂lln)\n",
    "- **Echte Koordinaten** f√ºr jede PLZ enth√§lt (Lat/Lon)\n",
    "- **H√∂here r√§umliche Pr√§zision** f√ºr die Visualisierung erm√∂glicht\n",
    "\n",
    "Dies ist ein wichtiger Schritt zur Verbesserung der Datenqualit√§t und r√§umlichen Genauigkeit unserer Analyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0410dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ERWEITERTE PLZ-GEOLOCATION HINZUF√úGEN\n",
      "============================================================\n",
      "‚úÖ PLZ-Mapping geladen: 190 Eintr√§ge\n",
      "   Spalten: ['PLZ', 'Ortsteil', 'Bezirk', 'Lat', 'Lon']\n",
      "\n",
      "=== BEISPIELE VERBESSERTER PLZ-MAPPINGS ===\n",
      "PLZ 10249: Friedrichshain (Friedrichshain-Kreuzberg) ‚Üí 52.5159, 13.4533\n",
      "PLZ 12355: Rudow (Neuk√∂lln) ‚Üí 52.4000, 13.4667\n",
      "PLZ 13347: Gesundbrunnen (Mitte) ‚Üí 52.5511, 13.3885\n",
      "PLZ 14050: Westend (Charlottenburg-Wilmersdorf) ‚Üí 52.5167, 13.2833\n",
      "PLZ 10553: Moabit (Mitte) ‚Üí 52.5280, 13.3430\n",
      "\n",
      "=== PLZ-VERF√úGBARKEIT IM KOMBINIERTEN DATASET ===\n",
      "PLZ verf√ºgbar: 4,492/17,487 (25.7%)\n",
      "Einzigartige PLZ: 188\n",
      "H√§ufigste PLZ:\n",
      "  10315.0: 127 Eintr√§ge\n",
      "  13593.0: 113 Eintr√§ge\n",
      "  10245.0: 107 Eintr√§ge\n",
      "  12627.0: 100 Eintr√§ge\n",
      "  12555.0: 99 Eintr√§ge\n",
      "\n",
      "=== DATENTYP-PROBLEM BEHEBEN ===\n",
      "PLZ-Datentyp im combined_df: float64\n",
      "PLZ-Datentyp im mapping: object\n",
      "Beispiel PLZ aus combined_df: [13591.0, 12527.0, 13053.0, 13158.0, 14199.0]\n",
      "Beispiel PLZ aus mapping: ['10115', '10117', '10119', '10178', '10179']\n",
      "\n",
      "=== PLZ-MAPPING-JOIN DURCHF√úHREN ===\n",
      "PLZ nach Bereinigung - Beispiele:\n",
      "  Combined_df: ['13591', '12527', '13053', '13158', '14199']\n",
      "  Mapping: ['10115', '10117', '10119', '10178', '10179']\n",
      "Join-Ergebnis:\n",
      "  Zeilen vorher: 17,487\n",
      "  Zeilen nachher: 17,487\n",
      "  Koordinaten matched: 4,375/17,487 (25.0%)\n",
      "\n",
      "‚úÖ ERFOLGREICHE MATCHES (Top 5):\n",
      "  PLZ 13591.0 ‚Üí Spandau (Spandau) ‚Üí 52.5333, 13.2000\n",
      "  PLZ 12527.0 ‚Üí Schm√∂ckwitz (Treptow-K√∂penick) ‚Üí 52.3667, 13.6500\n",
      "  PLZ 13053.0 ‚Üí Neu-Hohensch√∂nhausen (Lichtenberg) ‚Üí 52.5500, 13.5000\n",
      "  PLZ 13158.0 ‚Üí Wilhelmsruh (Pankow) ‚Üí 52.5897, 13.4186\n",
      "  PLZ 14199.0 ‚Üí Schmargendorf (Charlottenburg-Wilmersdorf) ‚Üí 52.4667, 13.2833\n",
      "\n",
      "‚ö†Ô∏è  Nicht gematchte PLZ (Top 10):\n",
      "  13599.0: 51 Eintr√§ge\n",
      "  13629.0: 47 Eintr√§ge\n",
      "  13086.0: 17 Eintr√§ge\n",
      "  14612.0: 1 Eintr√§ge\n",
      "  10000.0: 1 Eintr√§ge\n",
      "\n",
      "‚úÖ Erweiterte PLZ-Geolocation erfolgreich hinzugef√ºgt!\n",
      "Neue Spalten: ortsteil, bezirk, lat, lon\n",
      "Finale Spalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'wol', 'plz', 'ortsteil', 'bezirk', 'lat', 'lon']\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ERWEITERTE PLZ-GEOLOCATION HINZUF√úGEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Lade die erweiterte PLZ-Mapping-Datei\n",
    "plz_mapping_file = 'data/processed/berlin_plz_mapping_enhanced.csv'\n",
    "try:\n",
    "    plz_mapping = pd.read_csv(plz_mapping_file, dtype={'PLZ': str})\n",
    "    print(f\"‚úÖ PLZ-Mapping geladen: {len(plz_mapping):,} Eintr√§ge\")\n",
    "    print(f\"   Spalten: {list(plz_mapping.columns)}\")\n",
    "    \n",
    "    # Zeige einige Beispiele\n",
    "    print(f\"\\n=== BEISPIELE VERBESSERTER PLZ-MAPPINGS ===\")\n",
    "    examples = ['10249', '12355', '13347', '14050', '10553']\n",
    "    for plz in examples:\n",
    "        if plz in plz_mapping['PLZ'].values:\n",
    "            row = plz_mapping[plz_mapping['PLZ'] == plz].iloc[0]\n",
    "            print(f\"PLZ {plz}: {row['Ortsteil']} ({row['Bezirk']}) ‚Üí {row['Lat']:.4f}, {row['Lon']:.4f}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå PLZ-Mapping-Datei nicht gefunden: {plz_mapping_file}\")\n",
    "    print(\"‚ö†Ô∏è  Erstelle die Datei mit: python3 create_enhanced_plz_mapping_with_coords.py\")\n",
    "    plz_mapping = None\n",
    "\n",
    "# Pr√ºfe PLZ-Verf√ºgbarkeit im kombinierten Dataset\n",
    "if 'plz' in combined_df.columns:\n",
    "    print(f\"\\n=== PLZ-VERF√úGBARKEIT IM KOMBINIERTEN DATASET ===\")\n",
    "    plz_available = combined_df['plz'].notna().sum()\n",
    "    print(f\"PLZ verf√ºgbar: {plz_available:,}/{len(combined_df):,} ({plz_available/len(combined_df)*100:.1f}%)\")\n",
    "    \n",
    "    # Zeige PLZ-Statistiken\n",
    "    unique_plz = combined_df['plz'].nunique()\n",
    "    print(f\"Einzigartige PLZ: {unique_plz}\")\n",
    "    \n",
    "    # Zeige h√§ufigste PLZ\n",
    "    plz_counts = combined_df['plz'].value_counts().head(5)\n",
    "    print(f\"H√§ufigste PLZ:\")\n",
    "    for plz, count in plz_counts.items():\n",
    "        print(f\"  {plz}: {count:,} Eintr√§ge\")\n",
    "    \n",
    "    # DATENTYP-PROBLEM BEHEBEN\n",
    "    print(f\"\\n=== DATENTYP-PROBLEM BEHEBEN ===\")\n",
    "    print(f\"PLZ-Datentyp im combined_df: {combined_df['plz'].dtype}\")\n",
    "    print(f\"PLZ-Datentyp im mapping: {plz_mapping['PLZ'].dtype}\")\n",
    "    \n",
    "    # Zeige ein paar Beispiel-PLZ aus combined_df\n",
    "    sample_plz = combined_df['plz'].dropna().head(5).tolist()\n",
    "    print(f\"Beispiel PLZ aus combined_df: {sample_plz}\")\n",
    "    \n",
    "    # Zeige ein paar Beispiel-PLZ aus mapping\n",
    "    sample_mapping_plz = plz_mapping['PLZ'].head(5).tolist()\n",
    "    print(f\"Beispiel PLZ aus mapping: {sample_mapping_plz}\")\n",
    "else:\n",
    "    print(\"‚ùå Keine PLZ-Spalte im kombinierten Dataset gefunden!\")\n",
    "\n",
    "# F√ºhre den Join durch (wenn m√∂glich)\n",
    "if plz_mapping is not None and 'plz' in combined_df.columns:\n",
    "    print(f\"\\n=== PLZ-MAPPING-JOIN DURCHF√úHREN ===\")\n",
    "    \n",
    "    # Bereite die Daten f√ºr den Join vor - KORRIGIERE DATENTYPEN\n",
    "    # Konvertiere Float-PLZ zu String und entferne .0\n",
    "    def clean_plz(plz_value):\n",
    "        if pd.isna(plz_value):\n",
    "            return None\n",
    "        # Konvertiere zu String und entferne .0 wenn es ein Float ist\n",
    "        plz_str = str(plz_value)\n",
    "        if plz_str.endswith('.0'):\n",
    "            plz_str = plz_str[:-2]\n",
    "        return plz_str\n",
    "    \n",
    "    # Bereite PLZ-Spalten vor\n",
    "    combined_df['plz_clean'] = combined_df['plz'].apply(clean_plz)\n",
    "    plz_mapping['PLZ'] = plz_mapping['PLZ'].astype(str)\n",
    "    \n",
    "    print(f\"PLZ nach Bereinigung - Beispiele:\")\n",
    "    sample_clean_plz = combined_df['plz_clean'].dropna().head(5).tolist()\n",
    "    print(f\"  Combined_df: {sample_clean_plz}\")\n",
    "    print(f\"  Mapping: {plz_mapping['PLZ'].head(5).tolist()}\")\n",
    "    \n",
    "    # Anzahl vor dem Join\n",
    "    rows_before = len(combined_df)\n",
    "    \n",
    "    # Join durchf√ºhren\n",
    "    combined_enhanced = combined_df.merge(\n",
    "        plz_mapping, \n",
    "        left_on='plz_clean', \n",
    "        right_on='PLZ', \n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    # Bereinige Spalten\n",
    "    combined_enhanced = combined_enhanced.drop(['PLZ', 'plz_clean'], axis=1)  # Duplikate entfernen\n",
    "    combined_enhanced = combined_enhanced.rename(columns={\n",
    "        'Ortsteil': 'ortsteil',\n",
    "        'Bezirk': 'bezirk', \n",
    "        'Lat': 'lat',\n",
    "        'Lon': 'lon'\n",
    "    })\n",
    "    \n",
    "    # Statistiken nach dem Join\n",
    "    rows_after = len(combined_enhanced)\n",
    "    matched_coords = combined_enhanced['lat'].notna().sum()\n",
    "    \n",
    "    print(f\"Join-Ergebnis:\")\n",
    "    print(f\"  Zeilen vorher: {rows_before:,}\")\n",
    "    print(f\"  Zeilen nachher: {rows_after:,}\")\n",
    "    print(f\"  Koordinaten matched: {matched_coords:,}/{rows_after:,} ({matched_coords/rows_after*100:.1f}%)\")\n",
    "    \n",
    "    # Zeige gematchte PLZ-Beispiele\n",
    "    if matched_coords > 0:\n",
    "        print(f\"\\n‚úÖ ERFOLGREICHE MATCHES (Top 5):\")\n",
    "        matched_data = combined_enhanced[combined_enhanced['lat'].notna()]\n",
    "        for _, row in matched_data.head(5).iterrows():\n",
    "            print(f\"  PLZ {row['plz']} ‚Üí {row['ortsteil']} ({row['bezirk']}) ‚Üí {row['lat']:.4f}, {row['lon']:.4f}\")\n",
    "    \n",
    "    # Zeige nicht gematchte PLZ\n",
    "    unmatched_plz = combined_enhanced[combined_enhanced['lat'].isna()]['plz'].value_counts().head(10)\n",
    "    if len(unmatched_plz) > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  Nicht gematchte PLZ (Top 10):\")\n",
    "        for plz, count in unmatched_plz.items():\n",
    "            if pd.notna(plz):\n",
    "                print(f\"  {plz}: {count:,} Eintr√§ge\")\n",
    "    \n",
    "    # Ersetze das combined_df durch das enhanced\n",
    "    combined_df = combined_enhanced\n",
    "    \n",
    "    print(f\"\\n‚úÖ Erweiterte PLZ-Geolocation erfolgreich hinzugef√ºgt!\")\n",
    "    print(f\"Neue Spalten: ortsteil, bezirk, lat, lon\")\n",
    "    print(f\"Finale Spalten: {list(combined_df.columns)}\")\n",
    "else:\n",
    "    print(f\"\\n‚ùå PLZ-Mapping-Join nicht m√∂glich\")\n",
    "    print(\"Grund: Fehlende PLZ-Mapping-Datei oder keine PLZ-Spalte im Dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "648e53e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß PLZ-DATENTYP-REPARATUR\n",
      "==================================================\n",
      "Verf√ºgbare Spalten:\n",
      "  combined_df: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'wol', 'plz', 'ortsteil', 'bezirk', 'lat', 'lon']\n",
      "  plz_mapping: ['PLZ', 'Ortsteil', 'Bezirk', 'Lat', 'Lon']\n",
      "\n",
      "PLZ-Datentypen vor Reparatur:\n",
      "  combined_df['plz']: float64\n",
      "  plz_mapping['PLZ']: object\n",
      "\n",
      "PLZ-Beispiele vor Reparatur:\n",
      "  combined_df PLZ:\n",
      "    13591.0 (Type: <class 'float'>)\n",
      "    12527.0 (Type: <class 'float'>)\n",
      "    13053.0 (Type: <class 'float'>)\n",
      "    13158.0 (Type: <class 'float'>)\n",
      "    14199.0 (Type: <class 'float'>)\n",
      "  plz_mapping PLZ:\n",
      "    10115 (Type: <class 'str'>)\n",
      "    10117 (Type: <class 'str'>)\n",
      "    10119 (Type: <class 'str'>)\n",
      "    10178 (Type: <class 'str'>)\n",
      "    10179 (Type: <class 'str'>)\n",
      "\n",
      "üö® KRITISCHE REPARATUR: PLZ-DATENTYP-HARMONISIERUNG\n",
      "============================================================\n",
      "\n",
      "PLZ-Datentypen nach Reparatur:\n",
      "  combined_df['plz']: object\n",
      "  plz_mapping['PLZ']: object\n",
      "\n",
      "PLZ-Beispiele nach Reparatur:\n",
      "  combined_df PLZ:\n",
      "    13591 (Type: <class 'str'>)\n",
      "    12527 (Type: <class 'str'>)\n",
      "    13053 (Type: <class 'str'>)\n",
      "    13158 (Type: <class 'str'>)\n",
      "    14199 (Type: <class 'str'>)\n",
      "  plz_mapping PLZ:\n",
      "    10115 (Type: <class 'str'>)\n",
      "    10117 (Type: <class 'str'>)\n",
      "    10119 (Type: <class 'str'>)\n",
      "    10178 (Type: <class 'str'>)\n",
      "    10179 (Type: <class 'str'>)\n",
      "\n",
      "üîç PLZ-MATCH-VALIDIERUNG\n",
      "========================================\n",
      "PLZ in combined_df: 188 eindeutige Werte\n",
      "PLZ in plz_mapping: 190 eindeutige Werte\n",
      "√úberschneidung: 186 PLZ k√∂nnen gematched werden\n",
      "‚úÖ PLZ-JOIN WIRD FUNKTIONIEREN!\n",
      "Beispiel-Matches:\n",
      "  12043 ist in beiden Datasets vorhanden\n",
      "  12437 ist in beiden Datasets vorhanden\n",
      "  10829 ist in beiden Datasets vorhanden\n",
      "  14059 ist in beiden Datasets vorhanden\n",
      "  10717 ist in beiden Datasets vorhanden\n",
      "\n",
      "‚úÖ PLZ-Datentyp-Reparatur abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# PLZ-DATENTYP-REPARATUR VOR JOIN\n",
    "# ===================================================================\n",
    "print(\"\\nüîß PLZ-DATENTYP-REPARATUR\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Pr√ºfe die verf√ºgbaren Spalten\n",
    "print(\"Verf√ºgbare Spalten:\")\n",
    "print(f\"  combined_df: {list(combined_df.columns)}\")\n",
    "print(f\"  plz_mapping: {list(plz_mapping.columns)}\")\n",
    "\n",
    "# Pr√ºfe PLZ-Datentypen vor der Reparatur\n",
    "print(\"\\nPLZ-Datentypen vor Reparatur:\")\n",
    "print(f\"  combined_df['plz']: {combined_df['plz'].dtype}\")\n",
    "\n",
    "# Identifiziere die PLZ-Spalte in plz_mapping\n",
    "if 'plz' in plz_mapping.columns:\n",
    "    plz_col = 'plz'\n",
    "elif 'PLZ' in plz_mapping.columns:\n",
    "    plz_col = 'PLZ'\n",
    "else:\n",
    "    print(\"‚ùå Keine PLZ-Spalte in plz_mapping gefunden!\")\n",
    "    print(f\"Verf√ºgbare Spalten: {list(plz_mapping.columns)}\")\n",
    "    plz_col = None\n",
    "\n",
    "if plz_col:\n",
    "    print(f\"  plz_mapping['{plz_col}']: {plz_mapping[plz_col].dtype}\")\n",
    "    \n",
    "    # Zeige PLZ-Beispiele vor Reparatur\n",
    "    print(\"\\nPLZ-Beispiele vor Reparatur:\")\n",
    "    print(\"  combined_df PLZ:\")\n",
    "    combined_plz_sample = combined_df['plz'].dropna().head(5)\n",
    "    for plz in combined_plz_sample:\n",
    "        print(f\"    {plz} (Type: {type(plz)})\")\n",
    "\n",
    "    print(f\"  plz_mapping {plz_col}:\")\n",
    "    mapping_plz_sample = plz_mapping[plz_col].head(5)\n",
    "    for plz in mapping_plz_sample:\n",
    "        print(f\"    {plz} (Type: {type(plz)})\")\n",
    "\n",
    "    # ===================================================================\n",
    "    # KRITISCHE REPARATUR: PLZ-DATENTYP-HARMONISIERUNG\n",
    "    # ===================================================================\n",
    "    print(\"\\nüö® KRITISCHE REPARATUR: PLZ-DATENTYP-HARMONISIERUNG\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # PROBLEM: combined_df['plz'] ist float (mit .0), plz_mapping['plz'] ist string\n",
    "    # L√ñSUNG: Beide zu String konvertieren\n",
    "\n",
    "    # Repariere combined_df PLZ: float ‚Üí string\n",
    "    combined_df['plz'] = combined_df['plz'].apply(\n",
    "        lambda x: str(int(x)) if pd.notna(x) and x != '' else None\n",
    "    )\n",
    "\n",
    "    # Repariere plz_mapping PLZ: Stelle sicher, dass es string ist\n",
    "    plz_mapping[plz_col] = plz_mapping[plz_col].astype(str)\n",
    "\n",
    "    # Pr√ºfe PLZ-Datentypen nach der Reparatur\n",
    "    print(\"\\nPLZ-Datentypen nach Reparatur:\")\n",
    "    print(f\"  combined_df['plz']: {combined_df['plz'].dtype}\")\n",
    "    print(f\"  plz_mapping['{plz_col}']: {plz_mapping[plz_col].dtype}\")\n",
    "\n",
    "    # Zeige PLZ-Beispiele nach Reparatur\n",
    "    print(\"\\nPLZ-Beispiele nach Reparatur:\")\n",
    "    print(\"  combined_df PLZ:\")\n",
    "    combined_plz_sample_after = combined_df['plz'].dropna().head(5)\n",
    "    for plz in combined_plz_sample_after:\n",
    "        print(f\"    {plz} (Type: {type(plz)})\")\n",
    "\n",
    "    print(f\"  plz_mapping {plz_col}:\")\n",
    "    mapping_plz_sample_after = plz_mapping[plz_col].head(5)\n",
    "    for plz in mapping_plz_sample_after:\n",
    "        print(f\"    {plz} (Type: {type(plz)})\")\n",
    "\n",
    "    # Validiere, dass PLZ-Werte jetzt matchbar sind\n",
    "    print(\"\\nüîç PLZ-MATCH-VALIDIERUNG\")\n",
    "    print(\"=\" * 40)\n",
    "\n",
    "    # Pr√ºfe √úberschneidungen\n",
    "    combined_plz_unique = set(combined_df['plz'].dropna().unique())\n",
    "    mapping_plz_unique = set(plz_mapping[plz_col].unique())\n",
    "\n",
    "    overlap = combined_plz_unique.intersection(mapping_plz_unique)\n",
    "    print(f\"PLZ in combined_df: {len(combined_plz_unique):,} eindeutige Werte\")\n",
    "    print(f\"PLZ in plz_mapping: {len(mapping_plz_unique):,} eindeutige Werte\")\n",
    "    print(f\"√úberschneidung: {len(overlap):,} PLZ k√∂nnen gematched werden\")\n",
    "\n",
    "    if len(overlap) > 0:\n",
    "        print(f\"‚úÖ PLZ-JOIN WIRD FUNKTIONIEREN!\")\n",
    "        print(f\"Beispiel-Matches:\")\n",
    "        for plz in list(overlap)[:5]:\n",
    "            print(f\"  {plz} ist in beiden Datasets vorhanden\")\n",
    "    else:\n",
    "        print(f\"‚ùå PLZ-JOIN WIRD FEHLSCHLAGEN!\")\n",
    "        print(\"Grund: Keine √úberschneidung zwischen den PLZ-Werten\")\n",
    "        \n",
    "        # Zeige PLZ-Beispiele zum Debugging\n",
    "        print(\"\\nPLZ-Beispiele f√ºr Debugging:\")\n",
    "        print(\"  combined_df PLZ (erste 10):\")\n",
    "        for plz in list(combined_plz_unique)[:10]:\n",
    "            print(f\"    '{plz}'\")\n",
    "        print(\"  plz_mapping PLZ (erste 10):\")\n",
    "        for plz in list(mapping_plz_unique)[:10]:\n",
    "            print(f\"    '{plz}'\")\n",
    "\n",
    "    print(f\"\\n‚úÖ PLZ-Datentyp-Reparatur abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81123c9",
   "metadata": {},
   "source": [
    "## 6. Finale Datenvalidierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c95a414f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FINALE DATENVALIDIERUNG\n",
      "============================================================\n",
      "Duplikate: 1,232 (7.05%)\n",
      "\n",
      "=== FEHLENDE WERTE ===\n",
      "  rooms: 4,424 (25.3%)\n",
      "  wol: 12,997 (74.3%)\n",
      "  plz: 12,995 (74.3%)\n",
      "  ortsteil: 12,997 (74.3%)\n",
      "  bezirk: 12,997 (74.3%)\n",
      "  lat: 13,112 (75.0%)\n",
      "  lon: 13,112 (75.0%)\n",
      "\n",
      "=== GEOLOCATION-QUALIT√ÑT ===\n",
      "Vollst√§ndige Koordinaten: 4,375/17,487 (25.0%)\n",
      "Koordinaten in Berlin-Bounds: 4,375/4,375 (100.0%)\n",
      "\n",
      "=== ORTSTEIL-VERTEILUNG ===\n",
      "Anzahl Ortsteile: 78\n",
      "Top 10 Ortsteile:\n",
      "  Friedrichshain: 241 Eintr√§ge (1.4%)\n",
      "  Charlottenburg: 220 Eintr√§ge (1.3%)\n",
      "  Hellersdorf: 189 Eintr√§ge (1.1%)\n",
      "  Hakenfelde: 157 Eintr√§ge (0.9%)\n",
      "  Mitte: 146 Eintr√§ge (0.8%)\n",
      "  Wilhelmstadt: 136 Eintr√§ge (0.8%)\n",
      "  Friedrichsfelde: 136 Eintr√§ge (0.8%)\n",
      "  Franz√∂sisch Buchholz: 135 Eintr√§ge (0.8%)\n",
      "  Neuk√∂lln: 133 Eintr√§ge (0.8%)\n",
      "  Moabit: 133 Eintr√§ge (0.8%)\n",
      "\n",
      "=== STATISTIKEN KOMBINIERTES DATASET ===\n",
      "Preis - Min: 150‚Ç¨, Max: 9990‚Ç¨, Median: 931‚Ç¨\n",
      "Gr√∂√üe - Min: 10m¬≤, Max: 482m¬≤, Median: 69m¬≤\n",
      "Zimmer - Min: 1.0, Max: 10.0, Median: 2.0\n",
      "\n",
      "=== BEZIRKSVERTEILUNG (ORIGINAL) ===\n",
      "Anzahl Bezirke: 87\n",
      "  Mitte: 1,923 Eintr√§ge (11.0%)\n",
      "  Pankow: 1,189 Eintr√§ge (6.8%)\n",
      "  Neuk√∂lln: 904 Eintr√§ge (5.2%)\n",
      "  Spandau: 837 Eintr√§ge (4.8%)\n",
      "  Tiergarten: 832 Eintr√§ge (4.8%)\n",
      "  Charlottenburg: 792 Eintr√§ge (4.5%)\n",
      "  Friedrichshain-Kreuzberg: 775 Eintr√§ge (4.4%)\n",
      "  Friedrichshain: 666 Eintr√§ge (3.8%)\n",
      "  Charlottenburg-Wilmersdorf: 636 Eintr√§ge (3.6%)\n",
      "  Reinickendorf: 614 Eintr√§ge (3.5%)\n",
      "\n",
      "=== BEZIRKSVERTEILUNG (PLZ-ENHANCED) ===\n",
      "Anzahl Bezirke: 13\n",
      "  Spandau: 595 Eintr√§ge (3.4%)\n",
      "  Treptow-K√∂penick: 429 Eintr√§ge (2.5%)\n",
      "  Mitte: 415 Eintr√§ge (2.4%)\n",
      "  Reinickendorf: 407 Eintr√§ge (2.3%)\n",
      "  Charlottenburg-Wilmersdorf: 388 Eintr√§ge (2.2%)\n",
      "  Marzahn-Hellersdorf: 349 Eintr√§ge (2.0%)\n",
      "  Lichtenberg: 348 Eintr√§ge (2.0%)\n",
      "  Pankow: 341 Eintr√§ge (2.0%)\n",
      "  Tempelhof-Sch√∂neberg: 340 Eintr√§ge (1.9%)\n",
      "  Friedrichshain-Kreuzberg: 316 Eintr√§ge (1.8%)\n",
      "\n",
      "‚úÖ Finale Datenvalidierung abgeschlossen\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"FINALE DATENVALIDIERUNG\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Duplikate pr√ºfen\n",
    "duplicates = combined_df.duplicated().sum()\n",
    "print(f\"Duplikate: {duplicates:,} ({duplicates/len(combined_df)*100:.2f}%)\")\n",
    "\n",
    "# Fehlende Werte\n",
    "print(f\"\\n=== FEHLENDE WERTE ===\")\n",
    "missing_summary = combined_df.isnull().sum()\n",
    "for col, missing_count in missing_summary.items():\n",
    "    if missing_count > 0:\n",
    "        print(f\"  {col}: {missing_count:,} ({missing_count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "# Geolocation-Qualit√§t pr√ºfen\n",
    "if 'lat' in combined_df.columns and 'lon' in combined_df.columns:\n",
    "    print(f\"\\n=== GEOLOCATION-QUALIT√ÑT ===\")\n",
    "    coords_available = combined_df[['lat', 'lon']].notna().all(axis=1).sum()\n",
    "    print(f\"Vollst√§ndige Koordinaten: {coords_available:,}/{len(combined_df):,} ({coords_available/len(combined_df)*100:.1f}%)\")\n",
    "    \n",
    "    if coords_available > 0:\n",
    "        # Koordinaten-Plausibilit√§t pr√ºfen (Berlin bounds)\n",
    "        berlin_bounds = {\n",
    "            'lat_min': 52.3, 'lat_max': 52.7,\n",
    "            'lon_min': 13.0, 'lon_max': 13.8\n",
    "        }\n",
    "        \n",
    "        valid_coords = combined_df[\n",
    "            (combined_df['lat'] >= berlin_bounds['lat_min']) & \n",
    "            (combined_df['lat'] <= berlin_bounds['lat_max']) & \n",
    "            (combined_df['lon'] >= berlin_bounds['lon_min']) & \n",
    "            (combined_df['lon'] <= berlin_bounds['lon_max'])\n",
    "        ]\n",
    "        \n",
    "        print(f\"Koordinaten in Berlin-Bounds: {len(valid_coords):,}/{coords_available:,} ({len(valid_coords)/coords_available*100:.1f}%)\")\n",
    "\n",
    "# Ortsteil-Verteilung\n",
    "if 'ortsteil' in combined_df.columns:\n",
    "    print(f\"\\n=== ORTSTEIL-VERTEILUNG ===\")\n",
    "    ortsteil_counts = combined_df['ortsteil'].value_counts()\n",
    "    print(f\"Anzahl Ortsteile: {len(ortsteil_counts)}\")\n",
    "    print(f\"Top 10 Ortsteile:\")\n",
    "    for ortsteil, count in ortsteil_counts.head(10).items():\n",
    "        print(f\"  {ortsteil}: {count:,} Eintr√§ge ({count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "# Statistiken der Kernfelder\n",
    "print(f\"\\n=== STATISTIKEN KOMBINIERTES DATASET ===\")\n",
    "if 'price' in combined_df.columns:\n",
    "    price_stats = combined_df['price'].describe()\n",
    "    print(f\"Preis - Min: {price_stats['min']:.0f}‚Ç¨, Max: {price_stats['max']:.0f}‚Ç¨, Median: {price_stats['50%']:.0f}‚Ç¨\")\n",
    "\n",
    "if 'size' in combined_df.columns:\n",
    "    size_stats = combined_df['size'].describe()\n",
    "    print(f\"Gr√∂√üe - Min: {size_stats['min']:.0f}m¬≤, Max: {size_stats['max']:.0f}m¬≤, Median: {size_stats['50%']:.0f}m¬≤\")\n",
    "\n",
    "if 'rooms' in combined_df.columns:\n",
    "    rooms_stats = combined_df['rooms'].describe()\n",
    "    print(f\"Zimmer - Min: {rooms_stats['min']:.1f}, Max: {rooms_stats['max']:.1f}, Median: {rooms_stats['50%']:.1f}\")\n",
    "\n",
    "# Bezirksverteilung (alt vs neu)\n",
    "if 'district' in combined_df.columns:\n",
    "    print(f\"\\n=== BEZIRKSVERTEILUNG (ORIGINAL) ===\")\n",
    "    district_counts = combined_df['district'].value_counts()\n",
    "    print(f\"Anzahl Bezirke: {len(district_counts)}\")\n",
    "    for district, count in district_counts.head(10).items():\n",
    "        print(f\"  {district}: {count:,} Eintr√§ge ({count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "if 'bezirk' in combined_df.columns:\n",
    "    print(f\"\\n=== BEZIRKSVERTEILUNG (PLZ-ENHANCED) ===\")\n",
    "    bezirk_counts = combined_df['bezirk'].value_counts()\n",
    "    print(f\"Anzahl Bezirke: {len(bezirk_counts)}\")\n",
    "    for bezirk, count in bezirk_counts.head(10).items():\n",
    "        print(f\"  {bezirk}: {count:,} Eintr√§ge ({count/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Finale Datenvalidierung abgeschlossen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4279a7",
   "metadata": {},
   "source": [
    "## 7. Export des finalen Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13c3b62b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT FINALES KOMBINIERTES DATASET\n",
      "============================================================\n",
      "‚úÖ Finales Dataset exportiert: data/processed/berlin_housing_combined_enriched_final.csv\n",
      "Dateigr√∂√üe: 17,487 Zeilen x 13 Spalten\n",
      "‚úÖ Export-Validierung erfolgreich: 17,487 Zeilen geladen\n",
      "\n",
      "=== FINALE ZUSAMMENFASSUNG ===\n",
      "Input-Datasets: 3\n",
      "  - Dataset 2018-2019: 10,387 Zeilen\n",
      "  - Dataset 2022: 2,676 Zeilen\n",
      "  - Dataset 2025: 4,424 Zeilen\n",
      "Output: data/processed/berlin_housing_combined_enriched_final.csv (17,487 Zeilen)\n",
      "Zeitspanne: 2018-2025 (3 Jahre)\n",
      "Berliner Bezirke (PLZ-Enhanced): 14 abgedeckt\n",
      "Berliner Ortsteile: 79 abgedeckt\n",
      "Geolocation-Koordinaten: 4,375/17,487 (25.0%)\n",
      "Standardisierte Spalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'wol', 'plz', 'ortsteil', 'bezirk', 'lat', 'lon']\n",
      "\n",
      "üéØ VERBESSERUNGEN DURCH PLZ-ENHANCEMENT:\n",
      "üìç Ortsteil-Level-Genauigkeit statt nur Bezirks-Level\n",
      "üó∫Ô∏è  Echte Koordinaten f√ºr pr√§zise Geolocation\n",
      "üìä Beispiel: PLZ 12355 ‚Üí Rudow (Neuk√∂lln) statt nur 'Neuk√∂lln'\n",
      "üé® Bereit f√ºr hochaufl√∂sende Karten und Visualisierungen\n",
      "\n",
      "üéâ DATASET-KOMBINATION ERFOLGREICH ABGESCHLOSSEN!\n",
      "Das finale Dataset ist bereit f√ºr die **AWESOME** Mietpreis-Analyse mit pr√§ziser Geolocation.\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPORT FINALES KOMBINIERTES DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Export\n",
    "output_file = 'data/processed/berlin_housing_combined_enriched_final.csv'\n",
    "combined_df.to_csv(output_file, index=False)\n",
    "\n",
    "print(f\"‚úÖ Finales Dataset exportiert: {output_file}\")\n",
    "print(f\"Dateigr√∂√üe: {len(combined_df):,} Zeilen x {len(combined_df.columns)} Spalten\")\n",
    "\n",
    "# Validierung durch Wiedereinlesen\n",
    "test_df = pd.read_csv(output_file)\n",
    "print(f\"‚úÖ Export-Validierung erfolgreich: {len(test_df):,} Zeilen geladen\")\n",
    "\n",
    "# Finale Zusammenfassung\n",
    "print(f\"\\n=== FINALE ZUSAMMENFASSUNG ===\")\n",
    "print(f\"Input-Datasets: {len(datasets)}\")\n",
    "print(f\"  - Dataset 2018-2019: {quality_report.get('2018_2019', {}).get('total_rows', 0):,} Zeilen\")\n",
    "print(f\"  - Dataset 2022: {quality_report.get('2022', {}).get('total_rows', 0):,} Zeilen\") \n",
    "print(f\"  - Dataset 2025: {quality_report.get('2025', {}).get('total_rows', 0):,} Zeilen\")\n",
    "print(f\"Output: {output_file} ({len(combined_df):,} Zeilen)\")\n",
    "print(f\"Zeitspanne: 2018-2025 ({len(combined_df['year'].unique())} Jahre)\")\n",
    "\n",
    "# Geolocation-Zusammenfassung\n",
    "if 'bezirk' in combined_df.columns:\n",
    "    print(f\"Berliner Bezirke (PLZ-Enhanced): {len(combined_df['bezirk'].unique())} abgedeckt\")\n",
    "else:\n",
    "    print(f\"Berliner Bezirke: {len(combined_df['district'].unique())} abgedeckt\")\n",
    "\n",
    "if 'ortsteil' in combined_df.columns:\n",
    "    print(f\"Berliner Ortsteile: {len(combined_df['ortsteil'].unique())} abgedeckt\")\n",
    "\n",
    "if 'lat' in combined_df.columns and 'lon' in combined_df.columns:\n",
    "    coords_available = combined_df[['lat', 'lon']].notna().all(axis=1).sum()\n",
    "    print(f\"Geolocation-Koordinaten: {coords_available:,}/{len(combined_df):,} ({coords_available/len(combined_df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"Standardisierte Spalten: {list(combined_df.columns)}\")\n",
    "\n",
    "# Verbesserungen hervorheben\n",
    "print(f\"\\nüéØ VERBESSERUNGEN DURCH PLZ-ENHANCEMENT:\")\n",
    "print(f\"üìç Ortsteil-Level-Genauigkeit statt nur Bezirks-Level\")\n",
    "print(f\"üó∫Ô∏è  Echte Koordinaten f√ºr pr√§zise Geolocation\")\n",
    "print(f\"üìä Beispiel: PLZ 12355 ‚Üí Rudow (Neuk√∂lln) statt nur 'Neuk√∂lln'\")\n",
    "print(f\"üé® Bereit f√ºr hochaufl√∂sende Karten und Visualisierungen\")\n",
    "\n",
    "print(f\"\\nüéâ DATASET-KOMBINATION ERFOLGREICH ABGESCHLOSSEN!\")\n",
    "print(f\"Das finale Dataset ist bereit f√ºr die **AWESOME** Mietpreis-Analyse mit pr√§ziser Geolocation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
