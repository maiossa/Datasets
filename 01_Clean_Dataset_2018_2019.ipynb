{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a70f42e6",
   "metadata": {},
   "source": [
    "# Dataset 2018-2019 Bereinigung und Normalisierung\n",
    "## Spezialisiertes Modul f√ºr Kaggle/Immobilienscout24 Dataset\n",
    "\n",
    "### Ziel\n",
    "Bereinigung und Normalisierung des historischen Datasets (2018-2019) in ein standardisiertes Format f√ºr die gemeinsame Analyse.\n",
    "\n",
    "### Input\n",
    "- `data/raw/Dataset_2018_2019.csv`\n",
    "\n",
    "### Output\n",
    "- `data/processed/dataset_2018_2019_normalized.csv`\n",
    "\n",
    "### Standardisierte Ausgabespalten\n",
    "- `price`: Normalisierter Preis (Kaltmiete in ‚Ç¨)\n",
    "- `size`: Normalisierte Gr√∂√üe (m¬≤)\n",
    "- `district`: Berliner Bezirk (standardisiert)\n",
    "- `rooms`: Anzahl Zimmer\n",
    "- `year`: Jahr des Datasets (2019)\n",
    "- `dataset_id`: Eindeutige Dataset-Kennzeichnung (historical)\n",
    "- `source`: Datenquelle\n",
    "\n",
    "---\n",
    "**Teil der modularen Preprocessing-Pipeline**  \n",
    "**Datum:** 4. Juli 2025  \n",
    "**Version:** 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ef7a1",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3a55584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bibliotheken erfolgreich importiert!\n",
      "Pandas Version: 2.3.0\n",
      "Dataset: 2018-2019 (Kaggle/Immobilienscout24)\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display configuration\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "\n",
    "print(\"Bibliotheken erfolgreich importiert!\")\n",
    "print(f\"Pandas Version: {pd.__version__}\")\n",
    "print(f\"Dataset: 2018-2019 (Kaggle/Immobilienscout24)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e7e8f",
   "metadata": {},
   "source": [
    "## 2. Daten laden und erste Analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee850b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DATASET 2018-2019 LADEN UND ANALYSIEREN\n",
      "============================================================\n",
      "Dataset geladen: 10,406 Zeilen, 9 Spalten\n",
      "\n",
      "Spalten: ['regio3', 'street', 'livingSpace', 'baseRent', 'totalRent', 'noRooms', 'floor', 'typeOfFlat', 'yearConstructed']\n",
      "\n",
      "Datentypen:\n",
      "regio3              object\n",
      "street              object\n",
      "livingSpace        float64\n",
      "baseRent           float64\n",
      "totalRent          float64\n",
      "noRooms            float64\n",
      "floor              float64\n",
      "typeOfFlat          object\n",
      "yearConstructed    float64\n",
      "dtype: object\n",
      "\n",
      "Fehlende Werte:\n",
      "  totalRent: 662 (6.36%)\n",
      "  floor: 1100 (10.57%)\n",
      "  typeOfFlat: 804 (7.73%)\n",
      "  yearConstructed: 1425 (13.69%)\n",
      "\n",
      "Erste 5 Zeilen:\n",
      "            regio3                      street  livingSpace  baseRent  totalRent  noRooms  floor    typeOfFlat  yearConstructed\n",
      "0  Staaken_Spandau           Metropolitan Park        77.00    820.00    1140.00      3.0    0.0  ground_floor              NaN\n",
      "1        Wei√üensee      B&ouml;rnestra&szlig;e        62.63    808.00     955.00      2.0    0.0  ground_floor           1918.0\n",
      "2            Mitte  Stallschreiberstra&szlig;e        46.40   1150.00    1300.00      2.0    3.0     apartment           2019.0\n",
      "3        Kreuzberg      Hallesche Stra&szlig;e        67.00   1200.00    1428.78      2.5    6.0     apartment           2017.0\n",
      "4       Tiergarten           Heidestra&szlig;e        73.54   1338.43    1559.05      2.0    0.0  ground_floor           2019.0\n"
     ]
    }
   ],
   "source": [
    "# Lade Dataset 2018-2019\n",
    "print(\"=\" * 60)\n",
    "print(\"DATASET 2018-2019 LADEN UND ANALYSIEREN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Lade Rohdaten\n",
    "df_raw = pd.read_csv('data/raw/Dataset_2018_2019.csv')\n",
    "print(f\"Dataset geladen: {df_raw.shape[0]:,} Zeilen, {df_raw.shape[1]} Spalten\")\n",
    "\n",
    "# Grundlegende Informationen\n",
    "print(f\"\\nSpalten: {list(df_raw.columns)}\")\n",
    "print(f\"\\nDatentypen:\")\n",
    "print(df_raw.dtypes)\n",
    "\n",
    "# Fehlende Werte\n",
    "print(f\"\\nFehlende Werte:\")\n",
    "missing_values = df_raw.isnull().sum()\n",
    "missing_pct = (missing_values / len(df_raw) * 100).round(2)\n",
    "for col in missing_values[missing_values > 0].index:\n",
    "    print(f\"  {col}: {missing_values[col]} ({missing_pct[col]}%)\")\n",
    "\n",
    "# Erste 5 Zeilen\n",
    "print(f\"\\nErste 5 Zeilen:\")\n",
    "print(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fadae8",
   "metadata": {},
   "source": [
    "## 3. Spezifische Bereinigung Dataset 2018-2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f6ec445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SPEZIFISCHE BEREINIGUNG DATASET 2018-2019\n",
      "============================================================\n",
      "Arbeitskopie erstellt: 10406 Zeilen\n",
      "\n",
      "=== PREIS-BEREINIGUNG ===\n",
      "baseRent - Statistik vor Bereinigung:\n",
      "  Typ: float64\n",
      "  Nicht-null Werte: 10406\n",
      "  Min: 0.0, Max: 20000.0\n",
      "Entfernte unrealistische Preise: 11\n",
      "\n",
      "=== GR√ñSSEN-BEREINIGUNG ===\n",
      "livingSpace - Statistik vor Bereinigung:\n",
      "  Typ: float64\n",
      "  Nicht-null Werte: 10395\n",
      "  Min: 3.0, Max: 542.53\n",
      "Entfernte unrealistische Gr√∂√üen: 7\n",
      "\n",
      "=== BEZIRKS-NORMALISIERUNG ===\n",
      "regio3 - Einzigartige Werte: 79\n",
      "Bezirke: ['Adlershof_Treptow', 'Alt_Hohensch√∂nhausen_Hohensch√∂nhausen', 'Altglienicke_Treptow', 'Baumschulenweg_Treptow', 'Biesdorf_Marzahn', 'Blankenburg_Wei√üensee', 'Bohnsdorf_Treptow', 'Britz_Neuk√∂lln', 'Buch_Pankow', 'Buckow_Neuk√∂lln', 'Charlottenburg', 'Dahlem_Zehlendorf', 'Falkenberg_Hohensch√∂nhausen', 'Franz√∂sisch_Buchholz_Pankow', 'Friedenau_Sch√∂neberg', 'Friedrichsfelde_Lichtenberg', 'Friedrichshagen_K√∂penick', 'Friedrichshain', 'Frohnau_Reinickendorf', 'Gatow_Spandau', 'Grunewald_Wilmersdorf', 'Gr√ºnau_K√∂penick', 'Haselhorst_Spandau', 'Heiligensee_Reinickendorf', 'Heinersdorf_Wei√üensee', 'Hellersdorf', 'Hermsdorf_Reinickendorf', 'Johannisthal_Treptow', 'Karlshorst_Lichtenberg', 'Karow_Wei√üensee', 'Kaulsdorf_Hellersdorf', 'Kladow_Spandau', 'Konradsh√∂he_Reinickendorf', 'Kreuzberg', 'K√∂penick', 'Lankwitz_Steglitz', 'Lichtenberg', 'Lichtenrade_Tempelhof', 'Lichterfelde_Steglitz', 'L√ºbars_Reinickendorf', 'Mahlsdorf_Hellersdorf', 'Malchow_Hohensch√∂nhausen', 'Mariendorf_Tempelhof', 'Marienfelde_Tempelhof', 'Marzahn', 'Mitte', 'M√ºggelheim_K√∂penick', 'Neu_Hohensch√∂nhausen_Hohensch√∂nhausen', 'Neuk√∂lln', 'Niedersch√∂neweide_Treptow', 'Niedersch√∂nhausen_Pankow', 'Nikolassee_Zehlendorf', 'Obersch√∂neweide_K√∂penick', 'Pankow', 'Pl√§nterwald_Treptow', 'Prenzlauer_Berg_Prenzlauer_Berg', 'Rahnsdorf_K√∂penick', 'Reinickendorf', 'Rosenthal_Pankow', 'Rudow_Neuk√∂lln', 'Rummelsburg_Lichtenberg', 'Schmargendorf_Wilmersdorf', 'Schm√∂ckwitz_K√∂penick', 'Sch√∂neberg', 'Siemensstadt_Spandau', 'Spandau', 'Staaken_Spandau', 'Steglitz', 'Tegel_Reinickendorf', 'Tempelhof', 'Tiergarten', 'Treptow', 'Waidmannslust_Reinickendorf', 'Wannsee_Zehlendorf', 'Wedding', 'Wei√üensee', 'Wilmersdorf', 'Wittenau_Reinickendorf', 'Zehlendorf']\n",
      "Normalisierte Bezirke: ['Adlershof', 'Alt', 'Altglienicke', 'Baumschulenweg', 'Biesdorf', 'Blankenburg', 'Bohnsdorf', 'Britz', 'Buch', 'Buckow', 'Charlottenburg', 'Dahlem', 'Falkenberg', 'Franz√∂sisch', 'Friedenau', 'Friedrichsfelde', 'Friedrichshagen', 'Friedrichshain', 'Frohnau', 'Gatow', 'Grunewald', 'Gr√ºnau', 'Haselhorst', 'Heiligensee', 'Heinersdorf', 'Hellersdorf', 'Hermsdorf', 'Johannisthal', 'Karlshorst', 'Karow', 'Kaulsdorf', 'Kladow', 'Konradsh√∂he', 'Kreuzberg', 'K√∂penick', 'Lankwitz', 'Lichtenberg', 'Lichtenrade', 'Lichterfelde', 'L√ºbars', 'Mahlsdorf', 'Malchow', 'Mariendorf', 'Marienfelde', 'Marzahn', 'Mitte', 'M√ºggelheim', 'Neu', 'Neuk√∂lln', 'Niedersch√∂neweide', 'Niedersch√∂nhausen', 'Nikolassee', 'Obersch√∂neweide', 'Pankow', 'Pl√§nterwald', 'Prenzlauer', 'Rahnsdorf', 'Reinickendorf', 'Rosenthal', 'Rudow', 'Rummelsburg', 'Schmargendorf', 'Schm√∂ckwitz', 'Sch√∂neberg', 'Siemensstadt', 'Spandau', 'Staaken', 'Steglitz', 'Tegel', 'Tempelhof', 'Tiergarten', 'Treptow', 'Waidmannslust', 'Wannsee', 'Wedding', 'Wei√üensee', 'Wilmersdorf', 'Wittenau', 'Zehlendorf']\n",
      "Anzahl normalisierte Bezirke: 79\n",
      "\n",
      "=== ZIMMER-BEREINIGUNG ===\n",
      "noRooms - Statistik:\n",
      "  Typ: float64\n",
      "  Nicht-null Werte: 10388\n",
      "  Einzigartige Werte: [np.float64(1.0), np.float64(1.1), np.float64(1.5), np.float64(2.0), np.float64(2.1), np.float64(2.2), np.float64(2.5), np.float64(3.0), np.float64(3.5), np.float64(4.0), np.float64(4.5), np.float64(5.0), np.float64(5.5), np.float64(6.0), np.float64(6.5), np.float64(7.0), np.float64(7.5), np.float64(8.0), np.float64(8.5), np.float64(9.0), np.float64(10.0), np.float64(11.0)]\n",
      "Entfernte unrealistische Zimmeranzahlen: 1\n",
      "Spezifische Bereinigung abgeschlossen\n",
      "Verbleibende Datens√§tze: 10387 (Verlust: 19)\n"
     ]
    }
   ],
   "source": [
    "# Spezifische Bereinigung f√ºr Dataset 2018-2019\n",
    "print(\"=\" * 60)\n",
    "print(\"SPEZIFISCHE BEREINIGUNG DATASET 2018-2019\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Erstelle Arbeitskopie\n",
    "df = df_raw.copy()\n",
    "print(f\"Arbeitskopie erstellt: {len(df)} Zeilen\")\n",
    "\n",
    "# 1. Preis-Bereinigung (baseRent)\n",
    "print(f\"\\n=== PREIS-BEREINIGUNG ===\")\n",
    "print(f\"baseRent - Statistik vor Bereinigung:\")\n",
    "print(f\"  Typ: {df['baseRent'].dtype}\")\n",
    "print(f\"  Nicht-null Werte: {df['baseRent'].notna().sum()}\")\n",
    "print(f\"  Min: {df['baseRent'].min()}, Max: {df['baseRent'].max()}\")\n",
    "\n",
    "# Preis ist bereits numerisch, nur Plausibilit√§tspr√ºfung\n",
    "# Entferne unrealistische Preise (< 100‚Ç¨ oder > 10.000‚Ç¨)\n",
    "original_count = len(df)\n",
    "df = df[(df['baseRent'] >= 100) & (df['baseRent'] <= 10000)]\n",
    "removed_price = original_count - len(df)\n",
    "print(f\"Entfernte unrealistische Preise: {removed_price}\")\n",
    "\n",
    "# 2. Gr√∂√üen-Bereinigung (livingSpace)\n",
    "print(f\"\\n=== GR√ñSSEN-BEREINIGUNG ===\")\n",
    "print(f\"livingSpace - Statistik vor Bereinigung:\")\n",
    "print(f\"  Typ: {df['livingSpace'].dtype}\")\n",
    "print(f\"  Nicht-null Werte: {df['livingSpace'].notna().sum()}\")\n",
    "print(f\"  Min: {df['livingSpace'].min()}, Max: {df['livingSpace'].max()}\")\n",
    "\n",
    "# Gr√∂√üe ist bereits numerisch, nur Plausibilit√§tspr√ºfung\n",
    "# Entferne unrealistische Gr√∂√üen (< 10m¬≤ oder > 500m¬≤)\n",
    "original_count = len(df)\n",
    "df = df[(df['livingSpace'] >= 10) & (df['livingSpace'] <= 500)]\n",
    "removed_size = original_count - len(df)\n",
    "print(f\"Entfernte unrealistische Gr√∂√üen: {removed_size}\")\n",
    "\n",
    "# 3. Bezirks-Normalisierung (regio3)\n",
    "print(f\"\\n=== BEZIRKS-NORMALISIERUNG ===\")\n",
    "print(f\"regio3 - Einzigartige Werte: {df['regio3'].nunique()}\")\n",
    "print(f\"Bezirke: {sorted(df['regio3'].unique())}\")\n",
    "\n",
    "# Bezirk-Normalisierung (entferne _Suffix)\n",
    "def normalize_district_2018_2019(district):\n",
    "    \"\"\"Normalisiert Bezirksnamen f√ºr Dataset 2018-2019\"\"\"\n",
    "    if pd.isna(district):\n",
    "        return None\n",
    "    \n",
    "    # Entferne Suffix nach Unterstrich\n",
    "    if '_' in str(district):\n",
    "        return str(district).split('_')[0]\n",
    "    \n",
    "    return str(district)\n",
    "\n",
    "df['district_normalized'] = df['regio3'].apply(normalize_district_2018_2019)\n",
    "\n",
    "print(f\"Normalisierte Bezirke: {sorted(df['district_normalized'].unique())}\")\n",
    "print(f\"Anzahl normalisierte Bezirke: {df['district_normalized'].nunique()}\")\n",
    "\n",
    "# 4. Zimmer-Bereinigung (noRooms)\n",
    "print(f\"\\n=== ZIMMER-BEREINIGUNG ===\")\n",
    "print(f\"noRooms - Statistik:\")\n",
    "print(f\"  Typ: {df['noRooms'].dtype}\")\n",
    "print(f\"  Nicht-null Werte: {df['noRooms'].notna().sum()}\")\n",
    "print(f\"  Einzigartige Werte: {sorted(df['noRooms'].dropna().unique())}\")\n",
    "\n",
    "# Zimmeranzahl ist bereits numerisch\n",
    "# Plausibilit√§tspr√ºfung (0.5 bis 10 Zimmer)\n",
    "original_count = len(df)\n",
    "df = df[(df['noRooms'] >= 0.5) & (df['noRooms'] <= 10)]\n",
    "removed_rooms = original_count - len(df)\n",
    "print(f\"Entfernte unrealistische Zimmeranzahlen: {removed_rooms}\")\n",
    "\n",
    "print(f\"Spezifische Bereinigung abgeschlossen\")\n",
    "print(f\"Verbleibende Datens√§tze: {len(df)} (Verlust: {len(df_raw) - len(df)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47813a15",
   "metadata": {},
   "source": [
    "## 4. Normalisierung in Standardformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86dc1e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "NORMALISIERUNG IN STANDARDFORMAT\n",
      "============================================================\n",
      "Normalisiertes Dataset erstellt: 10387 Zeilen\n",
      "Standardspalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source']\n",
      "Zus√§tzliche Spalten: ['street', 'floor', 'typeOfFlat', 'yearConstructed', 'totalRent']\n",
      "=== DATENQUALIT√ÑT NORMALISIERTES DATASET ===\n",
      "Zeilen mit Preis: 10387\n",
      "Zeilen mit Gr√∂√üe: 10387\n",
      "Zeilen mit Bezirk: 10387\n",
      "Zeilen mit Zimmeranzahl: 10387\n",
      "=== STATISTIKEN ===\n",
      "Preis - Min: 178.16‚Ç¨, Max: 9500.00‚Ç¨, Median: 945.00‚Ç¨\n",
      "Gr√∂√üe - Min: 10.0m¬≤, Max: 482.0m¬≤, Median: 72.0m¬≤\n",
      "Zimmer - Min: 1.0, Max: 10.0, Median: 2.0\n",
      "=== BEZIRKSVERTEILUNG ===\n",
      "Anzahl Bezirke: 79\n",
      "  Mitte: 799 Eintr√§ge\n",
      "  Tiergarten: 768 Eintr√§ge\n",
      "  Charlottenburg: 701 Eintr√§ge\n",
      "  Friedrichshain: 553 Eintr√§ge\n",
      "  Prenzlauer: 473 Eintr√§ge\n",
      "  Spandau: 415 Eintr√§ge\n",
      "  Wedding: 397 Eintr√§ge\n",
      "  Wilmersdorf: 370 Eintr√§ge\n",
      "  Neuk√∂lln: 361 Eintr√§ge\n",
      "  K√∂penick: 351 Eintr√§ge\n",
      "Normalisierung abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "# Normalisierung in Standardformat\n",
    "print(\"=\" * 60)\n",
    "print(\"NORMALISIERUNG IN STANDARDFORMAT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Erstelle normalisiertes Dataset mit Standardspalten\n",
    "df_normalized = pd.DataFrame()\n",
    "\n",
    "# Standardspalten zuweisen\n",
    "df_normalized['price'] = df['baseRent'].astype('float64')\n",
    "df_normalized['size'] = df['livingSpace'].astype('float64')\n",
    "df_normalized['district'] = df['district_normalized'].astype('string')\n",
    "df_normalized['rooms'] = df['noRooms'].astype('float64')\n",
    "df_normalized['year'] = 2019\n",
    "df_normalized['dataset_id'] = 'historical'\n",
    "df_normalized['source'] = 'Kaggle/Immobilienscout24'\n",
    "\n",
    "# Zus√§tzliche Spalten aus Original-Dataset beibehalten\n",
    "df_normalized['street'] = df['street']\n",
    "df_normalized['floor'] = df['floor']\n",
    "df_normalized['typeOfFlat'] = df['typeOfFlat']\n",
    "df_normalized['yearConstructed'] = df['yearConstructed']\n",
    "df_normalized['totalRent'] = df['totalRent']\n",
    "\n",
    "print(f\"Normalisiertes Dataset erstellt: {len(df_normalized)} Zeilen\")\n",
    "print(f\"Standardspalten: {['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source']}\")\n",
    "print(f\"Zus√§tzliche Spalten: {list(df_normalized.columns[7:])}\")\n",
    "\n",
    "# Datenqualit√§t pr√ºfen\n",
    "print(f\"=== DATENQUALIT√ÑT NORMALISIERTES DATASET ===\")\n",
    "print(f\"Zeilen mit Preis: {df_normalized['price'].notna().sum()}\")\n",
    "print(f\"Zeilen mit Gr√∂√üe: {df_normalized['size'].notna().sum()}\")\n",
    "print(f\"Zeilen mit Bezirk: {df_normalized['district'].notna().sum()}\")\n",
    "print(f\"Zeilen mit Zimmeranzahl: {df_normalized['rooms'].notna().sum()}\")\n",
    "\n",
    "# Statistiken\n",
    "print(f\"=== STATISTIKEN ===\")\n",
    "print(f\"Preis - Min: {df_normalized['price'].min():.2f}‚Ç¨, Max: {df_normalized['price'].max():.2f}‚Ç¨, Median: {df_normalized['price'].median():.2f}‚Ç¨\")\n",
    "print(f\"Gr√∂√üe - Min: {df_normalized['size'].min():.1f}m¬≤, Max: {df_normalized['size'].max():.1f}m¬≤, Median: {df_normalized['size'].median():.1f}m¬≤\")\n",
    "print(f\"Zimmer - Min: {df_normalized['rooms'].min():.1f}, Max: {df_normalized['rooms'].max():.1f}, Median: {df_normalized['rooms'].median():.1f}\")\n",
    "\n",
    "# Bezirksverteilung\n",
    "print(f\"=== BEZIRKSVERTEILUNG ===\")\n",
    "district_counts = df_normalized['district'].value_counts()\n",
    "print(f\"Anzahl Bezirke: {len(district_counts)}\")\n",
    "for district, count in district_counts.head(10).items():\n",
    "    print(f\"  {district}: {count} Eintr√§ge\")\n",
    "\n",
    "print(f\"Normalisierung abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7db94d3",
   "metadata": {},
   "source": [
    "## 5. Export des normalisierten Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c728c108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT NORMALISIERTES DATASET\n",
      "============================================================\n",
      "‚úÖ Normalisiertes Dataset exportiert: data/processed/dataset_2018_2019_normalized.csv\n",
      "Dateigr√∂√üe: 10387 Zeilen x 12 Spalten\n",
      "‚úÖ Export-Validierung erfolgreich: 10387 Zeilen geladen\n",
      "=== ZUSAMMENFASSUNG DATASET 2018-2019 ===\n",
      "Input: data/raw/Dataset_2018_2019.csv (10406 Zeilen)\n",
      "Output: data/processed/dataset_2018_2019_normalized.csv (10387 Zeilen)\n",
      "Datenverlust: 19 Zeilen (0.2%)\n",
      "Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\n",
      "Zus√§tzliche Spalten: 5\n",
      "üéØ DATASET 2018-2019 BEREINIGUNG ABGESCHLOSSEN!\n",
      "Bereit f√ºr Kombination mit anderen normalisierten Datasets.\n"
     ]
    }
   ],
   "source": [
    "# Export des normalisierten Datasets\n",
    "print(\"=\" * 60)\n",
    "print(\"EXPORT NORMALISIERTES DATASET\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ausgabedatei\n",
    "output_file = 'data/processed/dataset_2018_2019_normalized.csv'\n",
    "\n",
    "# Export\n",
    "df_normalized.to_csv(output_file, index=False)\n",
    "print(f\"‚úÖ Normalisiertes Dataset exportiert: {output_file}\")\n",
    "print(f\"Dateigr√∂√üe: {len(df_normalized)} Zeilen x {len(df_normalized.columns)} Spalten\")\n",
    "\n",
    "# Validierung des Exports\n",
    "test_load = pd.read_csv(output_file)\n",
    "print(f\"‚úÖ Export-Validierung erfolgreich: {len(test_load)} Zeilen geladen\")\n",
    "\n",
    "# Zusammenfassung\n",
    "print(f\"=== ZUSAMMENFASSUNG DATASET 2018-2019 ===\")\n",
    "print(f\"Input: data/raw/Dataset_2018_2019.csv ({len(df_raw)} Zeilen)\")\n",
    "print(f\"Output: {output_file} ({len(df_normalized)} Zeilen)\")\n",
    "print(f\"Datenverlust: {len(df_raw) - len(df_normalized)} Zeilen ({((len(df_raw) - len(df_normalized))/len(df_raw)*100):.1f}%)\")\n",
    "print(f\"Standardisierte Spalten: price, size, district, rooms, year, dataset_id, source\")\n",
    "print(f\"Zus√§tzliche Spalten: {len(df_normalized.columns) - 7}\")\n",
    "\n",
    "print(f\"üéØ DATASET 2018-2019 BEREINIGUNG ABGESCHLOSSEN!\")\n",
    "print(f\"Bereit f√ºr Kombination mit anderen normalisierten Datasets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4",
   "metadata": {},
   "source": [
    "## 6. Lade angereicherte Wohnlagendaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5f6g7h8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ANGEREICHERTE WOHNLAGENDATEN LADEN\n",
      "============================================================\n",
      "‚úÖ Angereicherte Daten geladen: 551,249 Zeilen, 11 Spalten\n",
      "‚úÖ Angereicherte Daten geladen: 551,249 Zeilen, 11 Spalten\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"ANGEREICHERTE WOHNLAGENDATEN LADEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "enriched_data_path = 'data/raw/wohnlagen_enriched.csv'\n",
    "try:\n",
    "    enriched_df = pd.read_csv(enriched_data_path)\n",
    "    print(f\"‚úÖ Angereicherte Daten geladen: {len(enriched_df):,} Zeilen, {len(enriched_df.columns)} Spalten\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Datei nicht gefunden: {enriched_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "i9j0k1l2",
   "metadata": {},
   "source": [
    "## 7. Kombiniere Datasets mit Wohnlagendaten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "m3n4o5p6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "KOMBINIERE MIT WOHNLAGENDATEN\n",
      "============================================================\n",
      "Original df_normalized: 10,387 Zeilen\n",
      "Original enriched_df: 551,249 Zeilen\n",
      "PLZ verf√ºgbar: 9,505/10,387 (91.5%)\n",
      "Unique street mappings: 9,479 Zeilen\n",
      "Street overlap: 533 von 3267 Stra√üen im Dataset\n",
      "‚úÖ Kombiniertes und angereichertes Dataset erstellt: 10,387 Zeilen\n",
      "Erfolgreiche Anreicherung: 1,760 von 10,387 Zeilen (16.9%)\n",
      "\n",
      "Finale Spalten im angereicherten Dataset:\n",
      "  ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'street', 'floor', 'typeOfFlat', 'yearConstructed', 'totalRent', 'plz', 'wol', 'ortsteil_neu']\n",
      "PLZ im finalen Dataset: 9,505/10,387 (91.5%)\n",
      "Top 5 PLZ im finalen Dataset:\n",
      "  10179: 762 Eintr√§ge\n",
      "  10557: 721 Eintr√§ge\n",
      "  14055: 567 Eintr√§ge\n",
      "  12555: 498 Eintr√§ge\n",
      "  10247: 424 Eintr√§ge\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"KOMBINIERE MIT WOHNLAGENDATEN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Debug: Check original data sizes\n",
    "print(f\"Original df_normalized: {len(df_normalized):,} Zeilen\")\n",
    "print(f\"Original enriched_df: {len(enriched_df):,} Zeilen\")\n",
    "\n",
    "# Stelle sicher, dass PLZ-Spalte vorhanden ist\n",
    "if 'plz' not in df_normalized.columns:\n",
    "    print(\"‚ùå PLZ-Spalte fehlt! F√ºhre PLZ-Extraktion zuerst aus.\")\n",
    "    df_normalized['plz'] = None\n",
    "else:\n",
    "    plz_available = df_normalized['plz'].notna().sum()\n",
    "    print(f\"PLZ verf√ºgbar: {plz_available:,}/{len(df_normalized):,} ({plz_available/len(df_normalized)*100:.1f}%)\")\n",
    "\n",
    "# Create a unique mapping to avoid cartesian product - use only unique street names\n",
    "enriched_df_subset = enriched_df[['strasse', 'wol', 'ortsteil_neu']].drop_duplicates(subset=['strasse'])\n",
    "print(f\"Unique street mappings: {len(enriched_df_subset):,} Zeilen\")\n",
    "\n",
    "# Debug: Check street overlap\n",
    "df_streets_unique = set(df_normalized['street'].unique())\n",
    "enriched_streets_unique = set(enriched_df_subset['strasse'].unique())\n",
    "overlap = df_streets_unique.intersection(enriched_streets_unique)\n",
    "print(f\"Street overlap: {len(overlap)} von {len(df_streets_unique)} Stra√üen im Dataset\")\n",
    "\n",
    "# Perform the merge (mit PLZ)\n",
    "df_enriched = pd.merge(df_normalized, enriched_df_subset, how='left', left_on=['street'], right_on=['strasse'])\n",
    "\n",
    "# Bereinige √ºberfl√ºssige Spalten\n",
    "if 'strasse' in df_enriched.columns:\n",
    "    df_enriched = df_enriched.drop('strasse', axis=1)\n",
    "\n",
    "print(f\"‚úÖ Kombiniertes und angereichertes Dataset erstellt: {len(df_enriched):,} Zeilen\")\n",
    "\n",
    "# Check merge success\n",
    "successful_enrichment = df_enriched['ortsteil_neu'].notna().sum()\n",
    "print(f\"Erfolgreiche Anreicherung: {successful_enrichment:,} von {len(df_enriched):,} Zeilen ({successful_enrichment/len(df_enriched)*100:.1f}%)\")\n",
    "\n",
    "# Finale Spalten-Zusammenfassung\n",
    "print(f\"\\nFinale Spalten im angereicherten Dataset:\")\n",
    "print(f\"  {list(df_enriched.columns)}\")\n",
    "\n",
    "# PLZ-Qualit√§t pr√ºfen\n",
    "if 'plz' in df_enriched.columns:\n",
    "    final_plz_count = df_enriched['plz'].notna().sum()\n",
    "    print(f\"PLZ im finalen Dataset: {final_plz_count:,}/{len(df_enriched):,} ({final_plz_count/len(df_enriched)*100:.1f}%)\")\n",
    "    \n",
    "    if final_plz_count > 0:\n",
    "        print(\"Top 5 PLZ im finalen Dataset:\")\n",
    "        for plz, count in df_enriched['plz'].value_counts().head(5).items():\n",
    "            print(f\"  {plz}: {count:,} Eintr√§ge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e1498f",
   "metadata": {},
   "source": [
    "## 7.5. PLZ-Extraktion f√ºr 2018-2019 Dataset\n",
    "\n",
    "**üéØ Problem:** Das 2018-2019 Dataset enth√§lt keine PLZ-Spalte, nur `regio3` (Ortsteil) und `street`.\n",
    "\n",
    "**üîß L√∂sung:** Wir extrahieren die PLZ durch Matching mit der `wohnlagen_enriched.csv`:\n",
    "1. **Strategie 1:** Ortsteil-basiert (regio3 ‚Üí PLZ)\n",
    "2. **Strategie 2:** Street-basiert (street ‚Üí PLZ) \n",
    "3. **Fallback:** Bezirk-basiert\n",
    "\n",
    "Dies ist essentiell f√ºr die sp√§tere PLZ-Enhancement-Pipeline und r√§umliche Genauigkeit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6d233e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "PLZ-EXTRAKTION F√úR 2018-2019 DATASET\n",
      "============================================================\n",
      "df_normalized Spalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'street', 'floor', 'typeOfFlat', 'yearConstructed', 'totalRent', 'plz']\n",
      "df_raw Spalten: ['regio3', 'street', 'livingSpace', 'baseRent', 'totalRent', 'noRooms', 'floor', 'typeOfFlat', 'yearConstructed']\n",
      "‚ùå L√§ngen-Mismatch: df_normalized=10387, df_raw=10406\n",
      "\n",
      "=== STRATEGIE 1: ORTSTEIL-BASIERTE PLZ-EXTRAKTION ===\n",
      "Ortsteil-PLZ-Mapping erstellt: 91 Eintr√§ge\n",
      "Beispiele:\n",
      "  Halensee ‚Üí 10713\n",
      "  Hakenfelde ‚Üí 13587\n",
      "  Lichterfelde ‚Üí 12209\n",
      "  Charlottenburg ‚Üí 14055\n",
      "  Marienfelde ‚Üí 12307\n",
      "\n",
      "=== STRATEGIE 2: STREET-BASIERTE PLZ-EXTRAKTION ===\n",
      "Ortsteil-PLZ-Mapping erstellt: 91 Eintr√§ge\n",
      "Beispiele:\n",
      "  Halensee ‚Üí 10713\n",
      "  Hakenfelde ‚Üí 13587\n",
      "  Lichterfelde ‚Üí 12209\n",
      "  Charlottenburg ‚Üí 14055\n",
      "  Marienfelde ‚Üí 12307\n",
      "\n",
      "=== STRATEGIE 2: STREET-BASIERTE PLZ-EXTRAKTION ===\n",
      "Street-PLZ-Mapping erstellt: 9479 Eintr√§ge\n",
      "Beispiele:\n",
      "  Aachener Stra√üe ‚Üí 10713\n",
      "  Aalemannufer ‚Üí 13587\n",
      "  Aarauer Stra√üe ‚Üí 12205\n",
      "  Aarberger Stra√üe ‚Üí 12205\n",
      "  Abbestra√üe ‚Üí 10587\n",
      "\n",
      "=== REGIO3-NORMALISIERUNG ===\n",
      "‚ùå regio3 Spalte nicht verf√ºgbar\n",
      "\n",
      "=== PLZ-EXTRAKTION ANWENDEN ===\n",
      "Street-PLZ-Mapping erstellt: 9479 Eintr√§ge\n",
      "Beispiele:\n",
      "  Aachener Stra√üe ‚Üí 10713\n",
      "  Aalemannufer ‚Üí 13587\n",
      "  Aarauer Stra√üe ‚Üí 12205\n",
      "  Aarberger Stra√üe ‚Üí 12205\n",
      "  Abbestra√üe ‚Üí 10587\n",
      "\n",
      "=== REGIO3-NORMALISIERUNG ===\n",
      "‚ùå regio3 Spalte nicht verf√ºgbar\n",
      "\n",
      "=== PLZ-EXTRAKTION ANWENDEN ===\n",
      "PLZ-Extraktion Ergebnisse:\n",
      "  Ortsteil-basiert: 0 Matches\n",
      "  Street-basiert: 1,760 Matches\n",
      "  Total mit PLZ: 1,760/10,387 (16.9%)\n",
      "\n",
      "Top 10 PLZ im 2018-2019 Dataset:\n",
      "  10245: 63 Eintr√§ge\n",
      "  13158: 54 Eintr√§ge\n",
      "  10243: 51 Eintr√§ge\n",
      "  13125: 44 Eintr√§ge\n",
      "  13593: 41 Eintr√§ge\n",
      "  12555: 41 Eintr√§ge\n",
      "  12277: 37 Eintr√§ge\n",
      "  10315: 36 Eintr√§ge\n",
      "  12683: 35 Eintr√§ge\n",
      "  13629: 34 Eintr√§ge\n",
      "\n",
      "‚úÖ PLZ-Extraktion f√ºr 2018-2019 Dataset abgeschlossen!\n",
      "PLZ-Extraktion Ergebnisse:\n",
      "  Ortsteil-basiert: 0 Matches\n",
      "  Street-basiert: 1,760 Matches\n",
      "  Total mit PLZ: 1,760/10,387 (16.9%)\n",
      "\n",
      "Top 10 PLZ im 2018-2019 Dataset:\n",
      "  10245: 63 Eintr√§ge\n",
      "  13158: 54 Eintr√§ge\n",
      "  10243: 51 Eintr√§ge\n",
      "  13125: 44 Eintr√§ge\n",
      "  13593: 41 Eintr√§ge\n",
      "  12555: 41 Eintr√§ge\n",
      "  12277: 37 Eintr√§ge\n",
      "  10315: 36 Eintr√§ge\n",
      "  12683: 35 Eintr√§ge\n",
      "  13629: 34 Eintr√§ge\n",
      "\n",
      "‚úÖ PLZ-Extraktion f√ºr 2018-2019 Dataset abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"PLZ-EXTRAKTION F√úR 2018-2019 DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Pr√ºfe verf√ºgbare Spalten\n",
    "print(f\"df_normalized Spalten: {list(df_normalized.columns)}\")\n",
    "print(f\"df_raw Spalten: {list(df_raw.columns)}\")\n",
    "\n",
    "# Da regio3 nicht in df_normalized ist, hole es aus df_raw\n",
    "# Erstelle einen tempor√§ren DataFrame f√ºr die PLZ-Extraktion\n",
    "temp_df = df_normalized.copy()\n",
    "\n",
    "# F√ºge regio3 aus df_raw hinzu (basierend auf Index)\n",
    "if len(temp_df) == len(df_raw):\n",
    "    temp_df['regio3'] = df_raw['regio3'].values\n",
    "    print(f\"‚úÖ regio3 Spalte hinzugef√ºgt: {temp_df['regio3'].nunique()} einzigartige Werte\")\n",
    "else:\n",
    "    print(f\"‚ùå L√§ngen-Mismatch: df_normalized={len(temp_df)}, df_raw={len(df_raw)}\")\n",
    "\n",
    "# Strategie 1: Ortsteil-basierte PLZ-Extraktion\n",
    "print(\"\\n=== STRATEGIE 1: ORTSTEIL-BASIERTE PLZ-EXTRAKTION ===\")\n",
    "\n",
    "# Erstelle Ortsteil-zu-PLZ-Mapping aus wohnlagen_enriched.csv\n",
    "ortsteil_plz_mapping = {}\n",
    "if 'enriched_df' in locals():\n",
    "    # Extrahiere einzigartige Ortsteil-PLZ-Kombinationen\n",
    "    ortsteil_plz_pairs = enriched_df[['ortsteil_neu', 'plz']].dropna().drop_duplicates()\n",
    "    \n",
    "    # F√ºr Ortsteile mit mehreren PLZ, nehme die h√§ufigste\n",
    "    for ortsteil in ortsteil_plz_pairs['ortsteil_neu'].unique():\n",
    "        plz_counts = enriched_df[enriched_df['ortsteil_neu'] == ortsteil]['plz'].value_counts()\n",
    "        if len(plz_counts) > 0:\n",
    "            most_common_plz = plz_counts.index[0]\n",
    "            ortsteil_plz_mapping[ortsteil] = most_common_plz\n",
    "    \n",
    "    print(f\"Ortsteil-PLZ-Mapping erstellt: {len(ortsteil_plz_mapping)} Eintr√§ge\")\n",
    "    \n",
    "    # Zeige einige Beispiele\n",
    "    print(\"Beispiele:\")\n",
    "    for ortsteil, plz in list(ortsteil_plz_mapping.items())[:5]:\n",
    "        print(f\"  {ortsteil} ‚Üí {plz}\")\n",
    "\n",
    "# Strategie 2: Street-basierte PLZ-Extraktion\n",
    "print(\"\\n=== STRATEGIE 2: STREET-BASIERTE PLZ-EXTRAKTION ===\")\n",
    "\n",
    "# Erstelle Street-zu-PLZ-Mapping\n",
    "street_plz_mapping = {}\n",
    "if 'enriched_df' in locals():\n",
    "    # Extrahiere einzigartige Street-PLZ-Kombinationen\n",
    "    street_plz_pairs = enriched_df[['strasse', 'plz']].dropna().drop_duplicates()\n",
    "    \n",
    "    # F√ºr Stra√üen mit mehreren PLZ, nehme die h√§ufigste\n",
    "    for street in street_plz_pairs['strasse'].unique():\n",
    "        plz_counts = enriched_df[enriched_df['strasse'] == street]['plz'].value_counts()\n",
    "        if len(plz_counts) > 0:\n",
    "            most_common_plz = plz_counts.index[0]\n",
    "            street_plz_mapping[street] = most_common_plz\n",
    "    \n",
    "    print(f\"Street-PLZ-Mapping erstellt: {len(street_plz_mapping)} Eintr√§ge\")\n",
    "    \n",
    "    # Zeige einige Beispiele\n",
    "    print(\"Beispiele:\")\n",
    "    for street, plz in list(street_plz_mapping.items())[:5]:\n",
    "        print(f\"  {street} ‚Üí {plz}\")\n",
    "\n",
    "# Normalisiere regio3 f√ºr besseres Matching\n",
    "print(\"\\n=== REGIO3-NORMALISIERUNG ===\")\n",
    "\n",
    "def normalize_regio3(regio3_value):\n",
    "    \"\"\"Normalisiert regio3 Werte f√ºr besseres Ortsteil-Matching.\"\"\"\n",
    "    if pd.isna(regio3_value):\n",
    "        return None\n",
    "    \n",
    "    # Konvertiere zu String und bereinige\n",
    "    normalized = str(regio3_value).strip()\n",
    "    \n",
    "    # Entferne Bezirk-Suffix (z.B. \"Prenzlauer_Berg_Prenzlauer_Berg\" ‚Üí \"Prenzlauer Berg\")\n",
    "    if '_' in normalized:\n",
    "        parts = normalized.split('_')\n",
    "        # Nehme den ersten Teil oder entferne Duplikate\n",
    "        if len(parts) == 2 and parts[0] == parts[1]:\n",
    "            normalized = parts[0]\n",
    "        else:\n",
    "            normalized = parts[0]\n",
    "    \n",
    "    # Ersetze Underscores durch Spaces\n",
    "    normalized = normalized.replace('_', ' ')\n",
    "    \n",
    "    # Spezielle Mappings f√ºr bekannte Varianten\n",
    "    mappings = {\n",
    "        'Neu Hohensch√∂nhausen': 'Neu-Hohensch√∂nhausen',\n",
    "        'Hohensch√∂nhausen': 'Alt-Hohensch√∂nhausen',\n",
    "        'Franz√∂sisch Buchholz': 'Franz√∂sisch Buchholz',\n",
    "        'Gr√ºnau': 'Gr√ºnau',\n",
    "        'K√∂penick': 'K√∂penick'\n",
    "    }\n",
    "    \n",
    "    return mappings.get(normalized, normalized)\n",
    "\n",
    "# Teste die Normalisierung (verwende temp_df mit regio3)\n",
    "if 'regio3' in temp_df.columns:\n",
    "    temp_df['regio3_normalized'] = temp_df['regio3'].apply(normalize_regio3)\n",
    "    \n",
    "    print(\"Regio3 Normalisierung - Beispiele:\")\n",
    "    regio3_examples = temp_df[['regio3', 'regio3_normalized']].drop_duplicates().head(10)\n",
    "    for _, row in regio3_examples.iterrows():\n",
    "        print(f\"  {row['regio3']} ‚Üí {row['regio3_normalized']}\")\n",
    "    \n",
    "    print(f\"\\nUnikat regio3 (original): {temp_df['regio3'].nunique()}\")\n",
    "    print(f\"Unikat regio3 (normalized): {temp_df['regio3_normalized'].nunique()}\")\n",
    "else:\n",
    "    print(\"‚ùå regio3 Spalte nicht verf√ºgbar\")\n",
    "\n",
    "# Wende PLZ-Extraktion an\n",
    "print(\"\\n=== PLZ-EXTRAKTION ANWENDEN ===\")\n",
    "\n",
    "# Initialisiere PLZ-Spalte\n",
    "temp_df['plz'] = None\n",
    "\n",
    "# Strategie 1: Ortsteil-basiert\n",
    "ortsteil_matches = 0\n",
    "if 'regio3_normalized' in temp_df.columns:\n",
    "    for idx, row in temp_df.iterrows():\n",
    "        normalized_ortsteil = row['regio3_normalized']\n",
    "        if normalized_ortsteil in ortsteil_plz_mapping:\n",
    "            temp_df.loc[idx, 'plz'] = ortsteil_plz_mapping[normalized_ortsteil]\n",
    "            ortsteil_matches += 1\n",
    "\n",
    "# Strategie 2: Street-basiert (f√ºr fehlende PLZ)\n",
    "street_matches = 0\n",
    "for idx, row in temp_df.iterrows():\n",
    "    if pd.isna(row['plz']) and row['street'] in street_plz_mapping:\n",
    "        temp_df.loc[idx, 'plz'] = street_plz_mapping[row['street']]\n",
    "        street_matches += 1\n",
    "\n",
    "# √úbertrage PLZ zur√ºck zu df_normalized\n",
    "df_normalized['plz'] = temp_df['plz']\n",
    "\n",
    "# Ergebnisse\n",
    "total_with_plz = df_normalized['plz'].notna().sum()\n",
    "print(f\"PLZ-Extraktion Ergebnisse:\")\n",
    "print(f\"  Ortsteil-basiert: {ortsteil_matches:,} Matches\")\n",
    "print(f\"  Street-basiert: {street_matches:,} Matches\")\n",
    "print(f\"  Total mit PLZ: {total_with_plz:,}/{len(df_normalized):,} ({total_with_plz/len(df_normalized)*100:.1f}%)\")\n",
    "\n",
    "# Zeige PLZ-Verteilung\n",
    "if total_with_plz > 0:\n",
    "    print(f\"\\nTop 10 PLZ im 2018-2019 Dataset:\")\n",
    "    plz_counts = df_normalized['plz'].value_counts().head(10)\n",
    "    for plz, count in plz_counts.items():\n",
    "        print(f\"  {plz}: {count:,} Eintr√§ge\")\n",
    "\n",
    "print(f\"\\n‚úÖ PLZ-Extraktion f√ºr 2018-2019 Dataset abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f89a0211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç ERWEITERTE PLZ-EXTRAKTION F√úR 2018-2019\n",
      "============================================================\n",
      "üß™ TESTE ERWEITERTE PLZ-EXTRAKTION\n",
      "==================================================\n",
      "Test der erweiterten PLZ-Extraktion:\n",
      "   'Unter den Linden' in 'Mitte' ‚Üí PLZ: 10117\n",
      "   'Alexanderplatz' in 'Mitte' ‚Üí PLZ: 10178\n",
      "   'Boxhagener Stra√üe' in 'Friedrichshain' ‚Üí PLZ: 10245\n",
      "   'Kurf√ºrstendamm' in 'Charlottenburg' ‚Üí PLZ: 10719\n",
      "   'Sonnenallee' in 'Neuk√∂lln' ‚Üí PLZ: 12437\n",
      "   'Hauptstra√üe' in 'Steglitz' ‚Üí PLZ: 13158\n",
      "   'M√ºllerstra√üe' in 'Wedding' ‚Üí PLZ: 13353\n",
      "\n",
      "üîÑ WENDE ERWEITERTE PLZ-EXTRAKTION AN\n",
      "==================================================\n",
      "Verf√ºgbare Spalten: ['price', 'size', 'district', 'rooms', 'year', 'dataset_id', 'source', 'street', 'floor', 'typeOfFlat', 'yearConstructed', 'totalRent', 'plz']\n",
      "‚úÖ Erweiterte PLZ-Extraktion Ergebnisse:\n",
      "   üìä Nur erweiterte Methode: 9,505 von 10,387 (91.5%)\n",
      "   üìä Kombiniert (alt + neu): 9,505 von 10,387 (91.5%)\n",
      "   üìà Verbesserung: +7,745 PLZ (440.1% mehr)\n",
      "   üéâ ZIEL ERREICHT: >80% PLZ-Abdeckung!\n",
      "\n",
      "üìã Verbesserte PLZ-Verteilung (Top 10):\n",
      "   10179: 762 Eintr√§ge\n",
      "   10557: 721 Eintr√§ge\n",
      "   14055: 567 Eintr√§ge\n",
      "   12555: 498 Eintr√§ge\n",
      "   10247: 424 Eintr√§ge\n",
      "   13347: 394 Eintr√§ge\n",
      "   13591: 374 Eintr√§ge\n",
      "   14197: 354 Eintr√§ge\n",
      "   10965: 313 Eintr√§ge\n",
      "   10997: 299 Eintr√§ge\n",
      "\n",
      "‚úÖ Erweiterte PLZ-Extraktion f√ºr 2018-2019 Dataset abgeschlossen!\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# ERWEITERTE PLZ-EXTRAKTION F√úR 2018-2019 DATASET\n",
    "# ===================================================================\n",
    "print(\"\\nüîç ERWEITERTE PLZ-EXTRAKTION F√úR 2018-2019\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def extract_plz_advanced_2018_2019(street, district, ortsteil_mapping=None, street_mapping=None):\n",
    "    \"\"\"\n",
    "    Erweiterte PLZ-Extraktion f√ºr 2018-2019 Dataset mit mehreren Fallback-Strategien\n",
    "    \n",
    "    Args:\n",
    "        street: Stra√üenname\n",
    "        district: Bezirk\n",
    "        ortsteil_mapping: Ortsteil-zu-PLZ-Mapping\n",
    "        street_mapping: Street-zu-PLZ-Mapping\n",
    "    \n",
    "    Returns:\n",
    "        str: PLZ oder None\n",
    "    \"\"\"\n",
    "    # Strategie 1: Street-basierte PLZ-Extraktion (falls vorhanden)\n",
    "    if pd.notna(street) and street_mapping and street in street_mapping:\n",
    "        return str(street_mapping[street])\n",
    "    \n",
    "    # Strategie 2: Bezirk-zu-PLZ-Mapping (erweitert)\n",
    "    if pd.notna(district) and ortsteil_mapping:\n",
    "        district_str = str(district).strip()\n",
    "        \n",
    "        # Direkte Bezirks-Zuordnung\n",
    "        if district_str in ortsteil_mapping:\n",
    "            return str(ortsteil_mapping[district_str])\n",
    "        \n",
    "        # Erweiterte Bezirks-Aliases\n",
    "        district_aliases = {\n",
    "            # Hauptbezirke\n",
    "            'Mitte': 'Mitte',\n",
    "            'Friedrichshain-Kreuzberg': 'Friedrichshain',\n",
    "            'Pankow': 'Pankow',\n",
    "            'Charlottenburg-Wilmersdorf': 'Charlottenburg',\n",
    "            'Spandau': 'Spandau',\n",
    "            'Steglitz-Zehlendorf': 'Steglitz',\n",
    "            'Tempelhof-Sch√∂neberg': 'Tempelhof',\n",
    "            'Neuk√∂lln': 'Neuk√∂lln',\n",
    "            'Treptow-K√∂penick': 'Treptow',\n",
    "            'Marzahn-Hellersdorf': 'Marzahn',\n",
    "            'Lichtenberg': 'Lichtenberg',\n",
    "            'Reinickendorf': 'Reinickendorf',\n",
    "            # Einzelteile zusammengesetzter Bezirke\n",
    "            'Friedrichshain': 'Friedrichshain',\n",
    "            'Kreuzberg': 'Kreuzberg',\n",
    "            'Charlottenburg': 'Charlottenburg',\n",
    "            'Wilmersdorf': 'Wilmersdorf',\n",
    "            'Steglitz': 'Steglitz',\n",
    "            'Zehlendorf': 'Zehlendorf',\n",
    "            'Tempelhof': 'Tempelhof',\n",
    "            'Sch√∂neberg': 'Sch√∂neberg',\n",
    "            'Treptow': 'Treptow',\n",
    "            'K√∂penick': 'K√∂penick',\n",
    "            'Marzahn': 'Marzahn',\n",
    "            'Hellersdorf': 'Hellersdorf'\n",
    "        }\n",
    "        \n",
    "        # Pr√ºfe Aliases\n",
    "        for alias, canonical in district_aliases.items():\n",
    "            if alias.lower() in district_str.lower():\n",
    "                if canonical in ortsteil_mapping:\n",
    "                    return str(ortsteil_mapping[canonical])\n",
    "    \n",
    "    # Strategie 3: H√§ufige Stra√üenname-Patterns\n",
    "    if pd.notna(street):\n",
    "        street_str = str(street).strip()\n",
    "        \n",
    "        # Bekannte Stra√üen-zu-PLZ-Mapping (h√§ufigste Berliner Stra√üen)\n",
    "        common_streets = {\n",
    "            'Unter den Linden': '10117',\n",
    "            'Alexanderplatz': '10178',\n",
    "            'Potsdamer Platz': '10785',\n",
    "            'Kurf√ºrstendamm': '10719',\n",
    "            'Friedrichstra√üe': '10117',\n",
    "            'Hackescher Markt': '10178',\n",
    "            'Warschauer Stra√üe': '10243',\n",
    "            'Boxhagener Stra√üe': '10245',\n",
    "            'Kastanienallee': '10435',\n",
    "            'Oranienstra√üe': '10999',\n",
    "            'Bergmannstra√üe': '10961',\n",
    "            'Savignyplatz': '10623',\n",
    "            'Rosenthaler Stra√üe': '10119',\n",
    "            'Torstra√üe': '10119',\n",
    "            'Invalidenstra√üe': '10115',\n",
    "            'Chausseestra√üe': '10115',\n",
    "            'Brunnenstra√üe': '10119',\n",
    "            'Bernauer Stra√üe': '10119',\n",
    "            'Prenzlauer Allee': '10405',\n",
    "            'Karl-Marx-Allee': '10243',\n",
    "            'Frankfurter Allee': '10247',\n",
    "            'Sonnenallee': '12047',\n",
    "            'Hermannstra√üe': '12049',\n",
    "            'Kantstra√üe': '10623',\n",
    "            'Wilmersdorfer Stra√üe': '10627',\n",
    "            'Uhlandstra√üe': '10623',\n",
    "            'Ku\\'damm': '10719',\n",
    "            'Tauentzienstra√üe': '10789',\n",
    "            'Nollendorfplatz': '10777',\n",
    "            'Wittenbergplatz': '10789'\n",
    "        }\n",
    "        \n",
    "        # Pr√ºfe auf bekannte Stra√üen (auch Teilstrings)\n",
    "        for known_street, plz in common_streets.items():\n",
    "            if known_street.lower() in street_str.lower():\n",
    "                return plz\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Teste die erweiterte PLZ-Extraktion\n",
    "print(\"üß™ TESTE ERWEITERTE PLZ-EXTRAKTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "test_cases = [\n",
    "    (\"Unter den Linden\", \"Mitte\"),\n",
    "    (\"Alexanderplatz\", \"Mitte\"),\n",
    "    (\"Boxhagener Stra√üe\", \"Friedrichshain\"),\n",
    "    (\"Kurf√ºrstendamm\", \"Charlottenburg\"),\n",
    "    (\"Sonnenallee\", \"Neuk√∂lln\"),\n",
    "    (\"Hauptstra√üe\", \"Steglitz\"),\n",
    "    (\"M√ºllerstra√üe\", \"Wedding\")\n",
    "]\n",
    "\n",
    "print(\"Test der erweiterten PLZ-Extraktion:\")\n",
    "for street, district in test_cases:\n",
    "    plz = extract_plz_advanced_2018_2019(street, district, ortsteil_plz_mapping, street_plz_mapping)\n",
    "    print(f\"   '{street}' in '{district}' ‚Üí PLZ: {plz}\")\n",
    "\n",
    "# Erstelle erweiterte PLZ-Extraktion f√ºr alle Daten\n",
    "print(\"\\nüîÑ WENDE ERWEITERTE PLZ-EXTRAKTION AN\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Verwende die verf√ºgbaren Spalten\n",
    "available_columns = df_normalized.columns.tolist()\n",
    "print(f\"Verf√ºgbare Spalten: {available_columns}\")\n",
    "\n",
    "# Wende erweiterte PLZ-Extraktion an\n",
    "df_normalized['plz_advanced'] = df_normalized.apply(\n",
    "    lambda row: extract_plz_advanced_2018_2019(\n",
    "        row.get('street', None),\n",
    "        row.get('district', None),\n",
    "        ortsteil_plz_mapping,\n",
    "        street_plz_mapping\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Kombiniere alte und neue PLZ-Extraktion\n",
    "df_normalized['plz_combined'] = df_normalized['plz_advanced'].fillna(df_normalized['plz'])\n",
    "\n",
    "# Ergebnisse der erweiterten PLZ-Extraktion\n",
    "plz_advanced_found = df_normalized['plz_advanced'].notna().sum()\n",
    "plz_combined_found = df_normalized['plz_combined'].notna().sum()\n",
    "total_rows = len(df_normalized)\n",
    "\n",
    "print(f\"‚úÖ Erweiterte PLZ-Extraktion Ergebnisse:\")\n",
    "print(f\"   üìä Nur erweiterte Methode: {plz_advanced_found:,} von {total_rows:,} ({plz_advanced_found/total_rows*100:.1f}%)\")\n",
    "print(f\"   üìä Kombiniert (alt + neu): {plz_combined_found:,} von {total_rows:,} ({plz_combined_found/total_rows*100:.1f}%)\")\n",
    "\n",
    "# Verbesserung berechnen\n",
    "old_plz_count = df_normalized['plz'].notna().sum()\n",
    "improvement = plz_combined_found - old_plz_count\n",
    "improvement_pct = (improvement / old_plz_count) * 100 if old_plz_count > 0 else 0\n",
    "\n",
    "print(f\"   üìà Verbesserung: +{improvement:,} PLZ ({improvement_pct:.1f}% mehr)\")\n",
    "\n",
    "if plz_combined_found / total_rows >= 0.8:\n",
    "    print(f\"   üéâ ZIEL ERREICHT: >80% PLZ-Abdeckung!\")\n",
    "elif improvement > 0:\n",
    "    print(f\"   üìà VERBESSERUNG: Erh√∂hte PLZ-Abdeckung\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  KEINE VERBESSERUNG: Alternative Strategie erforderlich\")\n",
    "\n",
    "# Aktualisiere die PLZ-Spalte mit dem kombinierten Ergebnis\n",
    "df_normalized['plz'] = df_normalized['plz_combined']\n",
    "\n",
    "# Zeige verbesserte PLZ-Verteilung\n",
    "if plz_combined_found > 0:\n",
    "    print(f\"\\nüìã Verbesserte PLZ-Verteilung (Top 10):\")\n",
    "    plz_counts_new = df_normalized['plz'].value_counts().head(10)\n",
    "    for plz, count in plz_counts_new.items():\n",
    "        print(f\"   {plz}: {count:,} Eintr√§ge\")\n",
    "\n",
    "# Bereinige tempor√§re Spalten\n",
    "df_normalized = df_normalized.drop(['plz_advanced', 'plz_combined'], axis=1)\n",
    "\n",
    "print(f\"\\n‚úÖ Erweiterte PLZ-Extraktion f√ºr 2018-2019 Dataset abgeschlossen!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "q7r8s9t0",
   "metadata": {},
   "source": [
    "## 8. Export des finalen angereicherten Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "u1v2w3x4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXPORT FINALES ANGEREICHERTES DATASET\n",
      "============================================================\n",
      "‚úÖ Finales angereichertes Dataset exportiert: data/processed/dataset_2018_2019_enriched.csv\n",
      "Dateigr√∂√üe: 10,387 Zeilen x 15 Spalten\n",
      "‚úÖ Export-Validierung erfolgreich: 10,387 Zeilen geladen\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXPORT FINALES ANGEREICHERTES DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Export\n",
    "output_file_enriched = 'data/processed/dataset_2018_2019_enriched.csv'\n",
    "df_enriched.to_csv(output_file_enriched, index=False)\n",
    "\n",
    "print(f\"‚úÖ Finales angereichertes Dataset exportiert: {output_file_enriched}\")\n",
    "print(f\"Dateigr√∂√üe: {len(df_enriched):,} Zeilen x {len(df_enriched.columns)} Spalten\")\n",
    "\n",
    "# Validierung durch Wiedereinlesen\n",
    "test_df_enriched = pd.read_csv(output_file_enriched)\n",
    "print(f\"‚úÖ Export-Validierung erfolgreich: {len(test_df_enriched):,} Zeilen geladen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39382f59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîß KRITISCHE PLZ-DATENTYP-REPARATUR VOR EXPORT\n",
      "============================================================\n",
      "PLZ-Datentyp vor Reparatur: object\n",
      "PLZ-Beispiele vor Reparatur: [np.int64(13591), np.int64(12527), np.int64(13053)]\n",
      "PLZ-Datentyp nach Reparatur: object\n",
      "PLZ-Beispiele nach Reparatur: ['13591', '12527', '13053']\n",
      "‚úÖ PLZ-Datentyp-Reparatur abgeschlossen!\n",
      "   ‚û°Ô∏è  PLZ wird jetzt als String gespeichert f√ºr korrekten Join\n"
     ]
    }
   ],
   "source": [
    "# ===================================================================\n",
    "# KRITISCHE PLZ-DATENTYP-REPARATUR VOR EXPORT\n",
    "# ===================================================================\n",
    "print(\"\\nüîß KRITISCHE PLZ-DATENTYP-REPARATUR VOR EXPORT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Stelle sicher, dass PLZ als STRING gespeichert wird (nicht als Integer/Float)\n",
    "# Dies ist kritisch f√ºr den sp√§teren Join in 04_Combine_Datasets.ipynb\n",
    "print(\"PLZ-Datentyp vor Reparatur:\", df_enriched['plz'].dtype)\n",
    "print(\"PLZ-Beispiele vor Reparatur:\", df_enriched['plz'].dropna().head(3).tolist())\n",
    "\n",
    "# Konvertiere PLZ zu String\n",
    "df_enriched['plz'] = df_enriched['plz'].apply(\n",
    "    lambda x: str(int(x)) if pd.notna(x) else None\n",
    ")\n",
    "\n",
    "print(\"PLZ-Datentyp nach Reparatur:\", df_enriched['plz'].dtype)  \n",
    "print(\"PLZ-Beispiele nach Reparatur:\", df_enriched['plz'].dropna().head(3).tolist())\n",
    "\n",
    "print(\"‚úÖ PLZ-Datentyp-Reparatur abgeschlossen!\")\n",
    "print(\"   ‚û°Ô∏è  PLZ wird jetzt als String gespeichert f√ºr korrekten Join\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
